<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Datawhale零基础入门数据挖掘-Task5</title>
    <url>/2020/04/04/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task5/</url>
    <content><![CDATA[<ul>
<li>对于多种调参完成的模型进行模型融合</li>
</ul>
<ol>
<li>简单加权融合:</li>
</ol>
<ul>
<li>回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）；</li>
<li>分类：投票（Voting)</li>
<li>综合：排序融合(Rank averaging)，log融合</li>
</ul>
<ol start="2">
<li>stacking/blending:</li>
</ol>
<ul>
<li>构建多层模型，并利用预测结果再拟合预测。</li>
</ul>
<ol start="3">
<li>boosting/bagging（在xgboost，Adaboost,GBDT中已经用到）:</li>
</ol>
<ul>
<li>多树的提升方法</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">## 定义结果的加权平均函数</span><br><span class="line">def Weighted_method(test_pre1,test_pre2,test_pre3,w&#x3D;[1&#x2F;3,1&#x2F;3,1&#x2F;3]):</span><br><span class="line">    Weighted_result &#x3D; w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)</span><br><span class="line">    return Weighted_result</span><br><span class="line"></span><br><span class="line">from sklearn import metrics</span><br><span class="line"># 各模型的预测结果计算MAE</span><br><span class="line">print(&#39;Pred1 MAE:&#39;,metrics.mean_absolute_error(y_test_true, test_pre1))</span><br><span class="line">print(&#39;Pred2 MAE:&#39;,metrics.mean_absolute_error(y_test_true, test_pre2))</span><br><span class="line">print(&#39;Pred3 MAE:&#39;,metrics.mean_absolute_error(y_test_true, test_pre3))</span><br><span class="line"></span><br><span class="line">## 根据加权计算MAE</span><br><span class="line">w &#x3D; [0.3,0.4,0.3] # 定义比重权值</span><br><span class="line">Weighted_pre &#x3D; Weighted_method(test_pre1,test_pre2,test_pre3,w)</span><br><span class="line">print(&#39;Weighted_pre MAE:&#39;,metrics.mean_absolute_error(y_test_true, Weighted_pre))</span><br><span class="line"></span><br><span class="line">## 定义结果的加权平均函数</span><br><span class="line">def Mean_method(test_pre1,test_pre2,test_pre3):</span><br><span class="line">    Mean_result &#x3D; pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis&#x3D;1).mean(axis&#x3D;1)</span><br><span class="line">    return Mean_result</span><br><span class="line"></span><br><span class="line">Mean_pre &#x3D; Mean_method(test_pre1,test_pre2,test_pre3)</span><br><span class="line">print(&#39;Mean_pre MAE:&#39;,metrics.mean_absolute_error(y_test_true, Mean_pre))</span><br><span class="line"></span><br><span class="line">## 定义结果的加权平均函数</span><br><span class="line">def Median_method(test_pre1,test_pre2,test_pre3):</span><br><span class="line">    Median_result &#x3D; pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis&#x3D;1).median(axis&#x3D;1)</span><br><span class="line">    return Median_result</span><br><span class="line"></span><br><span class="line">Median_pre &#x3D; Median_method(test_pre1,test_pre2,test_pre3)</span><br><span class="line">print(&#39;Median_pre MAE:&#39;,metrics.mean_absolute_error(y_test_true, Median_pre))</span><br><span class="line"></span><br><span class="line">from sklearn import linear_model</span><br><span class="line"></span><br><span class="line">def Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2&#x3D; linear_model.LinearRegression()):</span><br><span class="line">    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis&#x3D;1).values,y_train_true)</span><br><span class="line">    Stacking_result &#x3D; model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis&#x3D;1).values)</span><br><span class="line">    return Stacking_result</span><br><span class="line"></span><br><span class="line">## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值</span><br><span class="line">train_reg1 &#x3D; [3.2, 8.2, 9.1, 5.2]</span><br><span class="line">train_reg2 &#x3D; [2.9, 8.1, 9.0, 4.9]</span><br><span class="line">train_reg3 &#x3D; [3.1, 7.9, 9.2, 5.0]</span><br><span class="line"># y_test_true 代表第模型的真实值</span><br><span class="line">y_train_true &#x3D; [3, 8, 9, 5] </span><br><span class="line"></span><br><span class="line">test_pre1 &#x3D; [1.2, 3.2, 2.1, 6.2]</span><br><span class="line">test_pre2 &#x3D; [0.9, 3.1, 2.0, 5.9]</span><br><span class="line">test_pre3 &#x3D; [1.1, 2.9, 2.2, 6.0]</span><br><span class="line"></span><br><span class="line"># y_test_true 代表第模型的真实值</span><br><span class="line">y_test_true &#x3D; [1, 3, 2, 6] </span><br><span class="line"></span><br><span class="line">model_L2&#x3D; linear_model.LinearRegression()</span><br><span class="line">Stacking_pre &#x3D; Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,</span><br><span class="line">                               test_pre1,test_pre2,test_pre3,model_L2)</span><br><span class="line">print(&#39;Stacking_pre MAE:&#39;,metrics.mean_absolute_error(y_test_true, Stacking_pre))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据挖掘及机器学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Datawhale零基础入门数据挖掘-Task4</title>
    <url>/2020/04/01/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task4/</url>
    <content><![CDATA[<ul>
<li>了解常用的机器学习模型，并掌握机器学习模型的建模与调参流程</li>
</ul>
<h1 id="相关原理"><a href="#相关原理" class="headerlink" title="相关原理"></a>相关原理</h1><h2 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h2><p><a href="https://zhuanlan.zhihu.com/p/49480391" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49480391</a></p>
<h2 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h2><p><a href="https://zhuanlan.zhihu.com/p/65304798" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65304798</a></p>
<h2 id="GBDT模型"><a href="#GBDT模型" class="headerlink" title="GBDT模型"></a>GBDT模型</h2><p><a href="https://zhuanlan.zhihu.com/p/45145899" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/45145899</a></p>
<h2 id="XGBoost模型"><a href="#XGBoost模型" class="headerlink" title="XGBoost模型"></a>XGBoost模型</h2><p><a href="https://zhuanlan.zhihu.com/p/86816771" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/86816771</a></p>
<h2 id="LightGBM模型"><a href="#LightGBM模型" class="headerlink" title="LightGBM模型"></a>LightGBM模型</h2><p><a href="https://zhuanlan.zhihu.com/p/89360721" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/89360721</a></p>
<h2 id="教材推荐"><a href="#教材推荐" class="headerlink" title="教材推荐"></a>教材推荐</h2><ul>
<li>《机器学习》 <a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">https://book.douban.com/subject/26708119/</a></li>
<li>《统计学习方法》 <a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">https://book.douban.com/subject/10590856/</a></li>
<li>《Python大战机器学习》 <a href="https://book.douban.com/subject/26987890/" target="_blank" rel="noopener">https://book.douban.com/subject/26987890/</a></li>
<li>《面向机器学习的特征工程》 <a href="https://book.douban.com/subject/26826639/" target="_blank" rel="noopener">https://book.douban.com/subject/26826639/</a></li>
<li>《数据科学家访谈录》 <a href="https://book.douban.com/subject/30129410/" target="_blank" rel="noopener">https://book.douban.com/subject/30129410/</a></li>
</ul>
<h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间</span><br><span class="line">def reduce_mem_usage(df):</span><br><span class="line">    &quot;&quot;&quot; iterate through all the columns of a dataframe and modify the data type</span><br><span class="line">        to reduce memory usage.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    start_mem &#x3D; df.memory_usage().sum()</span><br><span class="line">    print(&#39;Memory usage of dataframe is &#123;:.2f&#125; MB&#39;.format(start_mem))</span><br><span class="line"></span><br><span class="line">    for col in df.columns:</span><br><span class="line">        col_type &#x3D; df[col].dtype</span><br><span class="line"></span><br><span class="line">        if col_type !&#x3D; object:</span><br><span class="line">            c_min &#x3D; df[col].min()</span><br><span class="line">            c_max &#x3D; df[col].max()</span><br><span class="line">            if str(col_type)[:3] &#x3D;&#x3D; &#39;int&#39;:</span><br><span class="line">                if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.int8)</span><br><span class="line">                elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.int16)</span><br><span class="line">                elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.int32)</span><br><span class="line">                elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.int64)</span><br><span class="line">            else:</span><br><span class="line">                if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.float16)</span><br><span class="line">                elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.float32)</span><br><span class="line">                else:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.float64)</span><br><span class="line">        else:</span><br><span class="line">            df[col] &#x3D; df[col].astype(&#39;category&#39;)</span><br><span class="line"></span><br><span class="line">    end_mem &#x3D; df.memory_usage().sum()</span><br><span class="line">    print(&#39;Memory usage after optimization is: &#123;:.2f&#125; MB&#39;.format(end_mem))</span><br><span class="line">    print(&#39;Decreased by &#123;:.1f&#125;%&#39;.format(100 * (start_mem - end_mem) &#x2F; start_mem))</span><br><span class="line">    return df</span><br><span class="line"></span><br><span class="line">sample_feature &#x3D; reduce_mem_usage(pd.read_csv(&quot;data_for_tree.csv&quot;))</span><br></pre></td></tr></table></figure>

<pre><code>Memory usage of dataframe is 62099672.00 MB
Memory usage after optimization is: 16520303.00 MB
Decreased by 73.4%</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 返回 x in sample_feature.columns not include [&#39;price&#39;,&#39;brand&#39;,&#39;model&#39;,&#39;brand&#39;] 的列表</span><br><span class="line">continuous_feature_names &#x3D; [x for x in sample_feature.columns if x not in [&#39;price&#39;,&#39;brand&#39;,&#39;model&#39;,&#39;brand&#39;]]</span><br></pre></td></tr></table></figure>

<h1 id="线性回归-amp-五折交叉验证-amp-模拟真实业务情况"><a href="#线性回归-amp-五折交叉验证-amp-模拟真实业务情况" class="headerlink" title="线性回归 &amp; 五折交叉验证 &amp; 模拟真实业务情况"></a>线性回归 &amp; 五折交叉验证 &amp; 模拟真实业务情况</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample_feature &#x3D; sample_feature.dropna().replace(&#39;-&#39;, 0).reset_index(drop&#x3D;True)</span><br><span class="line">sample_feature[&#39;notRepairedDamage&#39;] &#x3D; sample_feature[&#39;notRepairedDamage&#39;].astype(np.float32)</span><br><span class="line">train &#x3D; sample_feature[continuous_feature_names + [&#39;price&#39;]]</span><br><span class="line"></span><br><span class="line">train_X &#x3D; train[continuous_feature_names]</span><br><span class="line">train_y &#x3D; train[&#39;price&#39;]</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure>

<h2 id="简单建模"><a href="#简单建模" class="headerlink" title="简单建模"></a>简单建模</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">model &#x3D; LinearRegression(normalize&#x3D;True)</span><br><span class="line"></span><br><span class="line">model &#x3D; model.fit(train_X, train_y)</span><br><span class="line"></span><br><span class="line"># 查看训练的线性回归模型的截距（intercept）与权重(coef)</span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line"></span><br><span class="line">print(sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key&#x3D;lambda x:x[1], reverse&#x3D;True))</span><br></pre></td></tr></table></figure>

<pre><code>intercept:-110670.68277241504

[(&apos;v_6&apos;, 3367064.3416418717), 
(&apos;v_8&apos;, 700675.5609398251),
 (&apos;v_9&apos;, 170630.27723215616), 
(&apos;v_7&apos;, 32322.661931980558), 
(&apos;v_12&apos;, 20473.670796988994), 
(&apos;v_3&apos;, 17868.07954151303), 
(&apos;v_11&apos;, 11474.9389967116), 
(&apos;v_13&apos;, 11261.764560019501), 
(&apos;v_10&apos;, 2683.9200906064084), 
(&apos;gearbox&apos;, 881.8225039250154), 
(&apos;fuelType&apos;, 363.9042507216036), 
(&apos;bodyType&apos;, 189.60271012073036), 
(&apos;city&apos;, 44.94975120522736), 
(&apos;power&apos;, 28.553901616752857), 
(&apos;brand_price_median&apos;, 0.5103728134078609), 
(&apos;brand_price_std&apos;, 0.4503634709263256), 
(&apos;brand_amount&apos;, 0.14881120395065583), 
(&apos;brand_price_max&apos;, 0.0031910186703138638), 
(&apos;SaleID&apos;, 5.355989919860593e-05), 
(&apos;offerType&apos;, 4.397239536046982e-06), 
(&apos;train&apos;, 2.7939677238464355e-07), 
(&apos;seller&apos;, -2.873130142688751e-07), 
(&apos;brand_price_sum&apos;, -2.175006868187596e-05), 
(&apos;name&apos;, -0.0002980012713074109), 
(&apos;used_time&apos;, -0.002515894332880479), 
(&apos;brand_price_average&apos;, -0.404904845101148), 
(&apos;brand_price_min&apos;, -2.2467753486888244), 
(&apos;power_bin&apos;, -34.42064411727887), 
(&apos;v_14&apos;, -274.7841180777388), 
(&apos;kilometer&apos;, -372.89752666073025), 
(&apos;notRepairedDamage&apos;, -495.1903844628239), 
(&apos;v_0&apos;, -2045.0549573558887), 
(&apos;v_5&apos;, -11022.98624082137), 
(&apos;v_4&apos;, -15121.731109860013), 
(&apos;v_2&apos;, -26098.29992055148), 
(&apos;v_1&apos;, -45556.18929726381)]</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">subsample_index &#x3D; np.random.randint(low&#x3D;0, high&#x3D;len(train_y), size&#x3D;50)</span><br><span class="line">#绘制特征v_9的值与标签的散点图，图片发现模型的预测结果（蓝色点）与真实标签（黑色点）的分布差异较大，</span><br><span class="line"># 且部分预测值出现了小于0的情况，说明我们的模型存在一些问题</span><br><span class="line">plt.scatter(train_X[&#39;v_9&#39;][subsample_index], train_y[subsample_index], color&#x3D;&#39;black&#39;)</span><br><span class="line">plt.scatter(train_X[&#39;v_9&#39;][subsample_index], model.predict(train_X.loc[subsample_index]), color&#x3D;&#39;blue&#39;)</span><br><span class="line">plt.xlabel(&#39;v_9&#39;)</span><br><span class="line">plt.ylabel(&#39;price&#39;)</span><br><span class="line">plt.legend([&#39;True Price&#39;,&#39;Predicted Price&#39;],loc&#x3D;&#39;upper right&#39;)</span><br><span class="line">print(&#39;The predicted price is obvious different from true price&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/04/01/G8VrRI.png" alt="1"></p>
<ul>
<li>通过作图我们发现数据的标签（price）呈现长尾分布，不利于我们的建模预测。原因是很多模型都假设数据误差项符合正态分布，而长尾分布的数据违背了这一假设。参考博客：<a href="https://blog.csdn.net/Noob_daniel/article/details/76087829" target="_blank" rel="noopener">https://blog.csdn.net/Noob_daniel/article/details/76087829</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import seaborn as sns</span><br><span class="line">print(&#39;It is clear to see the price shows a typical exponential distribution&#39;)</span><br><span class="line">plt.figure(figsize&#x3D;(15,5))</span><br><span class="line">plt.subplot(1,2,1)</span><br><span class="line">sns.distplot(train_y)</span><br><span class="line">plt.subplot(1,2,2)</span><br><span class="line">sns.distplot(train_y[train_y &lt; np.quantile(train_y, 0.9)])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="" alt="2"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在这里我们对标签进行了 log(x+1) 变换，使标签贴近于正态分布</span><br><span class="line">train_y_ln &#x3D; np.log(train_y + 1)</span><br><span class="line">print(&#39;The transformed price seems like normal distribution&#39;)</span><br><span class="line">plt.figure(figsize&#x3D;(15,5))</span><br><span class="line">plt.subplot(1,2,1)</span><br><span class="line">sns.distplot(train_y_ln)</span><br><span class="line">plt.subplot(1,2,2)</span><br><span class="line">sns.distplot(train_y_ln[train_y_ln &lt; np.quantile(train_y_ln, 0.9)])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="" alt="3"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; model.fit(train_X, train_y_ln)</span><br><span class="line"></span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line">sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key&#x3D;lambda x:x[1], reverse&#x3D;True)</span><br></pre></td></tr></table></figure>

<pre><code>intercept:18.750748443060488

[(&apos;v_9&apos;, 8.052410408822315), 
(&apos;v_5&apos;, 5.764240780403914), 
(&apos;v_12&apos;, 1.618206098241706), 
(&apos;v_1&apos;, 1.479831064546508), 
(&apos;v_11&apos;, 1.166900417358536), 
(&apos;v_13&apos;, 0.9404706327194452), 
(&apos;v_7&apos;, 0.7137281645215736), 
(&apos;v_3&apos;, 0.6837863827349204), 
(&apos;v_0&apos;, 0.00850050520973589), 
(&apos;power_bin&apos;, 0.008497968353528977), 
(&apos;gearbox&apos;, 0.007922378343285602), 
(&apos;fuelType&apos;, 0.006684768936305926), 
(&apos;bodyType&apos;, 0.004523520651791603), 
(&apos;power&apos;, 0.0007161895389359644), 
(&apos;brand_price_min&apos;, 3.334354528992352e-05), 
(&apos;brand_amount&apos;, 2.897880289491835e-06), 
(&apos;brand_price_median&apos;, 1.2571187771074404e-06), 
(&apos;brand_price_std&apos;, 6.659170007178332e-07), 
(&apos;brand_price_max&apos;, 6.194957302457314e-07), 
(&apos;brand_price_average&apos;, 5.999348706659352e-07), 
(&apos;SaleID&apos;, 2.1194159119234957e-08), 
(&apos;seller&apos;, 1.6262902136077173e-10), 
(&apos;offerType&apos;, 1.1036149771825876e-10), 
(&apos;train&apos;, 6.707523425575346e-12), 
(&apos;brand_price_sum&apos;, -1.5126514245669237e-10), 
(&apos;name&apos;, -7.015511195846627e-08), 
(&apos;used_time&apos;, -4.122477016270915e-06), 
(&apos;city&apos;, -0.002218783709616053), 
(&apos;v_14&apos;, -0.004234189820672137), 
(&apos;kilometer&apos;, -0.013835867353556136), 
(&apos;notRepairedDamage&apos;, -0.27027942480393996), 
(&apos;v_4&apos;, -0.8315697362911634), 
(&apos;v_2&apos;, -0.9470821267759207), 
(&apos;v_10&apos;, -1.6261468392032863), 
(&apos;v_8&apos;, -40.34300817115224), 
(&apos;v_6&apos;, -238.79035497319248)]</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#再次进行可视化，发现预测结果与真实值较为接近，且未出现异常状况</span><br><span class="line">plt.scatter(train_X[&#39;v_9&#39;][subsample_index], train_y[subsample_index], color&#x3D;&#39;black&#39;)</span><br><span class="line">plt.scatter(train_X[&#39;v_9&#39;][subsample_index], np.exp(model.predict(train_X.loc[subsample_index])), color&#x3D;&#39;blue&#39;)</span><br><span class="line">plt.xlabel(&#39;v_9&#39;)</span><br><span class="line">plt.ylabel(&#39;price&#39;)</span><br><span class="line">plt.legend([&#39;True Price&#39;,&#39;Predicted Price&#39;],loc&#x3D;&#39;upper right&#39;)</span><br><span class="line">print(&#39;The predicted price seems normal after np.log transforming&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="" alt="4"></p>
<h2 id="五折交叉验证"><a href="#五折交叉验证" class="headerlink" title="五折交叉验证"></a>五折交叉验证</h2><blockquote>
<p>在使用训练集对参数进行训练的时候，经常会发现人们通常会将一整个训练集分为三个部分（比如mnist手写训练集）。一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。这其实是为了保证训练效果而特意设置的。其中测试集很好理解，其实就是完全不参与训练的数据，仅仅用来观测测试效果的数&gt;&gt;据。而训练集和评估集则牵涉到下面的知识了。</p>
</blockquote>
<blockquote>
<p>因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这一部分不参加训练）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证（Cross Validation）</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##使用线性回归模型，对未处理标签的特征数据进行五折交叉验证</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.metrics import mean_absolute_error,  make_scorer</span><br><span class="line">def log_transfer(func):</span><br><span class="line">    def wrapper(y, yhat):</span><br><span class="line">        result &#x3D; func(np.log(y), np.nan_to_num(np.log(yhat)))</span><br><span class="line">        return result</span><br><span class="line">    return wrapper</span><br><span class="line"></span><br><span class="line">scores &#x3D; cross_val_score(model, X&#x3D;train_X, y&#x3D;train_y, verbose&#x3D;1, cv &#x3D; 5, scoring&#x3D;make_scorer(log_transfer(mean_absolute_error)))</span><br></pre></td></tr></table></figure>

<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.7s finished</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(&#39;AVG:&#39;, np.mean(scores))</span><br></pre></td></tr></table></figure>

<pre><code>AVG: 1.3658024027748357</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#使用线性回归模型，对处理过标签的特征数据进行五折交叉验证（</span><br><span class="line">scores &#x3D; cross_val_score(model, X&#x3D;train_X, y&#x3D;train_y_ln, verbose&#x3D;1, cv &#x3D; 5, scoring&#x3D;make_scorer(mean_absolute_error))</span><br></pre></td></tr></table></figure>

<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.8s finished</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(&#39;AVG:&#39;, np.mean(scores))</span><br></pre></td></tr></table></figure>

<pre><code>AVG: 0.19325301753940502</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scores &#x3D; pd.DataFrame(scores.reshape(1,-1))</span><br><span class="line">scores.columns &#x3D; [&#39;cv&#39; + str(x) for x in range(1, 6)]</span><br><span class="line">scores.index &#x3D; [&#39;MAE&#39;]</span><br><span class="line">print(scores)</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th></th>
<th>cv1</th>
<th>cv2</th>
<th>cv3</th>
<th>cv4</th>
<th>cv5</th>
</tr>
</thead>
<tbody><tr>
<td>MAE</td>
<td>0.190792</td>
<td>0.193758</td>
<td>0.194132</td>
<td>0.191825</td>
<td>0.195758</td>
</tr>
</tbody></table>
<h2 id="模拟真实业务情况"><a href="#模拟真实业务情况" class="headerlink" title="模拟真实业务情况"></a>模拟真实业务情况</h2><blockquote>
<p>但在事实上，由于我们并不具有预知未来的能力，五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况。通过2018年的二手车价格预测2017年的二手车价格，这显然是不合理的，因此我们还可以采用时间顺序对数据集进行分隔。在本例中，我们选用靠前时间的4/5样本当作训练集，靠后时间的1/5当作验证集，最终结果与五折交叉验证差距不大</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 采用时间顺序对数据集进行分隔 选用靠前时间的4&#x2F;5样本当作训练集，靠后时间的1&#x2F;5当作验证集</span><br><span class="line">import datetime</span><br><span class="line">sample_feature &#x3D; sample_feature.reset_index(drop&#x3D;True)</span><br><span class="line">split_point &#x3D; len(sample_feature) &#x2F;&#x2F; 5 * 4 # 取整除 - 返回商的整数部分（向下取整）</span><br><span class="line"></span><br><span class="line">train &#x3D; sample_feature.loc[:split_point].dropna()</span><br><span class="line">val &#x3D; sample_feature.loc[split_point:].dropna()</span><br><span class="line"></span><br><span class="line">train_X &#x3D; train[continuous_feature_names]</span><br><span class="line">train_y_ln &#x3D; np.log(train[&#39;price&#39;] + 1)</span><br><span class="line">val_X &#x3D; val[continuous_feature_names]</span><br><span class="line">val_y_ln &#x3D; np.log(val[&#39;price&#39;] + 1)</span><br><span class="line"></span><br><span class="line">model &#x3D; model.fit(train_X, train_y_ln)</span><br><span class="line">print(mean_absolute_error(val_y_ln, model.predict(val_X)))</span><br></pre></td></tr></table></figure>

<pre><code>0.19577667229471246</code></pre><h2 id="绘制学习率曲线与验证曲线"><a href="#绘制学习率曲线与验证曲线" class="headerlink" title="绘制学习率曲线与验证曲线"></a>绘制学习率曲线与验证曲线</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 绘制学习率曲线与验证曲线</span><br><span class="line">from sklearn.model_selection import learning_curve, validation_curve</span><br><span class="line">def plot_learning_curve(estimator, title, X, y, ylim&#x3D;None, cv&#x3D;None,n_jobs&#x3D;1, train_size&#x3D;np.linspace(.1, 1.0, 5 )):</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(title)</span><br><span class="line">    if ylim is not None:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(&#39;Training example&#39;)</span><br><span class="line">    plt.ylabel(&#39;score&#39;)</span><br><span class="line">    train_sizes, train_scores, test_scores &#x3D; learning_curve(estimator, X, y, cv&#x3D;cv, n_jobs&#x3D;n_jobs, train_sizes&#x3D;train_size, scoring &#x3D; make_scorer(mean_absolute_error))</span><br><span class="line">    train_scores_mean &#x3D; np.mean(train_scores, axis&#x3D;1)</span><br><span class="line">    train_scores_std &#x3D; np.std(train_scores, axis&#x3D;1)</span><br><span class="line">    test_scores_mean &#x3D; np.mean(test_scores, axis&#x3D;1)</span><br><span class="line">    test_scores_std &#x3D; np.std(test_scores, axis&#x3D;1)</span><br><span class="line">    plt.grid()#区域</span><br><span class="line"></span><br><span class="line">    # x：第一个参数表示覆盖的区域，我直接复制为x，表示整个x都覆盖</span><br><span class="line">    # 0：表示覆盖的下限</span><br><span class="line">    # y：表示覆盖的上限是y这个曲线</span><br><span class="line">    # facecolor：覆盖区域的颜色</span><br><span class="line">    # alpha：覆盖区域的透明度[0,1],其值越大，表示越不透明</span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha&#x3D;0.1,</span><br><span class="line">                     color&#x3D;&quot;r&quot;)</span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, alpha&#x3D;0.1,</span><br><span class="line">                     color&#x3D;&quot;g&quot;)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, &#39;o-&#39;, color&#x3D;&#39;r&#39;,</span><br><span class="line">             label&#x3D;&quot;Training score&quot;)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean,&#39;o-&#39;,color&#x3D;&quot;g&quot;,</span><br><span class="line">             label&#x3D;&quot;Cross-validation score&quot;)</span><br><span class="line">    plt.legend(loc&#x3D;&quot;best&quot;)</span><br><span class="line">    return plt</span><br><span class="line"></span><br><span class="line">plot_learning_curve(LinearRegression(), &#39;Liner_model&#39;, train_X[:1000], train_y_ln[:1000], ylim&#x3D;(0.0, 0.5), cv&#x3D;5, n_jobs&#x3D;1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="" alt="5"></p>
<h1 id="多种模型对比"><a href="#多种模型对比" class="headerlink" title="多种模型对比"></a>多种模型对比</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train &#x3D; sample_feature[continuous_feature_names + [&#39;price&#39;]].dropna()</span><br><span class="line"></span><br><span class="line">train_X &#x3D; train[continuous_feature_names]</span><br><span class="line">train_y &#x3D; train[&#39;price&#39;]</span><br><span class="line">train_y_ln &#x3D; np.log(train_y + 1)</span><br></pre></td></tr></table></figure>

<h2 id="线性模型-amp-嵌入式特征选择"><a href="#线性模型-amp-嵌入式特征选择" class="headerlink" title="线性模型 &amp; 嵌入式特征选择"></a>线性模型 &amp; 嵌入式特征选择</h2><ul>
<li>本章节默认，学习者已经了解关于过拟合、模型复杂度、正则化等概念。否则请寻找相关资料或参考如下连接：</li>
</ul>
<blockquote>
<p>用简单易懂的语言描述「过拟合 overfitting」<a href="https://www.zhihu.com/question/32246256/answer/55320482" target="_blank" rel="noopener">https://www.zhihu.com/question/32246256/answer/55320482</a><br>模型复杂度与模型的泛化能力 <a href="http://yangyingming.com/article/434/" target="_blank" rel="noopener">http://yangyingming.com/article/434/</a><br>正则化的直观理解 <a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">https://blog.csdn.net/jinping_shi/article/details/52433975</a></p>
</blockquote>
<p>在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是L1正则化与L2正则化。在对线性回归模型加入两种正则化方法后，他们分别变成了Lasso回归与岭(Ridge)回归。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 线性模型 &amp; 嵌入式特征选择</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.linear_model import Ridge</span><br><span class="line">from sklearn.linear_model import Lasso</span><br><span class="line"></span><br><span class="line">models &#x3D; [LinearRegression(),</span><br><span class="line">          Ridge(),</span><br><span class="line">          Lasso()]</span><br><span class="line"></span><br><span class="line">result &#x3D; dict()</span><br><span class="line">for model in models:</span><br><span class="line">    model_name &#x3D; str(model).split(&#39;(&#39;)[0]</span><br><span class="line">    scores &#x3D; cross_val_score(model, X&#x3D;train_X, y&#x3D;train_y_ln, verbose&#x3D;0, cv &#x3D; 5, scoring&#x3D;make_scorer(mean_absolute_error))</span><br><span class="line">    result[model_name] &#x3D; scores</span><br><span class="line">    print(model_name + &#39; is finished&#39;)</span><br></pre></td></tr></table></figure>

<pre><code>LinearRegression is finished
Ridge is finished
Lasso is finished</code></pre><ul>
<li>对三种方法的效果对比</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 对三种方法的效果对比</span><br><span class="line">result &#x3D; pd.DataFrame(result)</span><br><span class="line">result.index &#x3D; [&#39;cv&#39; + str(x) for x in range(1, 6)]</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th></th>
<th>LinearRegression</th>
<th>Ridge</th>
<th>Lasso</th>
</tr>
</thead>
<tbody><tr>
<td>cv1</td>
<td>0.190792</td>
<td>0.194832</td>
<td>0.383899</td>
</tr>
<tr>
<td>cv2</td>
<td>0.193758</td>
<td>0.197632</td>
<td>0.381893</td>
</tr>
<tr>
<td>cv3</td>
<td>0.194132</td>
<td>0.198123</td>
<td>0.384090</td>
</tr>
<tr>
<td>cv4</td>
<td>0.191825</td>
<td>0.195670</td>
<td>0.380526</td>
</tr>
<tr>
<td>cv5</td>
<td>0.195758</td>
<td>0.199676</td>
<td>0.383611</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; LinearRegression().fit(train_X, train_y_ln)</span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line">sns.barplot(abs(model.coef_), continuous_feature_names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>intercept:18.75072374836874<br><img src="" alt="6"></p>
<p>L2正则化在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; Ridge().fit(train_X, train_y_ln)</span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line">sns.barplot(abs(model.coef_), continuous_feature_names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>intercept:4.671710763117783<br><img src="" alt="7"></p>
<p>L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。如下图，我们发现power与userd_time特征非常重要</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; Lasso().fit(train_X, train_y_ln)</span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line">sns.barplot(abs(model.coef_), continuous_feature_names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>intercept:8.672182470075398<br><img src="" alt="8"></p>
<p>除此之外，决策树通过信息熵或GINI指数选择分裂节点时，优先选择的分裂特征也更加重要，这同样是一种特征选择的方法。XGBoost与LightGBM模型中的model_importance指标正是基于此计算的</p>
<h2 id="非线性模型"><a href="#非线性模型" class="headerlink" title="非线性模型"></a>非线性模型</h2>]]></content>
      <categories>
        <category>数据挖掘及机器学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Datawhale零基础入门数据挖掘-Task3</title>
    <url>/2020/03/28/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task3/</url>
    <content><![CDATA[<ul>
<li>特征工程：对于特征进行进一步分析，并对于数据进行处理</li>
</ul>
<h1 id="常见的特征工程包括"><a href="#常见的特征工程包括" class="headerlink" title="常见的特征工程包括"></a>常见的特征工程包括</h1><ol>
<li>异常处理</li>
</ol>
<ul>
<li>通过箱线图（或 3-Sigma）分析删除异常值；</li>
<li>BOX-COX 转换（处理有偏分布）；</li>
<li>长尾截断；</li>
</ul>
<ol start="2">
<li>特征归一化/标准化：</li>
</ol>
<ul>
<li>标准化（转换为标准正态分布）；</li>
<li>归一化（抓换到 [0,1] 区间）；</li>
<li>针对幂律分布，可以采用公式：$log(\frac{1+x}{1+median})$</li>
</ul>
<ol start="3">
<li>数据分桶：</li>
</ol>
<ul>
<li>等频分桶；</li>
<li>等距分桶；</li>
<li>Best-KS 分桶（类似利用基尼指数进行二分类）；</li>
<li>卡方分桶；</li>
</ul>
<ol start="4">
<li>缺失值处理：</li>
</ol>
<ul>
<li>不处理（针对类似 XGBoost 等树模型）；</li>
<li>删除（缺失数据太多）；</li>
<li>插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等；</li>
<li>分箱，缺失值一个箱；</li>
</ul>
<ol start="5">
<li>特征构造：</li>
</ol>
<ul>
<li>构造统计量特征，报告计数、求和、比例、标准差等；</li>
<li>时间特征，包括相对时间和绝对时间，节假日，双休日等；</li>
<li>地理信息，包括分箱，分布编码等方法；</li>
<li>非线性变换，包括 log/ 平方/ 根号等；</li>
<li>特征组合，特征交叉；</li>
<li>仁者见仁，智者见智。</li>
</ul>
<ol start="6">
<li>特征筛选</li>
</ol>
<ul>
<li>过滤式（filter）：先对数据进行特征选择，然后在训练学习器，常见的方法有 Relief/方差选择发/相关系数法/卡方检验法/互信息法；</li>
<li>包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有 LVM（Las Vegas Wrapper） ；</li>
<li>嵌入式（embedding）：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有 lasso 回归；</li>
</ul>
<ol start="7">
<li>降维</li>
</ol>
<ul>
<li>PCA/ LDA/ ICA；</li>
<li>特征选择也是一种降维。</li>
</ul>
<h1 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">from operator import itemgetter</span><br><span class="line"></span><br><span class="line">Train_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&quot;, sep&#x3D;&quot; &quot;)</span><br><span class="line">Test_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&quot;, sep&#x3D;&quot; &quot;)</span><br><span class="line"></span><br><span class="line">print(Train_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(150000, 31)</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data.head())</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th></th>
<th>SaleID</th>
<th>name</th>
<th>regDate</th>
<th>model</th>
<th>…</th>
<th>v_11</th>
<th>v_12</th>
<th>v_13</th>
<th>v_14</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>736</td>
<td>20040402</td>
<td>30.0</td>
<td>…</td>
<td>2.804097</td>
<td>-2.420821</td>
<td>0.795292</td>
<td>0.914762</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2262</td>
<td>20030301</td>
<td>40.0</td>
<td>…</td>
<td>2.096338</td>
<td>-1.030483</td>
<td>-1.722674</td>
<td>0.245522</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>14874</td>
<td>20040403</td>
<td>115.0</td>
<td>…</td>
<td>1.803559</td>
<td>1.565330</td>
<td>-0.832687</td>
<td>-0.229963</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>71865</td>
<td>19960908</td>
<td>109.0</td>
<td>…</td>
<td>1.285940</td>
<td>-0.501868</td>
<td>-2.438353</td>
<td>-0.478699</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
<td>111080</td>
<td>20120103</td>
<td>110.0</td>
<td>…</td>
<td>0.910783</td>
<td>0.931110</td>
<td>2.834518</td>
<td>1.923482</td>
</tr>
</tbody></table>
<p>[5 rows x 31 columns]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data.columns)</span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;SaleID&apos;, &apos;name&apos;, &apos;regDate&apos;, &apos;model&apos;, &apos;brand&apos;, &apos;bodyType&apos;, &apos;fuelType&apos;,
       &apos;gearbox&apos;, &apos;power&apos;, &apos;kilometer&apos;, &apos;notRepairedDamage&apos;, &apos;regionCode&apos;,
       &apos;seller&apos;, &apos;offerType&apos;, &apos;creatDate&apos;, &apos;price&apos;, &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;, &apos;v_3&apos;,
       &apos;v_4&apos;, &apos;v_5&apos;, &apos;v_6&apos;, &apos;v_7&apos;, &apos;v_8&apos;, &apos;v_9&apos;, &apos;v_10&apos;, &apos;v_11&apos;, &apos;v_12&apos;,
       &apos;v_13&apos;, &apos;v_14&apos;],
      dtype=&apos;object&apos;)</code></pre><h1 id="删除异常值"><a href="#删除异常值" class="headerlink" title="删除异常值"></a>删除异常值</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 删除异常值</span><br><span class="line"># 这里我包装了一个异常值处理的代码，可以随便调用</span><br><span class="line">def outliers_proc(data, col_name, scale&#x3D;3):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    用于清洗异常值，默认用box_plot(scale&#x3D;3)进行清洗</span><br><span class="line">    :param data:接受 pandas 数据格式</span><br><span class="line">    :param col_name:pandas 列名</span><br><span class="line">    :param scale:尺度</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def box_plot_outliers(data_ser, box_scale):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        利用箱线图去除异常值</span><br><span class="line">        :param data_ser:接收 pandas.Series 数据格式</span><br><span class="line">        :param box_scale: 箱线图尺度 （规定大于上四分位数1.5倍四分位数差 的值，或者小于下四分位数1.5倍四分位数差的值，划为异常值）</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        iqr &#x3D; box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25)) # 3倍四分位数差</span><br><span class="line">        val_low &#x3D; data_ser.quantile(0.25) - iqr # 下限&#x3D;Q1-3IQR</span><br><span class="line">        val_up &#x3D; data_ser.quantile(0.75) + iqr # 上限&#x3D;Q3+3IQR</span><br><span class="line">        rule_low &#x3D; (data_ser &lt; val_low)</span><br><span class="line">        rule_up &#x3D; (data_ser &gt; val_up) # 返回 pandas.Series 中对应值的bool</span><br><span class="line">        return (rule_low, rule_up), (val_low, val_up)</span><br><span class="line"></span><br><span class="line">    data_n &#x3D; data.copy() # copy 数据</span><br><span class="line">    data_series &#x3D; data_n[col_name] # 返回指定 col_name 数据</span><br><span class="line">    rule, value &#x3D; box_plot_outliers(data_series, box_scale&#x3D;scale)</span><br><span class="line">    index &#x3D; np.arange(data_series.shape[0])[rule[0]|rule[1]] # 返回rule_low, rule_up中为True的下标的列表</span><br><span class="line">    print(&quot;Delete number is:&#123;&#125;&quot;.format(len(index))) # 打印下标列表中个数</span><br><span class="line">    data_n &#x3D; data_n.drop(index) # 删除(删除后下标没变)</span><br><span class="line">    data_n.reset_index(drop&#x3D;True, inplace&#x3D;True) # 重置索引（drop&#x3D;True删除原来的索引;inplace&#x3D;True当前修改状态应用到原来Series中）</span><br><span class="line">    print(&quot;Now column number is:&#123;&#125;&quot;.format(data_n.shape[0])) # 查看删除后的数据个数</span><br><span class="line">    index_low &#x3D; np.arange(data_series.shape[0])[rule[0]]</span><br><span class="line">    outliers &#x3D; data_series.iloc[index_low] # ilco-按下标进行索引</span><br><span class="line">    print(&quot;Description of data larger than the lower bound is:&quot;)</span><br><span class="line">    print(pd.Series(outliers).describe())</span><br><span class="line">    index_up &#x3D; np.arange(data_series.shape[0])[rule[1]]</span><br><span class="line">    outliers &#x3D; data_series.iloc[index_up]</span><br><span class="line">    print(&quot;Description of data larger than the upper bound is:&quot;)</span><br><span class="line">    print(pd.Series(outliers).describe())</span><br><span class="line"></span><br><span class="line">    fig, ax &#x3D; plt.subplots(1, 2, figsize&#x3D;(10, 7)) # 创建子图:1行2列</span><br><span class="line">    sns.boxplot(y&#x3D;data[col_name], data&#x3D;data, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[0]) # 箱线图</span><br><span class="line">    sns.boxplot(y&#x3D;data_n[col_name], data&#x3D;data_n, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[1])</span><br><span class="line">    plt.show()</span><br><span class="line">    return data_n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 我们可以删掉一些异常数据,以 power 为例</span><br><span class="line">## 这里删不删可以自行判断</span><br><span class="line">## 但是注意 Test 的数据不能删</span><br><span class="line">Train_data &#x3D; outliers_proc(Train_data, &quot;power&quot;, scale&#x3D;3)</span><br></pre></td></tr></table></figure>

<pre><code>Delete number is:963
Now column number is:149037
Description of data larger than the lower bound is:
count    0.0
mean     NaN
std      NaN
min      NaN
25%      NaN
50%      NaN
75%      NaN
max      NaN
Name: power, dtype: float64
Description of data larger than the upper bound is:
count      963.000000
mean       846.836968
std       1929.418081
min        376.000000
25%        400.000000
50%        436.000000
75%        514.000000
max      19312.000000
Name: power, dtype: float64</code></pre><p><img src="https://s1.ax1x.com/2020/03/28/GAATiR.png" alt="1"></p>
<h1 id="特征构造"><a href="#特征构造" class="headerlink" title="特征构造"></a>特征构造</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 训练集和测试集放在一起，方便构造特征</span><br><span class="line">Train_data[&quot;train&quot;] &#x3D; 1 # 添加新字段，并设置值为1</span><br><span class="line">Test_data[&quot;train&quot;] &#x3D; 1</span><br><span class="line">data &#x3D; pd.concat([Train_data,Test_data],ignore_index&#x3D;True) # 连接函数 ignore_index&#x3D;True重置索引</span><br><span class="line"></span><br><span class="line"># 使用时间：data[&quot;createDate&quot;] - data[&quot;regDate&quot;], 反应汽车使用时间，一般来说价格与使用时间成反比</span><br><span class="line"># 不过要注意, 数据里有时间出错的格式, 所以我们需要 errors &#x3D; &quot;coerce&quot;</span><br><span class="line">data[&quot;used_time&quot;] &#x3D; (pd.to_datetime(data[&quot;creatDate&quot;], format&#x3D;&quot;%Y%m%d&quot;, errors&#x3D;&quot;coerce&quot;) -</span><br><span class="line">pd.to_datetime(data[&quot;regDate&quot;],format&#x3D;&quot;%Y%m%d&quot;,errors&#x3D;&quot;coerce&quot;)).dt.days # to_datetime将参数转换为日期 dt.days每个元素的天数</span><br><span class="line"></span><br><span class="line"># 看一下空数据, 有 15k 个样本的时间有问题的, 我们可以选择删除, 也可以选择放着</span><br><span class="line"># 但是这里不建议删除, 因为删除缺失数据占总样本量过大, 7.5%</span><br><span class="line"># 我们可以先放着, 因为如果我们 XGBoost 之类的决策树, 其本身就能处理缺失值, 所以可以不用管</span><br><span class="line">print(data[&quot;used_time&quot;].isnull().sum())</span><br></pre></td></tr></table></figure>

<pre><code>15072</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 从邮编中提取城市信息, 相当于加入了先验知识</span><br><span class="line">#print(data[&quot;regionCode&quot;])</span><br><span class="line"># 增加city 字段, 并从 regionCode 值的倒数第三位切片(apply 对regionCode每个元素运行指定运算 lambda 匿名函数)</span><br><span class="line">data[&quot;city&quot;] &#x3D; data[&quot;regionCode&quot;].apply(lambda x : str(x)[:-3])</span><br><span class="line"></span><br><span class="line"># 计算某品牌的销售统计量, 还可以计算其他特征的统计量</span><br><span class="line"># 这里以 train 的数据计算统计量</span><br><span class="line">Train_gb &#x3D; Train_data.groupby(&quot;brand&quot;) # 分组</span><br><span class="line">all_info &#x3D; &#123;&#125;</span><br><span class="line">for kind, kind_data in Train_gb:</span><br><span class="line">    info &#x3D; &#123;&#125;</span><br><span class="line">    kind_data &#x3D; kind_data[kind_data[&quot;price&quot;] &gt; 0]</span><br><span class="line">    info[&quot;brand_amount&quot;] &#x3D; len(kind_data)</span><br><span class="line">    info[&quot;brand_price_max&quot;] &#x3D; kind_data.price.max()</span><br><span class="line">    info[&quot;brand_price_median&quot;] &#x3D; kind_data.price.median()</span><br><span class="line">    info[&quot;brand_price_min&quot;] &#x3D; kind_data.price.min()</span><br><span class="line">    info[&quot;brand_price_sum&quot;] &#x3D; kind_data.price.sum()</span><br><span class="line">    info[&quot;brand_price_std&quot;] &#x3D; kind_data.price.std() # 样本方差</span><br><span class="line">    info[&quot;brand_price_average&quot;] &#x3D; round(kind_data.price.sum() &#x2F; (len(kind_data)+1), 2) # round(2)取近似值保留两位数</span><br><span class="line">    all_info[kind] &#x3D; info</span><br><span class="line"></span><br><span class="line">brand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;:&quot;brand&quot;&#125;) # T转置   reset_index还原索引  rename并修改列名</span><br><span class="line">data &#x3D; data.merge(brand_fe, how&#x3D;&quot;left&quot;, on&#x3D;&quot;brand&quot;) # 合并数据 data链接在brand_fe &quot;brand&quot;字段左边</span><br></pre></td></tr></table></figure>

<h2 id="数据分桶"><a href="#数据分桶" class="headerlink" title="数据分桶"></a>数据分桶</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 数据分桶 以 power 为例</span><br><span class="line"># 这时候缺失值也进桶了</span><br><span class="line"># 为什么要分桶：</span><br><span class="line"># 1. 离散后稀疏向量内积乘法运算速度更快, 计算结果也方便存储, 容易扩展</span><br><span class="line"># 2. 离散后的特征对异常值更具鲁棒性, 如 age&gt;30 为 1 否则为 0 , 对于年龄为 200 的也不会对模型造成很大的干扰</span><br><span class="line"># 3. LR 属于广义线性模型, 表达能力有限, 经过离散化后, 每个变量有单独的权重, 这相当于引入了非线性, 能够提升模型的表达能力, 加大拟合</span><br><span class="line"># 4. 离散后特征可以进行特征交叉, 提升表达能力, 由 M+N 个变量变成　Ｍ*N 个变量, 进一步引入非线性, 提升了表达能力</span><br><span class="line"># 5. 特征离散后模型更稳定, 如用户年龄区间, 不会因为用户年龄长了一岁就变化</span><br><span class="line"># 当然还有很多原因,　LightGBM 在改进 XGBoost 时就增加了数据分桶, 增强了模型的泛化性</span><br><span class="line"></span><br><span class="line">bin &#x3D; [i*10 for i in range(31)]</span><br><span class="line">data[&quot;power_bin&quot;] &#x3D; pd.cut(data[&quot;power&quot;], bin, labels&#x3D;False) # 分桶 cut切分数据(必须是一维的) bin定义区间 labels&#x3D;False返回第几个bin（从0开始）</span><br><span class="line">print(data[[&quot;power_bin&quot;, &quot;power&quot;]].head())</span><br></pre></td></tr></table></figure>

<pre><code> power_bin  power
0        5.0     60
1        NaN      0
2       16.0    163
3       19.0    193
4        6.0     68</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除不需要的数据</span><br><span class="line">data &#x3D; data.drop([&quot;creatDate&quot;, &quot;regDate&quot;, &quot;regionCode&quot;], axis&#x3D;1) # drop函数默认删除行，列需要加axis &#x3D; 1</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(199037, 39)</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(data.columns)</span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;SaleID&apos;, &apos;name&apos;, &apos;model&apos;, &apos;brand&apos;, &apos;bodyType&apos;, &apos;fuelType&apos;, &apos;gearbox&apos;,
       &apos;power&apos;, &apos;kilometer&apos;, &apos;notRepairedDamage&apos;, &apos;seller&apos;, &apos;offerType&apos;,
       &apos;price&apos;, &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;, &apos;v_3&apos;, &apos;v_4&apos;, &apos;v_5&apos;, &apos;v_6&apos;, &apos;v_7&apos;, &apos;v_8&apos;,
       &apos;v_9&apos;, &apos;v_10&apos;, &apos;v_11&apos;, &apos;v_12&apos;, &apos;v_13&apos;, &apos;v_14&apos;, &apos;train&apos;, &apos;used_time&apos;,
       &apos;city&apos;, &apos;brand_amount&apos;, &apos;brand_price_max&apos;, &apos;brand_price_median&apos;,
       &apos;brand_price_min&apos;, &apos;brand_price_sum&apos;, &apos;brand_price_std&apos;,
       &apos;brand_price_average&apos;, &apos;power_bin&apos;],
      dtype=&apos;object&apos;)</code></pre><h2 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 目前的数据其实已经可以给树模型使用了, 所以我们导出一下</span><br><span class="line">data.to_csv(&quot;data_for_tree.csv&quot;, index&#x3D;0) # index&#x3D;0不保存行索引</span><br></pre></td></tr></table></figure>
<h2 id="特征构造-1"><a href="#特征构造-1" class="headerlink" title="特征构造"></a>特征构造</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 我们可以再构造一份特征给 LR NN 之类的模型用</span><br><span class="line"># 之所以分开构造是因为, 不同模型对数据的要求不同</span><br><span class="line"># 先看下数据分布：</span><br><span class="line">data[&quot;power&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/Guw0jH.png" alt="2"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 我们刚刚已经对 train 进行异常值处理了，但是现在还有这么奇怪的分布是因为 test 中的 power 异常值，</span><br><span class="line"># 所以我们其实刚刚 train 中的 power 异常值不删为好，可以用长尾分布截断来代替</span><br><span class="line">Train_data[&quot;power&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/Guw6Et.png" alt="3"></p>
<h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 我们对其取 log, 再做归一化</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line"># 将数据的每一个特征缩放到给定的范围，将数据的每一个属性值减去其最小值，然后除以其极差（最大值 - 最小值）</span><br><span class="line">min_max_scaler &#x3D; preprocessing.MinMaxScaler()</span><br><span class="line">data[&quot;power&quot;] &#x3D; np.log(data[&quot;power&quot;] + 1)</span><br><span class="line"># 归一化：(0,1)标准化</span><br><span class="line">data[&quot;power&quot;] &#x3D; ((data[&quot;power&quot;] - np.min(data[&quot;power&quot;])) &#x2F; (np.max(data[&quot;power&quot;]) - np.min(data[&quot;power&quot;])))</span><br><span class="line">data[&quot;power&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/GuwcUP.png" alt="4"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># km 的比较正常, 应该已经做过分桶了</span><br><span class="line">data[&quot;kilometer&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/GuwRC8.png" alt="5"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 所以可以直接作归一化</span><br><span class="line">data[&quot;kilometer&quot;] &#x3D; ((data[&quot;kilometer&quot;] - np.min(data[&quot;kilometer&quot;])) &#x2F;</span><br><span class="line">                     (np.max(data[&quot;kilometer&quot;]) - np.min(data[&quot;kilometer&quot;])))</span><br><span class="line">data[&quot;kilometer&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/Guwfgg.png" alt="6"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 除此之外 还有我们刚刚构造的统计量特征：</span><br><span class="line"># &#39;brand_amount&#39;, &#39;brand_price_average&#39;, &#39;brand_price_max&#39;,</span><br><span class="line"># &#39;brand_price_median&#39;, &#39;brand_price_min&#39;, &#39;brand_price_std&#39;,</span><br><span class="line"># &#39;brand_price_sum&#39;</span><br><span class="line"># 这里不再一一举例分析了，直接做变换，</span><br><span class="line">def max_min(x):</span><br><span class="line">    return (x - np.min(x)) &#x2F; (np.max(x) - np.min(x))</span><br><span class="line"></span><br><span class="line">data[&#39;brand_amount&#39;] &#x3D; ((data[&#39;brand_amount&#39;] - np.min(data[&#39;brand_amount&#39;])) &#x2F;</span><br><span class="line">                        (np.max(data[&#39;brand_amount&#39;]) - np.min(data[&#39;brand_amount&#39;])))</span><br><span class="line">data[&#39;brand_price_average&#39;] &#x3D; ((data[&#39;brand_price_average&#39;] - np.min(data[&#39;brand_price_average&#39;])) &#x2F;</span><br><span class="line">                               (np.max(data[&#39;brand_price_average&#39;]) - np.min(data[&#39;brand_price_average&#39;])))</span><br><span class="line">data[&#39;brand_price_max&#39;] &#x3D; ((data[&#39;brand_price_max&#39;] - np.min(data[&#39;brand_price_max&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_max&#39;]) - np.min(data[&#39;brand_price_max&#39;])))</span><br><span class="line">data[&#39;brand_price_median&#39;] &#x3D; ((data[&#39;brand_price_median&#39;] - np.min(data[&#39;brand_price_median&#39;])) &#x2F;</span><br><span class="line">                              (np.max(data[&#39;brand_price_median&#39;]) - np.min(data[&#39;brand_price_median&#39;])))</span><br><span class="line">data[&#39;brand_price_min&#39;] &#x3D; ((data[&#39;brand_price_min&#39;] - np.min(data[&#39;brand_price_min&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_min&#39;]) - np.min(data[&#39;brand_price_min&#39;])))</span><br><span class="line">data[&#39;brand_price_std&#39;] &#x3D; ((data[&#39;brand_price_std&#39;] - np.min(data[&#39;brand_price_std&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_std&#39;]) - np.min(data[&#39;brand_price_std&#39;])))</span><br><span class="line">data[&#39;brand_price_sum&#39;] &#x3D; ((data[&#39;brand_price_sum&#39;] - np.min(data[&#39;brand_price_sum&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_sum&#39;]) - np.min(data[&#39;brand_price_sum&#39;])))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 对类别特征进行 OneEncoder</span><br><span class="line"></span><br><span class="line">data &#x3D; pd.get_dummies(data, columns&#x3D;[&#39;model&#39;, &#39;brand&#39;, &#39;bodyType&#39;, &#39;fuelType&#39;,</span><br><span class="line">                                     &#39;gearbox&#39;, &#39;notRepairedDamage&#39;, &#39;power_bin&#39;]) # 装换虚伪变量</span><br><span class="line"></span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(199037, 370)</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(data.columns)</span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;SaleID&apos;, &apos;name&apos;, &apos;power&apos;, &apos;kilometer&apos;, &apos;seller&apos;, &apos;offerType&apos;, &apos;price&apos;,
       &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;,
       ...
       &apos;power_bin_20.0&apos;, &apos;power_bin_21.0&apos;, &apos;power_bin_22.0&apos;, &apos;power_bin_23.0&apos;,
       &apos;power_bin_24.0&apos;, &apos;power_bin_25.0&apos;, &apos;power_bin_26.0&apos;, &apos;power_bin_27.0&apos;,
       &apos;power_bin_28.0&apos;, &apos;power_bin_29.0&apos;],
      dtype=&apos;object&apos;, length=370)</code></pre><h2 id="导出数据-1"><a href="#导出数据-1" class="headerlink" title="导出数据"></a>导出数据</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 这份数据可以给 LR 用</span><br><span class="line">data.to_csv(&quot;data_for_lr.csv&quot;, index&#x3D;0)</span><br></pre></td></tr></table></figure>

<h1 id="特征筛选"><a href="#特征筛选" class="headerlink" title="特征筛选"></a>特征筛选</h1><h2 id="过滤式"><a href="#过滤式" class="headerlink" title="过滤式"></a>过滤式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 1）过滤式</span><br><span class="line"># 相关性分析</span><br><span class="line">print(data[&#39;power&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;)) #spearman：非线性的，非正太分析的数据的相关系数</span><br><span class="line">print(data[&#39;kilometer&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line">print(data[&#39;brand_amount&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line">print(data[&#39;brand_price_average&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line">print(data[&#39;brand_price_max&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line">print(data[&#39;brand_price_median&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br></pre></td></tr></table></figure>

<pre><code>0.5728285196051496
-0.4082569701616764
0.058156610025581514
0.3834909576057687
0.259066833880992
0.38691042393409447</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 当然也可以直接看图</span><br><span class="line">data_numeric &#x3D; data[[&#39;power&#39;, &#39;kilometer&#39;, &#39;brand_amount&#39;, &#39;brand_price_average&#39;,</span><br><span class="line">                     &#39;brand_price_max&#39;, &#39;brand_price_median&#39;]]</span><br><span class="line">correlation &#x3D; data_numeric.corr() #返回data_numeric 相关性矩阵</span><br><span class="line"></span><br><span class="line">f, ax &#x3D; plt.subplots(figsize&#x3D;(7,7))</span><br><span class="line">plt.title(&quot;Correlation of Numeric Features with Price&quot;, y&#x3D;1, size&#x3D;16)</span><br><span class="line">square&#x3D;True # 将坐标轴方向设置为“equal”，以使每个单元格为方形 , vmax:色彩映射的值</span><br><span class="line">sns.heatmap(correlation, square&#x3D;True, vmax&#x3D;0.8)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/GuwIDs.png" alt="7"></p>
<h2 id="包裹式"><a href="#包裹式" class="headerlink" title="包裹式"></a>包裹式</h2><ul>
<li>下面的代码运行错误，看不懂<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># # 2)包裹式</span><br><span class="line">from mlxtend.feature_selection import SequentialFeatureSelector as SFS #序列特征算法的实现——贪婪搜索算法</span><br><span class="line">from sklearn.linear_model import  LinearRegression # 基于最小二乘法的线性回归</span><br><span class="line">sfs &#x3D; SFS(LinearRegression(), # 分类器或回归矩阵</span><br><span class="line">          k_features&#x3D;10, # 要选择的特征数量</span><br><span class="line">          forward&#x3D;True, #  如果为True，则向前选择，否则为反向选择</span><br><span class="line">          floating&#x3D;False, # 如果为True，则添加条件排除&#x2F;包含。</span><br><span class="line">          scoring&#x3D;&quot;r2&quot;, # 对于sklearn回归变量使用“ r2”</span><br><span class="line">          cv&#x3D;0) # 如果cv为None、False或0，则不进行交叉验证</span><br><span class="line">x &#x3D; data.drop([&quot;price&quot;], axis&#x3D;1)</span><br><span class="line">x &#x3D; x.fillna(0)</span><br><span class="line">y &#x3D; data[&quot;price&quot;]</span><br><span class="line"></span><br><span class="line">sfs.fit(x, y) # 执行特征选择并从训练数据中学习模型 x训练样本 y目标值</span><br><span class="line">sfs.k_feature_names_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 画出来，可以看到边际效益</span><br><span class="line">from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig1 &#x3D; plot_sfs(sfs.get_metric_dict(), kind&#x3D;&#39;std_dev&#39;)</span><br><span class="line">plt.grid()</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="嵌入式"><a href="#嵌入式" class="headerlink" title="嵌入式"></a>嵌入式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下一章介绍，Lasso 回归和决策树可以完成嵌入式特征选择</span><br><span class="line"># 大部分情况下都是用嵌入式做特征筛选</span><br></pre></td></tr></table></figure>

<h1 id="代码片段"><a href="#代码片段" class="headerlink" title="代码片段"></a>代码片段</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">from operator import itemgetter</span><br><span class="line"></span><br><span class="line"># %matplotlib inline 在终端中可以代替plt。show() 直接生成图</span><br><span class="line"></span><br><span class="line">Train_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&quot;, sep&#x3D;&quot; &quot;)</span><br><span class="line">Test_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&quot;, sep&#x3D;&quot; &quot;)</span><br><span class="line"></span><br><span class="line"># print(Train_data.shape)</span><br><span class="line"># print(Train_data.head())</span><br><span class="line"></span><br><span class="line">#print(Train_data.columns)</span><br><span class="line"></span><br><span class="line">## 删除异常值</span><br><span class="line"># 这里我包装了一个异常值处理的代码，可以随便调用</span><br><span class="line">def outliers_proc(data, col_name, scale&#x3D;3):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    用于清洗异常值，默认用box_plot(scale&#x3D;3)进行清洗</span><br><span class="line">    :param data:接受 pandas 数据格式</span><br><span class="line">    :param col_name:pandas 列名</span><br><span class="line">    :param scale:尺度</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def box_plot_outliers(data_ser, box_scale):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        利用箱线图去除异常值</span><br><span class="line">        :param data_ser:接收 pandas.Series 数据格式</span><br><span class="line">        :param box_scale: 箱线图尺度 （规定大于上四分位数1.5倍四分位数差 的值，或者小于下四分位数1.5倍四分位数差的值，划为异常值）</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        iqr &#x3D; box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25)) # 3倍四分位数差</span><br><span class="line">        val_low &#x3D; data_ser.quantile(0.25) - iqr # 下限&#x3D;Q1-3IQR</span><br><span class="line">        val_up &#x3D; data_ser.quantile(0.75) + iqr # 上限&#x3D;Q3+3IQR</span><br><span class="line">        rule_low &#x3D; (data_ser &lt; val_low)</span><br><span class="line">        rule_up &#x3D; (data_ser &gt; val_up) # 返回 pandas.Series 中对应值的bool</span><br><span class="line">        return (rule_low, rule_up), (val_low, val_up)</span><br><span class="line"></span><br><span class="line">    data_n &#x3D; data.copy() # copy 数据</span><br><span class="line">    data_series &#x3D; data_n[col_name] # 返回指定 col_name 数据</span><br><span class="line">    rule, value &#x3D; box_plot_outliers(data_series, box_scale&#x3D;scale)</span><br><span class="line">    index &#x3D; np.arange(data_series.shape[0])[rule[0]|rule[1]] # 返回rule_low, rule_up中为True的下标的列表</span><br><span class="line">    #print(&quot;Delete number is:&#123;&#125;&quot;.format(len(index))) # 打印下标列表中个数</span><br><span class="line">    data_n &#x3D; data_n.drop(index) # 删除(删除后下标没变)</span><br><span class="line">    data_n.reset_index(drop&#x3D;True, inplace&#x3D;True) # 重置索引（drop&#x3D;True删除原来的索引;inplace&#x3D;True当前修改状态应用到原来Series中）</span><br><span class="line">    #print(&quot;Now column number is:&#123;&#125;&quot;.format(data_n.shape[0])) # 查看删除后的数据个数</span><br><span class="line">    index_low &#x3D; np.arange(data_series.shape[0])[rule[0]]</span><br><span class="line">    outliers &#x3D; data_series.iloc[index_low] # ilco-按下标进行索引</span><br><span class="line">    #print(&quot;Description of data larger than the lower bound is:&quot;)</span><br><span class="line">    #print(pd.Series(outliers).describe())</span><br><span class="line">    index_up &#x3D; np.arange(data_series.shape[0])[rule[1]]</span><br><span class="line">    outliers &#x3D; data_series.iloc[index_up]</span><br><span class="line">    #print(&quot;Description of data larger than the upper bound is:&quot;)</span><br><span class="line">    #print(pd.Series(outliers).describe())</span><br><span class="line"></span><br><span class="line">    #fig, ax &#x3D; plt.subplots(1, 2, figsize&#x3D;(10, 7)) # 创建子图:1行2列</span><br><span class="line">    #sns.boxplot(y&#x3D;data[col_name], data&#x3D;data, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[0]) # 箱线图</span><br><span class="line">    #sns.boxplot(y&#x3D;data_n[col_name], data&#x3D;data_n, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[1])</span><br><span class="line">    #plt.show()</span><br><span class="line">    return data_n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 我们可以删掉一些异常数据,以 power 为例</span><br><span class="line">## 这里删不删可以自行判断</span><br><span class="line">## 但是注意 Test 的数据不能删</span><br><span class="line">Train_data &#x3D; outliers_proc(Train_data, &quot;power&quot;, scale&#x3D;3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 特征构造</span><br><span class="line"># 训练集和测试集放在一起，方便构造特征</span><br><span class="line">Train_data[&quot;train&quot;] &#x3D; 1 # 添加新字段，并设置值为1</span><br><span class="line">Test_data[&quot;train&quot;] &#x3D; 1</span><br><span class="line">data &#x3D; pd.concat([Train_data,Test_data],ignore_index&#x3D;True) # 连接函数 ignore_index&#x3D;True重置索引</span><br><span class="line"></span><br><span class="line"># 使用时间：data[&quot;createDate&quot;] - data[&quot;regDate&quot;], 反应汽车使用时间，一般来说价格与使用时间成反比</span><br><span class="line"># 不过要注意, 数据里有时间出错的格式, 所以我们需要 errors &#x3D; &quot;coerce&quot;</span><br><span class="line">data[&quot;used_time&quot;] &#x3D; (pd.to_datetime(data[&quot;creatDate&quot;], format&#x3D;&quot;%Y%m%d&quot;, errors&#x3D;&quot;coerce&quot;) -</span><br><span class="line">pd.to_datetime(data[&quot;regDate&quot;],format&#x3D;&quot;%Y%m%d&quot;,errors&#x3D;&quot;coerce&quot;)).dt.days # to_datetime将参数转换为日期 dt.days每个元素的天数</span><br><span class="line"></span><br><span class="line"># 看一下空数据, 有 15k 个样本的时间有问题的, 我们可以选择删除, 也可以选择放着</span><br><span class="line"># 但是这里不建议删除, 因为删除缺失数据占总样本量过大, 7.5%</span><br><span class="line"># 我们可以先放着, 因为如果我们 XGBoost 之类的决策树, 其本身就能处理缺失值, 所以可以不用管</span><br><span class="line">#print(data[&quot;used_time&quot;].isnull().sum())</span><br><span class="line"></span><br><span class="line"># 从邮编中提取城市信息, 相当于加入了先验知识</span><br><span class="line">#print(data[&quot;regionCode&quot;])</span><br><span class="line"># 增加city 字段, 并从 regionCode 值的倒数第三位切片(apply 对regionCode每个元素运行指定运算 lambda 匿名函数)</span><br><span class="line">data[&quot;city&quot;] &#x3D; data[&quot;regionCode&quot;].apply(lambda x : str(x)[:-3])</span><br><span class="line"></span><br><span class="line"># 计算某品牌的销售统计量, 还可以计算其他特征的统计量</span><br><span class="line"># 这里以 train 的数据计算统计量</span><br><span class="line">Train_gb &#x3D; Train_data.groupby(&quot;brand&quot;) # 分组</span><br><span class="line">all_info &#x3D; &#123;&#125;</span><br><span class="line">for kind, kind_data in Train_gb:</span><br><span class="line">    info &#x3D; &#123;&#125;</span><br><span class="line">    kind_data &#x3D; kind_data[kind_data[&quot;price&quot;] &gt; 0]</span><br><span class="line">    info[&quot;brand_amount&quot;] &#x3D; len(kind_data)</span><br><span class="line">    info[&quot;brand_price_max&quot;] &#x3D; kind_data.price.max()</span><br><span class="line">    info[&quot;brand_price_median&quot;] &#x3D; kind_data.price.median()</span><br><span class="line">    info[&quot;brand_price_min&quot;] &#x3D; kind_data.price.min()</span><br><span class="line">    info[&quot;brand_price_sum&quot;] &#x3D; kind_data.price.sum()</span><br><span class="line">    info[&quot;brand_price_std&quot;] &#x3D; kind_data.price.std() # 样本方差</span><br><span class="line">    info[&quot;brand_price_average&quot;] &#x3D; round(kind_data.price.sum() &#x2F; (len(kind_data)+1), 2) # round(2)取近似值保留两位数</span><br><span class="line">    all_info[kind] &#x3D; info</span><br><span class="line"></span><br><span class="line">brand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;:&quot;brand&quot;&#125;) # T转置   reset_index还原索引  rename并修改列名</span><br><span class="line">data &#x3D; data.merge(brand_fe, how&#x3D;&quot;left&quot;, on&#x3D;&quot;brand&quot;) # 合并数据 data链接在brand_fe &quot;brand&quot;字段左边</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据分桶 以 power 为例</span><br><span class="line"># 这时候缺失值也进桶了</span><br><span class="line"># 为什么要分桶：</span><br><span class="line"># 1. 离散后稀疏向量内积乘法运算速度更快, 计算结果也方便存储, 容易扩展</span><br><span class="line"># 2. 离散后的特征对异常值更具鲁棒性, 如 age&gt;30 为 1 否则为 0 , 对于年龄为 200 的也不会对模型造成很大的干扰</span><br><span class="line"># 3. LR 属于广义线性模型, 表达能力有限, 经过离散化后, 每个变量有单独的权重, 这相当于引入了非线性, 能够提升模型的表达能力, 加大拟合</span><br><span class="line"># 4. 离散后特征可以进行特征交叉, 提升表达能力, 由 M+N 个变量变成　Ｍ*N 个变量, 进一步引入非线性, 提升了表达能力</span><br><span class="line"># 5. 特征离散后模型更稳定, 如用户年龄区间, 不会因为用户年龄长了一岁就变化</span><br><span class="line"># 当然还有很多原因,　LightGBM 在改进 XGBoost 时就增加了数据分桶, 增强了模型的泛化性</span><br><span class="line"></span><br><span class="line">bin &#x3D; [i*10 for i in range(31)]</span><br><span class="line">data[&quot;power_bin&quot;] &#x3D; pd.cut(data[&quot;power&quot;], bin, labels&#x3D;False) # 分桶 cut切分数据(必须是一维的) bin定义区间 labels&#x3D;False返回第几个bin（从0开始）</span><br><span class="line">#print(data[[&quot;power_bin&quot;, &quot;power&quot;]].head())</span><br><span class="line"></span><br><span class="line"># 删除不需要的数据</span><br><span class="line">data &#x3D; data.drop([&quot;creatDate&quot;, &quot;regDate&quot;, &quot;regionCode&quot;], axis&#x3D;1) # drop函数默认删除行，列需要加axis &#x3D; 1</span><br><span class="line">#print(data.shape)</span><br><span class="line">#print(data.columns)</span><br><span class="line"></span><br><span class="line"># 目前的数据其实已经可以给树模型使用了, 所以我们导出一下</span><br><span class="line">#data.to_csv(&quot;data_for_tree.csv&quot;, index&#x3D;0) # index&#x3D;0不保存行索引</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 我们可以再构造一份特征给 LR NN 之类的模型用</span><br><span class="line"># 之所以分开构造是因为, 不同模型对数据的要求不同</span><br><span class="line"># 先看下数据分布：</span><br><span class="line">#data[&quot;power&quot;].plot.hist()</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line"># 我们刚刚已经对 train 进行异常值处理了，但是现在还有这么奇怪的分布是因为 test 中的 power 异常值，</span><br><span class="line"># 所以我们其实刚刚 train 中的 power 异常值不删为好，可以用长尾分布截断来代替</span><br><span class="line">#Train_data[&quot;power&quot;].plot.hist()</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line"># 我们对其取 log, 再做归一化</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line"># 将数据的每一个特征缩放到给定的范围，将数据的每一个属性值减去其最小值，然后除以其极差（最大值 - 最小值）</span><br><span class="line">min_max_scaler &#x3D; preprocessing.MinMaxScaler()</span><br><span class="line">data[&quot;power&quot;] &#x3D; np.log(data[&quot;power&quot;] + 1)</span><br><span class="line"># 归一化：(0,1)标准化</span><br><span class="line">data[&quot;power&quot;] &#x3D; ((data[&quot;power&quot;] - np.min(data[&quot;power&quot;])) &#x2F; (np.max(data[&quot;power&quot;]) - np.min(data[&quot;power&quot;])))</span><br><span class="line">#data[&quot;power&quot;].plot.hist()</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line"># km 的比较正常, 应该已经做过分桶了</span><br><span class="line"># data[&quot;kilometer&quot;].plot.hist()</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># 所以可以直接作归一化</span><br><span class="line">data[&quot;kilometer&quot;] &#x3D; ((data[&quot;kilometer&quot;] - np.min(data[&quot;kilometer&quot;])) &#x2F;</span><br><span class="line">                     (np.max(data[&quot;kilometer&quot;]) - np.min(data[&quot;kilometer&quot;])))</span><br><span class="line">#data[&quot;kilometer&quot;].plot.hist()</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line"># 除此之外 还有我们刚刚构造的统计量特征：</span><br><span class="line"># &#39;brand_amount&#39;, &#39;brand_price_average&#39;, &#39;brand_price_max&#39;,</span><br><span class="line"># &#39;brand_price_median&#39;, &#39;brand_price_min&#39;, &#39;brand_price_std&#39;,</span><br><span class="line"># &#39;brand_price_sum&#39;</span><br><span class="line"># 这里不再一一举例分析了，直接做变换，</span><br><span class="line">def max_min(x):</span><br><span class="line">    return (x - np.min(x)) &#x2F; (np.max(x) - np.min(x))</span><br><span class="line"></span><br><span class="line">data[&#39;brand_amount&#39;] &#x3D; ((data[&#39;brand_amount&#39;] - np.min(data[&#39;brand_amount&#39;])) &#x2F;</span><br><span class="line">                        (np.max(data[&#39;brand_amount&#39;]) - np.min(data[&#39;brand_amount&#39;])))</span><br><span class="line">data[&#39;brand_price_average&#39;] &#x3D; ((data[&#39;brand_price_average&#39;] - np.min(data[&#39;brand_price_average&#39;])) &#x2F;</span><br><span class="line">                               (np.max(data[&#39;brand_price_average&#39;]) - np.min(data[&#39;brand_price_average&#39;])))</span><br><span class="line">data[&#39;brand_price_max&#39;] &#x3D; ((data[&#39;brand_price_max&#39;] - np.min(data[&#39;brand_price_max&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_max&#39;]) - np.min(data[&#39;brand_price_max&#39;])))</span><br><span class="line">data[&#39;brand_price_median&#39;] &#x3D; ((data[&#39;brand_price_median&#39;] - np.min(data[&#39;brand_price_median&#39;])) &#x2F;</span><br><span class="line">                              (np.max(data[&#39;brand_price_median&#39;]) - np.min(data[&#39;brand_price_median&#39;])))</span><br><span class="line">data[&#39;brand_price_min&#39;] &#x3D; ((data[&#39;brand_price_min&#39;] - np.min(data[&#39;brand_price_min&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_min&#39;]) - np.min(data[&#39;brand_price_min&#39;])))</span><br><span class="line">data[&#39;brand_price_std&#39;] &#x3D; ((data[&#39;brand_price_std&#39;] - np.min(data[&#39;brand_price_std&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_std&#39;]) - np.min(data[&#39;brand_price_std&#39;])))</span><br><span class="line">data[&#39;brand_price_sum&#39;] &#x3D; ((data[&#39;brand_price_sum&#39;] - np.min(data[&#39;brand_price_sum&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_sum&#39;]) - np.min(data[&#39;brand_price_sum&#39;])))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 对类别特征进行 OneEncoder</span><br><span class="line"></span><br><span class="line">data &#x3D; pd.get_dummies(data, columns&#x3D;[&#39;model&#39;, &#39;brand&#39;, &#39;bodyType&#39;, &#39;fuelType&#39;,</span><br><span class="line">                                     &#39;gearbox&#39;, &#39;notRepairedDamage&#39;, &#39;power_bin&#39;]) # 装换虚伪变量</span><br><span class="line"></span><br><span class="line">#print(data.shape)</span><br><span class="line">#print(data.columns)</span><br><span class="line"></span><br><span class="line"># 这份数据可以给 LR 用</span><br><span class="line">#data.to_csv(&quot;data_for_lr.csv&quot;, index&#x3D;0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 特征筛选</span><br><span class="line"># 1）过滤式</span><br><span class="line"># 相关性分析</span><br><span class="line"># print(data[&#39;power&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;)) #spearman：非线性的，非正太分析的数据的相关系数</span><br><span class="line"># print(data[&#39;kilometer&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"># print(data[&#39;brand_amount&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"># print(data[&#39;brand_price_average&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"># print(data[&#39;brand_price_max&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"># print(data[&#39;brand_price_median&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"></span><br><span class="line"># 当然也可以直接看图</span><br><span class="line"># data_numeric &#x3D; data[[&#39;power&#39;, &#39;kilometer&#39;, &#39;brand_amount&#39;, &#39;brand_price_average&#39;,</span><br><span class="line">#                      &#39;brand_price_max&#39;, &#39;brand_price_median&#39;]]</span><br><span class="line"># correlation &#x3D; data_numeric.corr() #返回data_numeric 相关性矩阵</span><br><span class="line">#</span><br><span class="line"># f, ax &#x3D; plt.subplots(figsize&#x3D;(7,7))</span><br><span class="line"># plt.title(&quot;Correlation of Numeric Features with Price&quot;, y&#x3D;1, size&#x3D;16)</span><br><span class="line"># square&#x3D;True # 将坐标轴方向设置为“equal”，以使每个单元格为方形 , vmax:色彩映射的值</span><br><span class="line"># sns.heatmap(correlation, square&#x3D;True, vmax&#x3D;0.8)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># # 2)包裹式</span><br><span class="line">from mlxtend.feature_selection import SequentialFeatureSelector as SFS #序列特征算法的实现——贪婪搜索算法</span><br><span class="line">from sklearn.linear_model import  LinearRegression # 基于最小二乘法的线性回归</span><br><span class="line">sfs &#x3D; SFS(LinearRegression(), # 分类器或回归矩阵</span><br><span class="line">          k_features&#x3D;10, # 要选择的特征数量</span><br><span class="line">          forward&#x3D;True, #  如果为True，则向前选择，否则为反向选择</span><br><span class="line">          floating&#x3D;False, # 如果为True，则添加条件排除&#x2F;包含。</span><br><span class="line">          scoring&#x3D;&quot;r2&quot;, # 对于sklearn回归变量使用“ r2”</span><br><span class="line">          cv&#x3D;0) # 如果cv为None、False或0，则不进行交叉验证</span><br><span class="line">x &#x3D; data.drop([&quot;price&quot;], axis&#x3D;1)</span><br><span class="line">x &#x3D; x.fillna(0)</span><br><span class="line">y &#x3D; data[&quot;price&quot;]</span><br><span class="line"></span><br><span class="line">sfs.fit(x, y) # 执行特征选择并从训练数据中学习模型 x训练样本 y目标值</span><br><span class="line">sfs.k_feature_names_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 画出来，可以看到边际效益</span><br><span class="line">from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig1 &#x3D; plot_sfs(sfs.get_metric_dict(), kind&#x3D;&#39;std_dev&#39;)</span><br><span class="line">plt.grid()</span><br></pre></td></tr></table></figure>

<h1 id="经验总结"><a href="#经验总结" class="headerlink" title="经验总结"></a>经验总结</h1><p>特征工程是比赛中最至关重要的的一块，特别的传统的比赛，大家的模型可能都差不多，调参带来的效果增幅是非常有限的，但特征工程的好坏往往会决定了最终的排名和成绩。</p>
<p>特征工程的主要目的还是在于将数据转换为能更好地表示潜在问题的特征，从而提高机器学习的性能。比如，异常值处理是为了去除噪声，填补缺失值可以加入先验知识等。</p>
<p>特征构造也属于特征工程的一部分，其目的是为了增强数据的表达。</p>
<p>有些比赛的特征是匿名特征，这导致我们并不清楚特征相互直接的关联性，这时我们就只有单纯基于特征进行处理，比如装箱，groupby，agg 等这样一些操作进行一些特征统计，此外还可以对特征进行进一步的 log，exp 等变换，或者对多个特征进行四则运算（如上面我们算出的使用时长），多项式组合等然后进行筛选。由于特性的匿名性其实限制了很多对于特征的处理，当然有些时候用 NN 去提取一些特征也会达到意想不到的良好效果。</p>
<p>对于知道特征含义（非匿名）的特征工程，特别是在工业类型比赛中，会基于信号处理，频域提取，峰度，偏度等构建更为有实际意义的特征，这就是结合背景的特征构建，在推荐系统中也是这样的，各种类型点击率统计，各时段统计，加用户属性的统计等等，这样一种特征构建往往要深入分析背后的业务逻辑或者说物理原理，从而才能更好的找到 magic。</p>
<p>当然特征工程其实是和模型结合在一起的，这就是为什么要为 LR NN 做分桶和特征归一化的原因，而对于特征的处理效果和特征重要性等往往要通过模型来验证。</p>
<p>总的来说，特征工程是一个入门简单，但想精通非常难的一件事。</p>
]]></content>
      <categories>
        <category>数据挖掘及机器学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Datawhale零基础入门数据挖掘-Task2</title>
    <url>/2020/03/24/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task2/</url>
    <content><![CDATA[<ul>
<li>EDA的价值主要在于熟悉数据集，了解数据集，对数据集进行验证来确定所获得数据集可以用于接下来的机器学习或者深度学习使用</li>
<li>当了解了数据集之后我们下一步就是要去了解变量间的相互关系以及变量与预测值之间的存在关系</li>
<li>进行数据处理以及特征工程,使数据集的结构和特征集让接下来的预测问题更加可靠</li>
</ul>
<h1 id="载入各种数据科学以及可视化库"><a href="#载入各种数据科学以及可视化库" class="headerlink" title="载入各种数据科学以及可视化库"></a>载入各种数据科学以及可视化库</h1><h2 id="载入各种数据科学以及可视化库-1"><a href="#载入各种数据科学以及可视化库-1" class="headerlink" title="载入各种数据科学以及可视化库"></a>载入各种数据科学以及可视化库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 导入warnings包，利用过滤器来实现忽略警告语句</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import missingno as msno</span><br></pre></td></tr></table></figure>
<h1 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## pd.set_option(&#39;display.max_columns&#39;, None)# 显示所有列</span><br><span class="line">## pd.set_option(&#39;display.max_row&#39;, None)# 显示所有行</span><br><span class="line">## 1)载入训练集和测试集</span><br><span class="line">Train_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&quot;, sep &#x3D; &quot; &quot;)</span><br><span class="line">Test_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&quot;, sep &#x3D; &quot; &quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>以下主要以Train_data为例<h2 id="简略观察数据"><a href="#简略观察数据" class="headerlink" title="简略观察数据"></a>简略观察数据</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 2)简略观察数据（head()+shape)</span><br><span class="line">print(Train_data.head().append(Train_data.tail()))</span><br></pre></td></tr></table></figure>

</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>SaleID</th>
<th>name</th>
<th>regDate</th>
<th>model</th>
<th>…</th>
<th>v_11</th>
<th>v_12</th>
<th>v_13</th>
<th>v_14</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>736</td>
<td>20040402</td>
<td>30.0</td>
<td>…</td>
<td>2.804097</td>
<td>-2.420821</td>
<td>0.795292</td>
<td>0.914762</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2262</td>
<td>20030301</td>
<td>40.0</td>
<td>…</td>
<td>2.096338</td>
<td>-1.030483</td>
<td>-1.722674</td>
<td>0.245522</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>14874</td>
<td>20040403</td>
<td>115.0</td>
<td>…</td>
<td>1.803559</td>
<td>1.565330</td>
<td>-0.832687</td>
<td>-0.229963</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>71865</td>
<td>19960908</td>
<td>109.0</td>
<td>…</td>
<td>1.285940</td>
<td>-0.501868</td>
<td>-2.438353</td>
<td>-0.478699</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
<td>111080</td>
<td>20120103</td>
<td>110.0</td>
<td>…</td>
<td>0.910783</td>
<td>0.931110</td>
<td>2.834518</td>
<td>1.923482</td>
</tr>
<tr>
<td>149995</td>
<td>149995</td>
<td>163978</td>
<td>20000607</td>
<td>121.0</td>
<td>…</td>
<td>-2.983973</td>
<td>0.589167</td>
<td>-1.304370</td>
<td>-0.302592</td>
</tr>
<tr>
<td>149996</td>
<td>149996</td>
<td>184535</td>
<td>20091102</td>
<td>116.0</td>
<td>…</td>
<td>-2.774615</td>
<td>2.553994</td>
<td>0.924196</td>
<td>-0.272160</td>
</tr>
<tr>
<td>149997</td>
<td>149997</td>
<td>147587</td>
<td>20101003</td>
<td>60.0</td>
<td>…</td>
<td>-1.630677</td>
<td>2.290197</td>
<td>1.891922</td>
<td>0.414931</td>
</tr>
<tr>
<td>149998</td>
<td>149998</td>
<td>45907</td>
<td>20060312</td>
<td>34.0</td>
<td>…</td>
<td>-2.633719</td>
<td>1.414937</td>
<td>0.431981</td>
<td>-1.659014</td>
</tr>
<tr>
<td>149999</td>
<td>149999</td>
<td>177672</td>
<td>19990204</td>
<td>19.0</td>
<td>…</td>
<td>-3.179913</td>
<td>0.031724</td>
<td>-1.483350</td>
<td>-0.342674</td>
</tr>
</tbody></table>
<p>[10 rows x 31 columns]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(150000, 31)</code></pre><h1 id="总览数据概况"><a href="#总览数据概况" class="headerlink" title="总览数据概况"></a>总览数据概况</h1><ol>
<li>describe种有每列的统计量，个数count、平均值mean、方差std、最小值min、中位数25% 50% 75% 、以及最大值 看这个信息主要是瞬间掌握数据的大概的范围以及每个值的异常值的判断，比如有的时候会发现999 9999 -1 等值这些其实都是nan的另外一种表达方式，有的时候需要注意下</li>
<li>info 通过info来了解数据每列的type，有助于了解是否存在除了nan以外的特殊符号异常</li>
</ol>
<h2 id="通过describe-来熟悉相关统计量"><a href="#通过describe-来熟悉相关统计量" class="headerlink" title="通过describe()来熟悉相关统计量"></a>通过describe()来熟悉相关统计量</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 3)通过describe()来熟悉相关统计量</span><br><span class="line">print(Train_data.describe())</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th></th>
<th>SaleID</th>
<th>name</th>
<th>…</th>
<th>v_13</th>
<th>v_14</th>
</tr>
</thead>
<tbody><tr>
<td>count</td>
<td>150000.000000</td>
<td>150000.000000</td>
<td>…</td>
<td>150000.000000</td>
<td>150000.000000</td>
</tr>
<tr>
<td>mean</td>
<td>74999.500000</td>
<td>68349.172873</td>
<td>…</td>
<td>0.000313</td>
<td>-0.000688</td>
</tr>
<tr>
<td>std</td>
<td>43301.414527</td>
<td>61103.875095</td>
<td>…</td>
<td>1.288988</td>
<td>1.038685</td>
</tr>
<tr>
<td>min</td>
<td>0.000000</td>
<td>0.000000</td>
<td>…</td>
<td>-4.153899</td>
<td>-6.546556</td>
</tr>
<tr>
<td>25%</td>
<td>37499.750000</td>
<td>11156.000000</td>
<td>…</td>
<td>-1.057789</td>
<td>-0.437034</td>
</tr>
<tr>
<td>50%</td>
<td>74999.500000</td>
<td>51638.000000</td>
<td>…</td>
<td>-0.036245</td>
<td>0.141246</td>
</tr>
<tr>
<td>75%</td>
<td>112499.250000</td>
<td>118841.250000</td>
<td>…</td>
<td>0.942813</td>
<td>0.680378</td>
</tr>
<tr>
<td>max</td>
<td>149999.000000</td>
<td>196812.000000</td>
<td>…</td>
<td>11.147669</td>
<td>8.658418</td>
</tr>
</tbody></table>
<p>[8 rows x 30 columns]</p>
<h2 id="通过info-来熟悉数据类型"><a href="#通过info-来熟悉数据类型" class="headerlink" title="通过info()来熟悉数据类型"></a>通过info()来熟悉数据类型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 4)通过info()来熟悉数据类型</span><br><span class="line">print(Train_data.info())</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 150000 entries, 0 to 149999
Data columns (total 31 columns):
 #   Column             Non-Null Count   Dtype  
---  ------             --------------   -----  
 0   SaleID             150000 non-null  int64  
 1   name               150000 non-null  int64  
 2   regDate            150000 non-null  int64  
 3   model              149999 non-null  float64
 4   brand              150000 non-null  int64  
 5   bodyType           145494 non-null  float64
 6   fuelType           141320 non-null  float64
 7   gearbox            144019 non-null  float64
 8   power              150000 non-null  int64  
 9   kilometer          150000 non-null  float64
 10  notRepairedDamage  150000 non-null  object 
 11  regionCode         150000 non-null  int64  
 12  seller             150000 non-null  int64  
 13  offerType          150000 non-null  int64  
 14  creatDate          150000 non-null  int64  
 15  price              150000 non-null  int64  
 16  v_0                150000 non-null  float64
 17  v_1                150000 non-null  float64
 18  v_2                150000 non-null  float64
 19  v_3                150000 non-null  float64
 20  v_4                150000 non-null  float64
 21  v_5                150000 non-null  float64
 22  v_6                150000 non-null  float64
 23  v_7                150000 non-null  float64
 24  v_8                150000 non-null  float64
 25  v_9                150000 non-null  float64
 26  v_10               150000 non-null  float64
 27  v_11               150000 non-null  float64
 28  v_12               150000 non-null  float64
 29  v_13               150000 non-null  float64
 30  v_14               150000 non-null  float64
dtypes: float64(20), int64(10), object(1)
memory usage: 35.5+ MB
None</code></pre><h1 id="判断数据缺失和异常"><a href="#判断数据缺失和异常" class="headerlink" title="判断数据缺失和异常"></a>判断数据缺失和异常</h1><h2 id="查看每列的存在nan情况"><a href="#查看每列的存在nan情况" class="headerlink" title="查看每列的存在nan情况"></a>查看每列的存在nan情况</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 5) 查看每列的存在nan情况</span><br><span class="line">print(Train_data.isnull().sum())</span><br></pre></td></tr></table></figure>

<pre><code>SaleID                  0
name                    0
regDate                 0
model                   1
brand                   0
bodyType             4506
fuelType             8680
gearbox              5981
power                   0
kilometer               0
notRepairedDamage       0
regionCode              0
seller                  0
offerType               0
creatDate               0
price                   0
v_0                     0
v_1                     0
v_2                     0
v_3                     0
v_4                     0
v_5                     0
v_6                     0
v_7                     0
v_8                     0
v_9                     0
v_10                    0
v_11                    0
v_12                    0
v_13                    0
v_14                    0
dtype: int64</code></pre><h2 id="nan可视化"><a href="#nan可视化" class="headerlink" title="nan可视化"></a>nan可视化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#nan可视化</span><br><span class="line">missing &#x3D; Train_data.isnull().sum()</span><br><span class="line">missing &#x3D; missing[missing &gt; 0]</span><br><span class="line">missing.sort_values(inplace&#x3D;True) # 排序</span><br><span class="line">missing.plot.bar() # 绘柱状图</span><br><span class="line">plt.tight_layout() # 自动调整子图参数</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpS1ts.png" alt="1"></p>
<ul>
<li>通过以上可以很直观的了解哪些列存在 “nan”, 并可以把nan的个数打印，主要的目的在于 nan存在的个数是否真的很大，如果很小一般选择填充，如果使用lgb等树模型可以直接空缺，让树自己去优化，但如果nan存在的过多、可以考虑删掉</li>
</ul>
<h2 id="可视化看下缺省值"><a href="#可视化看下缺省值" class="headerlink" title="可视化看下缺省值"></a>可视化看下缺省值</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 可视化看下缺省值</span><br><span class="line">msno.matrix(Train_data.sample(250))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/Gp9H0S.png" alt="2"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">msno.bar(Train_data.sample(1000)) # 条形图</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/Gppugx.png" alt="3"></p>
<h2 id="查看异常值检测"><a href="#查看异常值检测" class="headerlink" title="查看异常值检测"></a>查看异常值检测</h2><ul>
<li><p>通过前面info()来熟悉数据类型，可以发现除了notRepairedDamage 为object类型其他都为数字 这里我们把他的几个不同的值都进行显示就知道了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data[&quot;notRepairedDamage&quot;].value_counts()) # 返回包含值和count</span><br></pre></td></tr></table></figure>

<p>  0.0    111361<br>  <code>-</code>       24324<br>  1.0     14315<br>  Name: notRepairedDamage, dtype: int64</p>
</li>
<li><p>可以看出来‘ - ’也为空缺值，因为很多模型对nan有直接的处理，这里我们先不做处理，先替换成nan</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Train_data[&quot;notRepairedDamage&quot;].replace(&quot;-&quot;, np.nan, inplace&#x3D;True) # 将数据中‘-’替换成nan值</span><br><span class="line">print(Train_data[&quot;notRepairedDamage&quot;].value_counts())</span><br></pre></td></tr></table></figure>

<pre><code>0.0    111361
1.0     14315
Name: notRepairedDamage, dtype: int64</code></pre><ul>
<li>再查看nan值情况</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data.isnull().sum())</span><br></pre></td></tr></table></figure>

<pre><code>SaleID                   0
name                     0
regDate                  0
model                    1
brand                    0
bodyType              4506
fuelType              8680
gearbox               5981
power                    0
kilometer                0
notRepairedDamage    24324
regionCode               0
seller                   0
offerType                0
creatDate                0
price                    0
v_0                      0
v_1                      0
v_2                      0
v_3                      0
v_4                      0
v_5                      0
v_6                      0
v_7                      0
v_8                      0
v_9                      0
v_10                     0
v_11                     0
v_12                     0
v_13                     0
v_14                     0
dtype: int64</code></pre><ul>
<li>以下两个类别特征严重倾斜，一般不会对预测有什么帮助，故这边先删掉，当然你也可以继续挖掘，但是一般意义不大</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data[&quot;seller&quot;].value_counts())</span><br></pre></td></tr></table></figure>

<pre><code>0    149999
1         1
Name: seller, dtype: int64</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data[&quot;offerType&quot;].value_counts())</span><br></pre></td></tr></table></figure>

<pre><code>0    150000
Name: offerType, dtype: int64</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除严重倾斜的数据</span><br><span class="line">del Train_data[&quot;seller&quot;]</span><br><span class="line">del Train_data[&quot;offerType&quot;]</span><br><span class="line">print(Train_data.info())</span><br><span class="line">print(Train_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 150000 entries, 0 to 149999
Data columns (total 29 columns):
 #   Column             Non-Null Count   Dtype  
---  ------             --------------   -----  
 0   SaleID             150000 non-null  int64  
 1   name               150000 non-null  int64  
 2   regDate            150000 non-null  int64  
 3   model              149999 non-null  float64
 4   brand              150000 non-null  int64  
 5   bodyType           145494 non-null  float64
 6   fuelType           141320 non-null  float64
 7   gearbox            144019 non-null  float64
 8   power              150000 non-null  int64  
 9   kilometer          150000 non-null  float64
 10  notRepairedDamage  150000 non-null  object 
 11  regionCode         150000 non-null  int64  
 12  creatDate          150000 non-null  int64  
 13  price              150000 non-null  int64  
 14  v_0                150000 non-null  float64
 15  v_1                150000 non-null  float64
 16  v_2                150000 non-null  float64
 17  v_3                150000 non-null  float64
 18  v_4                150000 non-null  float64
 19  v_5                150000 non-null  float64
 20  v_6                150000 non-null  float64
 21  v_7                150000 non-null  float64
 22  v_8                150000 non-null  float64
 23  v_9                150000 non-null  float64
 24  v_10               150000 non-null  float64
 25  v_11               150000 non-null  float64
 26  v_12               150000 non-null  float64
 27  v_13               150000 non-null  float64
 28  v_14               150000 non-null  float64
dtypes: float64(20), int64(8), object(1)
memory usage: 33.2+ MB
None

(150000, 29)</code></pre><h1 id="了解预测值的分布"><a href="#了解预测值的分布" class="headerlink" title="了解预测值的分布"></a>了解预测值的分布</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data[&quot;price&quot;])</span><br><span class="line">print(Train_data[&quot;price&quot;].value_counts())</span><br></pre></td></tr></table></figure>
<pre><code>0         1850
1         3600
2         6222
3         2400
4         5200
      ... 
149995    5900
149996    9500
149997    7500
149998    4999
149999    4700
Name: price, Length: 150000, dtype: int64

500      2337
1500     2158
1200     1922
1000     1850
2500     1821
     ... 
25321       1
8886        1
8801        1
37920       1
8188        1
Name: price, Length: 3763, dtype: int64</code></pre><h2 id="总体分布情况-无界约翰逊分布等）"><a href="#总体分布情况-无界约翰逊分布等）" class="headerlink" title="总体分布情况(无界约翰逊分布等）"></a>总体分布情况(无界约翰逊分布等）</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 1)总体分布情况(无界约翰逊分布等）</span><br><span class="line">import scipy.stats as st</span><br><span class="line">y &#x3D; Train_data[&quot;price&quot;]</span><br><span class="line">plt.figure(1); plt.title(&quot;Johnson SU&quot;) # 创建新图</span><br><span class="line">sns.distplot(y, kde&#x3D;False, fit&#x3D;st.johnsonsu)</span><br><span class="line">plt.figure(2); plt.title(&quot;Normal&quot;)</span><br><span class="line">sns.distplot(y, kde&#x3D;False, fit&#x3D;st.norm)</span><br><span class="line">plt.figure(3); plt.title(&quot;Log Normal&quot;)</span><br><span class="line">sns.distplot(y, kde&#x3D;False, fit&#x3D;st.lognorm)</span><br><span class="line">plt.show() # 最佳拟合是无界约翰逊分布</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpmBL9.png" alt="4"><br><img src="https://s1.ax1x.com/2020/03/26/GpmfQe.png" alt="5"><br><img src="https://s1.ax1x.com/2020/03/26/Gpm4Ld.png" alt="6"></p>
<ul>
<li>价格不服从正态分布，所以在进行回归之前，它必须进行转换。虽然对数变换做得很好，但最佳拟合是无界约翰逊分布</li>
</ul>
<h2 id="查看skewness-and-kurtosis"><a href="#查看skewness-and-kurtosis" class="headerlink" title="查看skewness and kurtosis"></a>查看skewness and kurtosis</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 2)查看skewness and kurtosis</span><br><span class="line">sns.distplot(Train_data[&quot;price&quot;])</span><br><span class="line">print(&quot;Skewness: %f&quot; % Train_data[&quot;price&quot;].skew()) # 偏度</span><br><span class="line">print(&quot;Kurtosis: %f&quot; % Train_data[&quot;price&quot;].kurt()) # 峰度</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Skewness: 3.346487
Kurtosis: 18.995183</code></pre><p><img src="https://s1.ax1x.com/2020/03/26/Gpu55t.png" alt="7"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data.skew())</span><br><span class="line">print(Train_data.kurt())</span><br></pre></td></tr></table></figure>

<pre><code>SaleID               6.017846e-17
name                 5.576058e-01
regDate              2.849508e-02
model                1.484388e+00
brand                1.150760e+00
bodyType             9.915299e-01
fuelType             1.595486e+00
gearbox              1.317514e+00
power                6.586318e+01
kilometer           -1.525921e+00
notRepairedDamage    2.430640e+00
regionCode           6.888812e-01
creatDate           -7.901331e+01
price                3.346487e+00
v_0                 -1.316712e+00
v_1                  3.594543e-01
v_2                  4.842556e+00
v_3                  1.062920e-01
v_4                  3.679890e-01
v_5                 -4.737094e+00
v_6                  3.680730e-01
v_7                  5.130233e+00
v_8                  2.046133e-01
v_9                  4.195007e-01
v_10                 2.522046e-02
v_11                 3.029146e+00
v_12                 3.653576e-01
v_13                 2.679152e-01
v_14                -1.186355e+00
dtype: float64

SaleID                 -1.200000
name                   -1.039945
regDate                -0.697308
model                   1.740483
brand                   1.076201
bodyType                0.206937
fuelType                5.880049
gearbox                -0.264161
power                5733.451054
kilometer               1.141934
notRepairedDamage       3.908072
regionCode             -0.340832
creatDate            6881.080328
price                  18.995183
v_0                     3.993841
v_1                    -1.753017
v_2                    23.860591
v_3                    -0.418006
v_4                    -0.197295
v_5                    22.934081
v_6                    -1.742567
v_7                    25.845489
v_8                    -0.636225
v_9                    -0.321491
v_10                   -0.577935
v_11                   12.568731
v_12                    0.268937
v_13                   -0.438274
v_14                    2.393526
dtype: float64</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sns.distplot(Train_data.skew(), color&#x3D;&quot;blue&quot;, axlabel&#x3D;&quot;Skewness&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/GpQ1UA.png" alt="8"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sns.distplot(Train_data.kurt(), color&#x3D;&quot;orange&quot;, axlabel&#x3D;&quot;Kurtness&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/GpQJ8P.png" alt="9"></p>
<ul>
<li>skew、kurt说明参考<a href="https://www.cnblogs.com/wyy1480/p/10474046.html" target="_blank" rel="noopener">https://www.cnblogs.com/wyy1480/p/10474046.html</a></li>
</ul>
<h2 id="查看预测值的具体频数"><a href="#查看预测值的具体频数" class="headerlink" title="查看预测值的具体频数"></a>查看预测值的具体频数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 3)查看预测值的具体频数</span><br><span class="line">plt.hist(Train_data[&quot;price&quot;], orientation&#x3D;&quot;vertical&quot;, histtype&#x3D;&quot;bar&quot;, color&#x3D;&quot;red&quot;)</span><br><span class="line">plt.show() # 直方图</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/Gp1rcV.png" alt="10"></p>
<ul>
<li>查看频数, 大于20000得值极少，其实这里也可以把这些当作特殊得值（异常值）直接用填充或者删掉，在前面进行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># log变换之后的分布比较均匀，可以进行log变换进行预测，这也是预测问题常用的trick</span><br><span class="line">plt.hist(np.log(Train_data[&quot;price&quot;]), orientation&#x3D;&quot;vertical&quot;, histtype&#x3D;&quot;bar&quot;, color&#x3D;&quot;red&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/Gp3Z3q.png" alt="11"></p>
<ul>
<li>log变换之后的分布较均匀，可以进行log变换进行预测，这也是预测问题常用的trick</li>
</ul>
<h1 id="特征分为类别特征和数字特征，并对类别特征查看nunique分布"><a href="#特征分为类别特征和数字特征，并对类别特征查看nunique分布" class="headerlink" title="特征分为类别特征和数字特征，并对类别特征查看nunique分布"></a>特征分为类别特征和数字特征，并对类别特征查看nunique分布</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ul>
<li>name - 汽车编码</li>
<li>regDate - 汽车注册时间</li>
<li>model - 车型编码</li>
<li>brand - 品牌</li>
<li>bodyType - 车身类型</li>
<li>fuelType - 燃油类型</li>
<li>gearbox - 变速箱</li>
<li>power - 汽车功率</li>
<li>kilometer - 汽车行驶公里</li>
<li>notRepairedDamage - 汽车有尚未修复的损坏</li>
<li>regionCode - 看车地区编码</li>
<li>seller - 销售方 【以删】</li>
<li>offerType - 报价类型 【以删】</li>
<li>creatDate - 广告发布时间</li>
<li>price - 汽车价格</li>
<li>v_0’, ‘v_1’, ‘v_2’, ‘v_3’, ‘v_4’, ‘v_5’, ‘v_6’, ‘v_7’, ‘v_8’, ‘v_9’, ‘v_10’, ‘v_11’, ‘v_12’, ‘v_13’,’v_14’【匿名特征，包含v0-14在内15个匿名特征】</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 分离label即预测值</span><br><span class="line">Y_train &#x3D; Train_data[&#39;price&#39;]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 这个区别方式适用于没有直接label coding的数据</span><br><span class="line"># 这里不适用，需要人为根据实际含义来区分</span><br><span class="line"># 数字特征</span><br><span class="line"># numeric_features &#x3D; Train_data.select_dtypes(include&#x3D;[np.number])</span><br><span class="line"># numeric_features.columns</span><br><span class="line"># # 类型特征</span><br><span class="line"># categorical_features &#x3D; Train_data.select_dtypes(include&#x3D;[np.object])</span><br><span class="line"># categorical_features.columns</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 数字特征</span><br><span class="line">#numeric_features &#x3D; [&#39;power&#39;, &#39;kilometer&#39;, &#39;v_0&#39;, &#39;v_1&#39;, &#39;v_2&#39;, &#39;v_3&#39;, &#39;v_4&#39;, &#39;v_5&#39;, &#39;v_6&#39;, &#39;v_7&#39;, &#39;v_8&#39;, &#39;v_9&#39;, &#39;v_10&#39;, &#39;v_11&#39;, &#39;v_12&#39;, &#39;v_13&#39;,&#39;v_14&#39; ]</span><br><span class="line"># 类别特征</span><br><span class="line">#categorical_features &#x3D; [&#39;name&#39;, &#39;model&#39;, &#39;brand&#39;, &#39;bodyType&#39;, &#39;fuelType&#39;, &#39;gearbox&#39;, &#39;notRepairedDamage&#39;, &#39;regionCode&#39;]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 类别特征nunique分布——Train_data</span><br><span class="line">for cat_fea in categorical_features:</span><br><span class="line">    print(cat_fea+&quot;的特征分布如下：&quot;)</span><br><span class="line">    print(&quot;&#123;&#125;特征有&#123;&#125;个不同的值&quot;.format(cat_fea, Train_data[cat_fea].nunique()))</span><br><span class="line">    print(Train_data[cat_fea].value_counts())</span><br></pre></td></tr></table></figure>


<pre><code>name的特征分布如下：
name特征有99662个不同的值
708       282
387       282
55        280
1541      263
203       233
     ... 
5074        1
7123        1
11221       1
13270       1
174485      1
Name: name, Length: 99662, dtype: int64

model的特征分布如下：
model特征有248个不同的值
0.0      11762
19.0      9573
4.0       8445
1.0       6038
29.0      5186
     ...  
245.0        2
209.0        2
240.0        2
242.0        2
247.0        1
Name: model, Length: 248, dtype: int64

brand的特征分布如下：
brand特征有40个不同的值
0     31480
4     16737
14    16089
10    14249
1     13794
6     10217
9      7306
5      4665
13     3817
11     2945
3      2461
7      2361
16     2223
8      2077
25     2064
27     2053
21     1547
15     1458
19     1388
20     1236
12     1109
22     1085
26      966
30      940
17      913
24      772
28      649
32      592
29      406
37      333
2       321
31      318
18      316
36      228
34      227
33      218
23      186
35      180
38       65
39        9
Name: brand, dtype: int64

bodyType的特征分布如下：
bodyType特征有8个不同的值
0.0    41420
1.0    35272
2.0    30324
3.0    13491
4.0     9609
5.0     7607
6.0     6482
7.0     1289
Name: bodyType, dtype: int64

fuelType的特征分布如下：
fuelType特征有7个不同的值
0.0    91656
1.0    46991
2.0     2212
3.0      262
4.0      118
5.0       45
6.0       36
Name: fuelType, dtype: int64

gearbox的特征分布如下：
gearbox特征有2个不同的值
0.0    111623
1.0     32396
Name: gearbox, dtype: int64

notRepairedDamage的特征分布如下：
notRepairedDamage特征有2个不同的值
0.0    111361
1.0     14315
Name: notRepairedDamage, dtype: int64

regionCode的特征分布如下：
regionCode特征有7905个不同的值
419     369
764     258
125     137
176     136
462     134
       ... 
6414      1
7063      1
4239      1
5931      1
7267      1
Name: regionCode, Length: 7905, dtype: int64</code></pre><h1 id="数字特征分析"><a href="#数字特征分析" class="headerlink" title="数字特征分析"></a>数字特征分析</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">numeric_features.append(&quot;price&quot;)</span><br><span class="line">print(numeric_features)</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;power&apos;, 
&apos;kilometer&apos;, 
&apos;v_0&apos;, 
&apos;v_1&apos;, 
&apos;v_2&apos;, 
&apos;v_3&apos;, 
&apos;v_4&apos;, 
&apos;v_5&apos;, 
&apos;v_6&apos;, 
&apos;v_7&apos;, 
&apos;v_8&apos;, 
&apos;v_9&apos;, 
&apos;v_10&apos;, 
&apos;v_11&apos;, 
&apos;v_12&apos;, 
&apos;v_13&apos;, 
&apos;v_14&apos;, 
&apos;price&apos;]</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data.head())</span><br></pre></td></tr></table></figure>


<table>
<thead>
<tr>
<th></th>
<th>SaleID</th>
<th>name</th>
<th>regDate</th>
<th>model</th>
<th>…</th>
<th>v_11</th>
<th>v_12</th>
<th>v_13</th>
<th>v_14</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>736</td>
<td>20040402</td>
<td>30.0</td>
<td>…</td>
<td>2.804097</td>
<td>-2.420821</td>
<td>0.795292</td>
<td>0.914762</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2262</td>
<td>20030301</td>
<td>40.0</td>
<td>…</td>
<td>2.096338</td>
<td>-1.030483</td>
<td>-1.722674</td>
<td>0.245522</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>14874</td>
<td>20040403</td>
<td>115.0</td>
<td>…</td>
<td>1.803559</td>
<td>1.565330</td>
<td>-0.832687</td>
<td>-0.229963</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>71865</td>
<td>19960908</td>
<td>109.0</td>
<td>…</td>
<td>1.285940</td>
<td>-0.501868</td>
<td>-2.438353</td>
<td>-0.478699</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
<td>111080</td>
<td>20120103</td>
<td>110.0</td>
<td>…</td>
<td>0.910783</td>
<td>0.931110</td>
<td>2.834518</td>
<td>1.923482</td>
</tr>
</tbody></table>
<p>[5 rows x 29 columns]</p>
<h2 id="相关性分析"><a href="#相关性分析" class="headerlink" title="相关性分析"></a>相关性分析</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 1)相关性分析</span><br><span class="line">price_numeric &#x3D; Train_data[numeric_features]</span><br><span class="line">correlation &#x3D; price_numeric.corr() # 返回一个相关系数的矩阵</span><br><span class="line">print(correlation[&quot;price&quot;].sort_values(ascending&#x3D;False),&quot;\n&quot;) # 降序排序</span><br></pre></td></tr></table></figure>



<pre><code>price        1.000000
v_12         0.692823
v_8          0.685798
v_0          0.628397
power        0.219834
v_5          0.164317
v_2          0.085322
v_6          0.068970
v_1          0.060914
v_14         0.035911
v_13        -0.013993
v_7         -0.053024
v_4         -0.147085
v_9         -0.206205
v_10        -0.246175
v_11        -0.275320
kilometer   -0.440519
v_3         -0.730946
Name: price, dtype: float64 </code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">f , ax &#x3D; plt.subplots(figsize &#x3D; (7, 7))</span><br><span class="line">plt.title(&quot;Correlation of Numeric Features with Price&quot;)</span><br><span class="line">sns.heatmap(correlation, square&#x3D;True, vmax&#x3D;0.8) # 热图（显示相关系数）</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpNCD0.png" alt="12"></p>
<h2 id="查看几个特征的偏度和峰度"><a href="#查看几个特征的偏度和峰度" class="headerlink" title="查看几个特征的偏度和峰度"></a>查看几个特征的偏度和峰度</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 2)查看几个特征的偏度和峰度</span><br><span class="line">for col in numeric_features:</span><br><span class="line">    print(&quot;&#123;:15&#125;&quot;.format(col),&quot;Skewness:&#123;:05.2f&#125;&quot;.format(Train_data[col].skew()),</span><br><span class="line">    &quot;   &quot;,</span><br><span class="line">          &quot;Kurtosis:&#123;:06.2f&#125;&quot;.format(Train_data[col].kurt()))</span><br></pre></td></tr></table></figure>


<pre><code>power           Skewness:65.86     Kurtosis:5733.45
kilometer       Skewness:-1.53     Kurtosis:001.14
v_0             Skewness:-1.32     Kurtosis:003.99
v_1             Skewness:00.36     Kurtosis:-01.75
v_2             Skewness:04.84     Kurtosis:023.86
v_3             Skewness:00.11     Kurtosis:-00.42
v_4             Skewness:00.37     Kurtosis:-00.20
v_5             Skewness:-4.74     Kurtosis:022.93
v_6             Skewness:00.37     Kurtosis:-01.74
v_7             Skewness:05.13     Kurtosis:025.85
v_8             Skewness:00.20     Kurtosis:-00.64
v_9             Skewness:00.42     Kurtosis:-00.32
v_10            Skewness:00.03     Kurtosis:-00.58
v_11            Skewness:03.03     Kurtosis:012.57
v_12            Skewness:00.37     Kurtosis:000.27
v_13            Skewness:00.27     Kurtosis:-00.44
v_14            Skewness:-1.19     Kurtosis:002.39
price           Skewness:03.35     Kurtosis:019.00</code></pre><h2 id="每个数字特征得分布可视化"><a href="#每个数字特征得分布可视化" class="headerlink" title="每个数字特征得分布可视化"></a>每个数字特征得分布可视化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 3)每个数字特征得分布可视化</span><br><span class="line">f &#x3D; pd.melt(Train_data, value_vars&#x3D;numeric_features) # 转换</span><br><span class="line">g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2, sharex&#x3D;False,sharey&#x3D;False) # 以”variable“作“格子&quot;绘图</span><br><span class="line"># plt.show()</span><br><span class="line">g &#x3D; g.map(sns.distplot, &quot;value&quot;) # 以”value“绘制到”格子”图中</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpNhZV.png" alt="13"></p>
<ul>
<li>可以看出匿名特征相对分布均匀</li>
</ul>
<h2 id="数字特征相互之间的关系可视化"><a href="#数字特征相互之间的关系可视化" class="headerlink" title="数字特征相互之间的关系可视化"></a>数字特征相互之间的关系可视化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 4）数字特征相互之间的关系可视化</span><br><span class="line">sns.set() # 风格设置</span><br><span class="line">colunms &#x3D; [&quot;price&quot;, &quot;v_12&quot;, &quot;v_8&quot;, &quot;v_0&quot;, &quot;power&quot;, &quot;v_5&quot;, &quot;v_2&quot;, &quot;v_6&quot;, &quot;v_1&quot;, &quot;v_14&quot;]</span><br><span class="line">sns.pairplot(Train_data[colunms],size&#x3D;2, kind&#x3D;&quot;scatter&quot;, diag_kind&#x3D;&quot;kde&quot;) # 多变量图</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpaMtO.png" alt="14"></p>
<h2 id="多变量互相回归关系可视化"><a href="#多变量互相回归关系可视化" class="headerlink" title="多变量互相回归关系可视化"></a>多变量互相回归关系可视化</h2><ul>
<li><p>此处是多变量之间的关系可视化，可视化更多学习可参考很不错的文章<a href="https://www.jianshu.com/p/6e18d21a4cad" target="_blank" rel="noopener">https://www.jianshu.com/p/6e18d21a4cad</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data.columns)</span><br></pre></td></tr></table></figure>

<p>  Index([‘SaleID’, ‘name’, ‘regDate’, ‘model’, ‘brand’, ‘bodyType’, ‘fuelType’,</p>
<pre><code> &apos;gearbox&apos;, &apos;power&apos;, &apos;kilometer&apos;, &apos;notRepairedDamage&apos;, &apos;regionCode&apos;,
 &apos;creatDate&apos;, &apos;price&apos;, &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;, &apos;v_3&apos;, &apos;v_4&apos;, &apos;v_5&apos;, &apos;v_6&apos;,
 &apos;v_7&apos;, &apos;v_8&apos;, &apos;v_9&apos;, &apos;v_10&apos;, &apos;v_11&apos;, &apos;v_12&apos;, &apos;v_13&apos;, &apos;v_14&apos;],
dtype=&apos;object&apos;)</code></pre></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Y_train)</span><br></pre></td></tr></table></figure>
<pre><code>0         1850
1         3600
2         6222
3         2400
4         5200
      ... 
149995    5900
149996    9500
149997    7500
149998    4999
149999    4700
Name: price, Length: 150000, dtype: int64</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 5)多变量互相关系回归关系可视化</span><br><span class="line">fig,((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) &#x3D; plt.subplots(nrows&#x3D;5, ncols&#x3D;2, figsize&#x3D;(24, 20)) # 生成5行2列十个子图</span><br><span class="line"># [&#39;v_12&#39;, &#39;v_8&#39; , &#39;v_0&#39;, &#39;power&#39;, &#39;v_5&#39;,  &#39;v_2&#39;, &#39;v_6&#39;, &#39;v_1&#39;, &#39;v_14&#39;]</span><br><span class="line">v_12_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&quot;v_12&quot;]], axis&#x3D;1) # 合并成一列</span><br><span class="line">#print(v_12_scatter_plot)</span><br><span class="line">sns.regplot(x&#x3D;&quot;v_12&quot;, y&#x3D;&quot;price&quot;, data&#x3D;v_12_scatter_plot,scatter&#x3D;True,fit_reg&#x3D;True,ax&#x3D;ax1) # 数据与回归模型拟合</span><br><span class="line"></span><br><span class="line">v_8_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_8&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_8&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_8_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax2)</span><br><span class="line"></span><br><span class="line">v_0_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_0&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_0&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_0_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax3)</span><br><span class="line"></span><br><span class="line">power_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;power&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;power&#39;,y &#x3D; &#39;price&#39;,data &#x3D; power_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax4)</span><br><span class="line"></span><br><span class="line">v_5_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_5&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_5&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_5_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax5)</span><br><span class="line"></span><br><span class="line">v_2_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_2&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_2&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_2_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax6)</span><br><span class="line"></span><br><span class="line">v_6_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_6&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_6&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_6_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax7)</span><br><span class="line"></span><br><span class="line">v_1_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_1&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_1&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_1_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax8)</span><br><span class="line"></span><br><span class="line">v_14_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_14&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_14&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_14_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax9)</span><br><span class="line"></span><br><span class="line">v_13_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_13&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_13&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_13_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax10)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/Gp04C8.png" alt="15"></p>
<h1 id="类别特征分析"><a href="#类别特征分析" class="headerlink" title="类别特征分析"></a>类别特征分析</h1><h2 id="nunique分布"><a href="#nunique分布" class="headerlink" title="nunique分布"></a>nunique分布</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 1）nunique分布</span><br><span class="line">for fea in categorical_features:</span><br><span class="line">    print(Train_data[fea].nunique())</span><br></pre></td></tr></table></figure>
<pre><code>99662
248
40
8
7
2
2
7905</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(categorical_features)</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;name&apos;, 
&apos;model&apos;, 
&apos;brand&apos;, 
&apos;bodyType&apos;, 
&apos;fuelType&apos;, 
&apos;gearbox&apos;, 
&apos;notRepairedDamage&apos;, 
&apos;regionCode&apos;]</code></pre><h2 id="类别特征箱形图可视化"><a href="#类别特征箱形图可视化" class="headerlink" title="类别特征箱形图可视化"></a>类别特征箱形图可视化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 2)类别箱形图可视化</span><br><span class="line"># 因为 name和 regionCode的类别太稀疏了，这里我们把不稀疏的几类画一下</span><br><span class="line">categorical_features &#x3D; [&quot;model&quot;,</span><br><span class="line">                        &quot;brand&quot;,</span><br><span class="line">                        &quot;bodyType&quot;,</span><br><span class="line">                        &quot;fuelType&quot;,</span><br><span class="line">                        &quot;gearbox&quot;,</span><br><span class="line">                        &quot;notRepairedDamage&quot;]</span><br><span class="line">for c in categorical_features:</span><br><span class="line">    Train_data[c] &#x3D; Train_data[c].astype(&quot;category&quot;) # 强制转换数据类型</span><br><span class="line">    if Train_data[c].isnull().any(): # 检查字段缺失</span><br><span class="line">        Train_data[c] &#x3D; Train_data[c].cat.add_categories([&quot;MISSING&quot;]) # 添加新类别</span><br><span class="line">        Train_data[c] &#x3D; Train_data[c].fillna(&quot;MISSING&quot;) # 填充为NAN的值</span><br><span class="line">def boxplot(x, y, **kwargs):</span><br><span class="line">    sns.boxplot(x&#x3D;x, y&#x3D;y) # 箱形图</span><br><span class="line">    x&#x3D;plt.xticks(rotation&#x3D;90) #  设置坐标轴</span><br><span class="line"></span><br><span class="line">f &#x3D; pd.melt(Train_data, id_vars&#x3D;[&quot;price&quot;], value_vars&#x3D;categorical_features)</span><br><span class="line">g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2, sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line">g &#x3D; g.map(boxplot, &quot;value&quot;, &quot;price&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/GpDs0A.png" alt="16"></p>
<h2 id="类别特征的小提琴图可视化"><a href="#类别特征的小提琴图可视化" class="headerlink" title="类别特征的小提琴图可视化"></a>类别特征的小提琴图可视化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(Train_data.columns)</span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;SaleID&apos;, &apos;name&apos;, &apos;regDate&apos;, &apos;model&apos;, &apos;brand&apos;, &apos;bodyType&apos;, &apos;fuelType&apos;,
       &apos;gearbox&apos;, &apos;power&apos;, &apos;kilometer&apos;, &apos;notRepairedDamage&apos;, &apos;regionCode&apos;,
       &apos;creatDate&apos;, &apos;price&apos;, &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;, &apos;v_3&apos;, &apos;v_4&apos;, &apos;v_5&apos;, &apos;v_6&apos;,
       &apos;v_7&apos;, &apos;v_8&apos;, &apos;v_9&apos;, &apos;v_10&apos;, &apos;v_11&apos;, &apos;v_12&apos;, &apos;v_13&apos;, &apos;v_14&apos;],
      dtype=&apos;object&apos;)</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 3)类别特征的小提琴图可视化</span><br><span class="line">catg_list &#x3D; categorical_features</span><br><span class="line">target &#x3D; &quot;price&quot;</span><br><span class="line">for catg in catg_list:</span><br><span class="line">    sns.violinplot(x&#x3D;catg,y&#x3D;target,data&#x3D;Train_data)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/GprvPP.png" alt="17"><br><img src="https://s1.ax1x.com/2020/03/26/GpsmxU.png" alt="18"><br><img src="https://s1.ax1x.com/2020/03/26/Gps1aR.png" alt="19"><br><img src="https://s1.ax1x.com/2020/03/26/GpymFI.png" alt="20"><br><img src="https://s1.ax1x.com/2020/03/26/GpynYt.png" alt="21"><br><img src="https://s1.ax1x.com/2020/03/26/GpyQl8.png" alt="22"></p>
<h2 id="类别特征的柱形图可视化"><a href="#类别特征的柱形图可视化" class="headerlink" title="类别特征的柱形图可视化"></a>类别特征的柱形图可视化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(categorical_features)</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;model&apos;, 
&apos;brand&apos;, 
&apos;bodyType&apos;, 
&apos;fuelType&apos;, 
&apos;gearbox&apos;, 
&apos;notRepairedDamage&apos;]</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 4)类别特征的柱形图可视化</span><br><span class="line">def bar_plot(x,y,**kwargs): # 柱形图</span><br><span class="line">    sns.barplot(x&#x3D;x,y&#x3D;y)</span><br><span class="line">    x&#x3D;plt.xticks(rotation&#x3D;90)</span><br><span class="line">f &#x3D; pd.melt(Train_data, id_vars&#x3D;[&quot;price&quot;], value_vars&#x3D;categorical_features)</span><br><span class="line">g &#x3D; sns.FacetGrid(f, col&#x3D;&quot;variable&quot;,col_wrap&#x3D;2,sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line">g &#x3D; g.map(bar_plot, &quot;value&quot;, &quot;price&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/Gp6mB4.png" alt="23"></p>
<h2 id="类别特征的每个类别频数可视化"><a href="#类别特征的每个类别频数可视化" class="headerlink" title="类别特征的每个类别频数可视化"></a>类别特征的每个类别频数可视化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 5)类别特征的每个类别频数可视化</span><br><span class="line">def count_plot(x,**kwargs): # 计数直方图</span><br><span class="line">    sns.countplot(x&#x3D;x)</span><br><span class="line">    x&#x3D;plt.xticks(rotation&#x3D;90)</span><br><span class="line">f &#x3D; pd.melt(Train_data,value_vars&#x3D;categorical_features)</span><br><span class="line">g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2,sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line">g &#x3D; g.map(count_plot,&quot;value&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/Gp6x8x.png" alt="24"></p>
<h2 id="用pandas-profiling生成数据报告"><a href="#用pandas-profiling生成数据报告" class="headerlink" title="用pandas_profiling生成数据报告"></a>用pandas_profiling生成数据报告</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 生成数据报告</span><br><span class="line">import pandas_profiling</span><br><span class="line"></span><br><span class="line">pfr &#x3D; pandas_profiling.ProfileReport(Train_data)</span><br><span class="line">pfr.to_file(&quot;.&#x2F;example.html&quot;)</span><br></pre></td></tr></table></figure>

<h1 id="代码片段"><a href="#代码片段" class="headerlink" title="代码片段"></a>代码片段</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 导入warnings包，利用过滤器来实现忽略警告语句</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import missingno as msno</span><br><span class="line"></span><br><span class="line">## pd.set_option(&#39;display.max_columns&#39;, None)# 显示所有列</span><br><span class="line">## pd.set_option(&#39;display.max_row&#39;, None)# 显示所有行</span><br><span class="line">## 1)载入训练集和测试集</span><br><span class="line">Train_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&quot;, sep &#x3D; &quot; &quot;)</span><br><span class="line">Test_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&quot;, sep &#x3D; &quot; &quot;)</span><br><span class="line"></span><br><span class="line">## 2)简略观察数据（head()+shape)</span><br><span class="line">#print(Train_data.head().append(Train_data.tail()))</span><br><span class="line">#print(Train_data.shape)</span><br><span class="line">#</span><br><span class="line"># ## 3)通过describe()来熟悉相关统计量</span><br><span class="line"># print(Train_data.describe())</span><br><span class="line">#</span><br><span class="line"># ## 4)通过info()来熟悉数据类型</span><br><span class="line"># print(Train_data.info())</span><br><span class="line">#</span><br><span class="line"># ## 5)判断数据缺失和异常</span><br><span class="line"># print(Train_data.isnull().sum())</span><br><span class="line">#</span><br><span class="line">#nan可视化</span><br><span class="line"># missing &#x3D; Train_data.isnull().sum()</span><br><span class="line"># missing &#x3D; missing[missing &gt; 0]</span><br><span class="line"># missing.sort_values(inplace&#x3D;True) # 排序</span><br><span class="line"># missing.plot.bar() # 绘柱状图</span><br><span class="line"># plt.tight_layout() # 自动调整子图参数</span><br><span class="line"># plt.show()</span><br><span class="line"># # # 可视化看下缺省值</span><br><span class="line"># msno.matrix(Train_data.sample(250))</span><br><span class="line"># # plt.show()</span><br><span class="line"># msno.bar(Train_data.sample(1000)) # 条形图</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 6)查看异常值检测</span><br><span class="line"># Train_data.info()</span><br><span class="line">## print(Train_data[&quot;notRepairedDamage&quot;].value_counts()) # 返回包含值和count</span><br><span class="line">Train_data[&quot;notRepairedDamage&quot;].replace(&quot;-&quot;, np.nan, inplace&#x3D;True) # 将数据中‘-’替换成nan值</span><br><span class="line"># print(Train_data.isnull().sum())</span><br><span class="line"></span><br><span class="line">#print(Train_data[&quot;notRepairedDamage&quot;].value_counts())</span><br><span class="line">#Test_data.info()</span><br><span class="line">##print(Test_data[&quot;notRepairedDamage&quot;].value_counts())</span><br><span class="line">#Test_data[&quot;notRepairedDamage&quot;].replace(&quot;-&quot;, np.nan, inplace&#x3D;True)</span><br><span class="line">##print(Test_data[&quot;notRepairedDamage&quot;].value_counts())</span><br><span class="line"></span><br><span class="line"># 删除严重倾斜的数据</span><br><span class="line">#print(Train_data[&quot;seller&quot;].value_counts())</span><br><span class="line">#print(Train_data[&quot;offerType&quot;].value_counts())</span><br><span class="line"># print(Test_data[&quot;seller&quot;].value_counts())</span><br><span class="line"># print(Test_data[&quot;offerType&quot;].value_counts())</span><br><span class="line"></span><br><span class="line">del Train_data[&quot;seller&quot;]</span><br><span class="line">del Train_data[&quot;offerType&quot;]</span><br><span class="line"># print(Train_data.info())</span><br><span class="line"># print(Train_data.shape)</span><br><span class="line">#del Test_data[&quot;seller&quot;]</span><br><span class="line">#del Test_data[&quot;offerType&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 了解预测值的分布</span><br><span class="line"># print(Train_data[&quot;price&quot;])</span><br><span class="line"># print(Train_data[&quot;price&quot;].value_counts())</span><br><span class="line"></span><br><span class="line">## 1)总体分布情况(无界约翰逊分布等）</span><br><span class="line">import scipy.stats as st</span><br><span class="line"># y &#x3D; Train_data[&quot;price&quot;]</span><br><span class="line"># plt.figure(1); plt.title(&quot;Johnson SU&quot;) # 创建新图</span><br><span class="line"># sns.distplot(y, kde&#x3D;False, fit&#x3D;st.johnsonsu)</span><br><span class="line"># plt.figure(2); plt.title(&quot;Normal&quot;)</span><br><span class="line"># sns.distplot(y, kde&#x3D;False, fit&#x3D;st.norm)</span><br><span class="line"># plt.figure(3); plt.title(&quot;Log Normal&quot;)</span><br><span class="line"># sns.distplot(y, kde&#x3D;False, fit&#x3D;st.lognorm)</span><br><span class="line"># plt.show() # 最佳拟合是无界约翰逊分布</span><br><span class="line"></span><br><span class="line">## 2)查看skewness and kurtosis</span><br><span class="line"># sns.distplot(Train_data[&quot;price&quot;])</span><br><span class="line"># print(&quot;Skewness: %f&quot; % Train_data[&quot;price&quot;].skew()) # 偏度</span><br><span class="line"># print(&quot;Kurtosis: %f&quot; % Train_data[&quot;price&quot;].kurt()) # 峰度</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># print(Train_data.skew())</span><br><span class="line"># print(Train_data.kurt())</span><br><span class="line"># sns.distplot(Train_data.skew(), color&#x3D;&quot;blue&quot;, axlabel&#x3D;&quot;Skewness&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"># sns.distplot(Train_data.kurt(), color&#x3D;&quot;orange&quot;, axlabel&#x3D;&quot;Kurtness&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># 3)查看预测值的具体频数</span><br><span class="line"># plt.hist(Train_data[&quot;price&quot;], orientation&#x3D;&quot;vertical&quot;, histtype&#x3D;&quot;bar&quot;, color&#x3D;&quot;red&quot;)</span><br><span class="line"># plt.show() # 直方图</span><br><span class="line"># log变换之后的分布比较均匀，可以进行log变换进行预测，这也是预测问题常用的trick</span><br><span class="line"># plt.hist(np.log(Train_data[&quot;price&quot;]), orientation&#x3D;&quot;vertical&quot;, histtype&#x3D;&quot;bar&quot;, color&#x3D;&quot;red&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 查看特征</span><br><span class="line"># 分离label即预测值</span><br><span class="line">Y_train &#x3D; Train_data[&quot;price&quot;]</span><br><span class="line">## 这个区别方式适用于没有直接label coding的数据</span><br><span class="line">## 这里不适用，需要人为根据实际含义来区分</span><br><span class="line">## 数字特征</span><br><span class="line">## numeric_features &#x3D; Train_data.select_dtypes(include&#x3D;[np.number])</span><br><span class="line">## numeric_features.columns</span><br><span class="line">## # 类型特征</span><br><span class="line">## categorical_features &#x3D; Train_data.select_dtypes(include&#x3D;[np.object])</span><br><span class="line">## categorical_features.columns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数字特征</span><br><span class="line">numeric_features &#x3D; [&#39;power&#39;, &#39;kilometer&#39;, &#39;v_0&#39;, &#39;v_1&#39;, &#39;v_2&#39;, &#39;v_3&#39;, &#39;v_4&#39;, &#39;v_5&#39;, &#39;v_6&#39;, &#39;v_7&#39;, &#39;v_8&#39;, &#39;v_9&#39;, &#39;v_10&#39;, &#39;v_11&#39;, &#39;v_12&#39;, &#39;v_13&#39;,&#39;v_14&#39; ]</span><br><span class="line"># 类别特征</span><br><span class="line">categorical_features &#x3D; [&#39;name&#39;, &#39;model&#39;, &#39;brand&#39;, &#39;bodyType&#39;, &#39;fuelType&#39;, &#39;gearbox&#39;, &#39;notRepairedDamage&#39;, &#39;regionCode&#39;]</span><br><span class="line">## 类别特征nunique分布——Train_data</span><br><span class="line"># for cat_fea in categorical_features:</span><br><span class="line">#     print(cat_fea+&quot;的特征分布如下：&quot;)</span><br><span class="line">#     print(&quot;&#123;&#125;特征有&#123;&#125;个不同的值&quot;.format(cat_fea, Train_data[cat_fea].nunique()))</span><br><span class="line">#     print(Train_data[cat_fea].value_counts())</span><br><span class="line">## 类别特征nunique分布——Test_data</span><br><span class="line"># for cat_fea in categorical_features:</span><br><span class="line">#     print(cat_fea+&quot;的特征分布如下：&quot;)</span><br><span class="line">#     print(&quot;&#123;&#125;特征有&#123;&#125;个不同的值&quot;.format(cat_fea, Test_data[cat_fea].nunique()))</span><br><span class="line">#     print(Test_data[cat_fea].value_counts())</span><br><span class="line"></span><br><span class="line">## 数字特征分析</span><br><span class="line">numeric_features.append(&quot;price&quot;)</span><br><span class="line"># print(numeric_features)</span><br><span class="line">#print(Train_data.head())</span><br><span class="line">## 1)相关性分析</span><br><span class="line">price_numeric &#x3D; Train_data[numeric_features]</span><br><span class="line">correlation &#x3D; price_numeric.corr() # 返回一个相关系数的矩阵</span><br><span class="line"># print(correlation[&quot;price&quot;].sort_values(ascending&#x3D;False),&quot;\n&quot;) # 降序排序</span><br><span class="line"></span><br><span class="line"># f , ax &#x3D; plt.subplots(figsize &#x3D; (7, 7))</span><br><span class="line"># plt.title(&quot;Correlation of Numeric Features with Price&quot;)</span><br><span class="line"># sns.heatmap(correlation, square&#x3D;True, vmax&#x3D;0.8) # 热图（显示相关系数）</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 2)查看几个特征的偏度和峰度</span><br><span class="line"># for col in numeric_features:</span><br><span class="line">#     print(&quot;&#123;:15&#125;&quot;.format(col),&quot;Skewness:&#123;:05.2f&#125;&quot;.format(Train_data[col].skew()),</span><br><span class="line">#     &quot;   &quot;,</span><br><span class="line">#           &quot;Kurtosis:&#123;:06.2f&#125;&quot;.format(Train_data[col].kurt()))</span><br><span class="line"></span><br><span class="line">## 3)每个数字特征得分布可视化</span><br><span class="line"># f &#x3D; pd.melt(Train_data, value_vars&#x3D;numeric_features) # 转换</span><br><span class="line"># g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2, sharex&#x3D;False,sharey&#x3D;False) # 以”variable“作“格子&quot;绘图</span><br><span class="line"># # plt.show()</span><br><span class="line"># g &#x3D; g.map(sns.distplot, &quot;value&quot;) # 以”value“绘制到”格子”图中</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 4）数字特征相互之间的关系可视化</span><br><span class="line"># sns.set() # 风格设置</span><br><span class="line"># colunms &#x3D; [&quot;price&quot;, &quot;v_12&quot;, &quot;v_8&quot;, &quot;v_0&quot;, &quot;power&quot;, &quot;v_5&quot;, &quot;v_2&quot;, &quot;v_6&quot;, &quot;v_1&quot;, &quot;v_14&quot;]</span><br><span class="line"># sns.pairplot(Train_data[colunms],size&#x3D;2, kind&#x3D;&quot;scatter&quot;, diag_kind&#x3D;&quot;kde&quot;) # 多变量图</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># print(Train_data.columns)</span><br><span class="line"># print(Y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 5)多变量互相关系回归关系可视化</span><br><span class="line"># fig,((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) &#x3D; plt.subplots(nrows&#x3D;5, ncols&#x3D;2, figsize&#x3D;(24, 20)) # 生成5行2列十个子图</span><br><span class="line"># # [&#39;v_12&#39;, &#39;v_8&#39; , &#39;v_0&#39;, &#39;power&#39;, &#39;v_5&#39;,  &#39;v_2&#39;, &#39;v_6&#39;, &#39;v_1&#39;, &#39;v_14&#39;]</span><br><span class="line"># v_12_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&quot;v_12&quot;]], axis&#x3D;1) # 合并成一列</span><br><span class="line"># #print(v_12_scatter_plot)</span><br><span class="line"># sns.regplot(x&#x3D;&quot;v_12&quot;, y&#x3D;&quot;price&quot;, data&#x3D;v_12_scatter_plot,scatter&#x3D;True,fit_reg&#x3D;True,ax&#x3D;ax1) # 数据与回归模型拟合</span><br><span class="line">#</span><br><span class="line"># v_8_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_8&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_8&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_8_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax2)</span><br><span class="line">#</span><br><span class="line"># v_0_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_0&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_0&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_0_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax3)</span><br><span class="line">#</span><br><span class="line"># power_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;power&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;power&#39;,y &#x3D; &#39;price&#39;,data &#x3D; power_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax4)</span><br><span class="line">#</span><br><span class="line"># v_5_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_5&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_5&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_5_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax5)</span><br><span class="line">#</span><br><span class="line"># v_2_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_2&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_2&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_2_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax6)</span><br><span class="line">#</span><br><span class="line"># v_6_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_6&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_6&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_6_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax7)</span><br><span class="line">#</span><br><span class="line"># v_1_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_1&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_1&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_1_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax8)</span><br><span class="line">#</span><br><span class="line"># v_14_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_14&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_14&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_14_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax9)</span><br><span class="line">#</span><br><span class="line"># v_13_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_13&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_13&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_13_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax10)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># 类别特征分析</span><br><span class="line">## 1）nunique分布</span><br><span class="line"># for fea in categorical_features:</span><br><span class="line">#     print(Train_data[fea].nunique())</span><br><span class="line">#</span><br><span class="line"># print(categorical_features)</span><br><span class="line"></span><br><span class="line">## 2)类别箱形图可视化</span><br><span class="line"># 因为 name和 regionCode的类别太稀疏了，这里我们把不稀疏的几类画一下</span><br><span class="line">categorical_features &#x3D; [&quot;model&quot;,</span><br><span class="line">                        &quot;brand&quot;,</span><br><span class="line">                        &quot;bodyType&quot;,</span><br><span class="line">                        &quot;fuelType&quot;,</span><br><span class="line">                        &quot;gearbox&quot;,</span><br><span class="line">                        &quot;notRepairedDamage&quot;]</span><br><span class="line">for c in categorical_features:</span><br><span class="line">    Train_data[c] &#x3D; Train_data[c].astype(&quot;category&quot;) # 强制转换数据类型</span><br><span class="line">    if Train_data[c].isnull().any(): # 检查字段缺失</span><br><span class="line">        Train_data[c] &#x3D; Train_data[c].cat.add_categories([&quot;MISSING&quot;]) # 添加新类别</span><br><span class="line">        Train_data[c] &#x3D; Train_data[c].fillna(&quot;MISSING&quot;) # 填充为NAN的值</span><br><span class="line"># def boxplot(x, y, **kwargs):</span><br><span class="line">#     sns.boxplot(x&#x3D;x, y&#x3D;y) # 箱形图</span><br><span class="line">#     x&#x3D;plt.xticks(rotation&#x3D;90) #  设置坐标轴</span><br><span class="line">#</span><br><span class="line"># f &#x3D; pd.melt(Train_data, id_vars&#x3D;[&quot;price&quot;], value_vars&#x3D;categorical_features)</span><br><span class="line"># g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2, sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line"># g &#x3D; g.map(boxplot, &quot;value&quot;, &quot;price&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 3)类别特征的小提琴图可视化</span><br><span class="line">#print(Train_data.columns)</span><br><span class="line"># catg_list &#x3D; categorical_features</span><br><span class="line"># target &#x3D; &quot;price&quot;</span><br><span class="line"># for catg in catg_list:</span><br><span class="line">#     sns.violinplot(x&#x3D;catg,y&#x3D;target,data&#x3D;Train_data)</span><br><span class="line">#     plt.show()</span><br><span class="line"></span><br><span class="line"># print(categorical_features)</span><br><span class="line"></span><br><span class="line">## 4)类别特征的柱形图可视化</span><br><span class="line"># def bar_plot(x,y,**kwargs): # 柱形图</span><br><span class="line">#     sns.barplot(x&#x3D;x,y&#x3D;y)</span><br><span class="line">#     x&#x3D;plt.xticks(rotation&#x3D;90)</span><br><span class="line"># f &#x3D; pd.melt(Train_data, id_vars&#x3D;[&quot;price&quot;], value_vars&#x3D;categorical_features)</span><br><span class="line"># g &#x3D; sns.FacetGrid(f, col&#x3D;&quot;variable&quot;,col_wrap&#x3D;2,sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line"># g &#x3D; g.map(bar_plot, &quot;value&quot;, &quot;price&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 5)类别特征的每个类别频数可视化</span><br><span class="line"># def count_plot(x,**kwargs): # 计数直方图</span><br><span class="line">#     sns.countplot(x&#x3D;x)</span><br><span class="line">#     x&#x3D;plt.xticks(rotation&#x3D;90)</span><br><span class="line"># f &#x3D; pd.melt(Train_data,value_vars&#x3D;categorical_features)</span><br><span class="line"># g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2,sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line"># g &#x3D; g.map(count_plot,&quot;value&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 生成数据报告</span><br><span class="line">import pandas_profiling</span><br><span class="line">#</span><br><span class="line"># pfr &#x3D; pandas_profiling.ProfileReport(Train_data)</span><br><span class="line"># pfr.to_file(&quot;.&#x2F;example.html&quot;)</span><br></pre></td></tr></table></figure>

<h1 id="经验总结"><a href="#经验总结" class="headerlink" title="经验总结"></a>经验总结</h1><p>所给出的EDA步骤为广为普遍的步骤，在实际的不管是工程还是比赛过程中，这只是最开始的一步，也是最基本的一步。</p>
<p>接下来一般要结合模型的效果以及特征工程等来分析数据的实际建模情况，根据自己的一些理解，查阅文献，对实际问题做出判断和深入的理解。</p>
<h2 id="最后不断进行EDA与数据处理和挖掘，来到达更好的数据结构和分布以及较为强势相关的特征"><a href="#最后不断进行EDA与数据处理和挖掘，来到达更好的数据结构和分布以及较为强势相关的特征" class="headerlink" title="最后不断进行EDA与数据处理和挖掘，来到达更好的数据结构和分布以及较为强势相关的特征"></a>最后不断进行EDA与数据处理和挖掘，来到达更好的数据结构和分布以及较为强势相关的特征</h2><p>数据探索在机器学习中我们一般称为EDA（Exploratory Data Analysis）：</p>
<blockquote>
<p>是指对已有的数据（特别是调查或观察得来的原始数据）在尽量少的先验假定下进行探索，通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律的一种数据分析方&gt;法。</p>
</blockquote>
<p>数据探索有利于我们发现数据的一些特性，数据之间的关联性，对于后续的特征构建是很有帮助的。</p>
<ol>
<li><p>对于数据的初步分析（直接查看数据，或.sum(), .mean()，.descirbe()等统计函数）可以从：样本数量，训练集数量，是否有时间特征，是否是时许问题，特征所表示的含义（非匿名特征），特征类型（字符类似，int，float，time），特征的缺失情况（注意缺失的在数据中的表现形式，有些是空的有些是”NAN”符号等），特征的均值方差情况。</p>
</li>
<li><p>分析记录某些特征值缺失占比30%以上样本的缺失处理，有助于后续的模型验证和调节，分析特征应该是填充（填充方式是什么，均值填充，0填充，众数填充等），还是舍去，还是先做样本分类用不同的特征模型去预测。</p>
</li>
<li><p>对于异常值做专门的分析，分析特征异常的label是否为异常值（或者偏离均值较远或者是特殊符号）,异常值是否应该剔除，还是用正常值填充，是记录异常，还是机器本身异常等。</p>
</li>
<li><p>对于Label做专门的分析，分析标签的分布情况等。</p>
</li>
<li><p>进步分析可以通过对特征作图，特征和label联合做图（统计图，离散图），直观了解特征的分布情况，通过这一步也可以发现数据之中的一些异常值等，通过箱型图分析一些特征值的偏离情况，对于特征和特征联合作图，对于特征和label联合作图，分析其中的一些关联性。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>数据挖掘及机器学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Datawhale零基础入门数据挖掘-Task1</title>
    <url>/2020/03/21/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task1/</url>
    <content><![CDATA[<ul>
<li><p>学习背景:由Datawhale与天池开放的零基础入门数据挖掘赛事-<a href="https://tianchi.aliyun.com/competition/entrance/231784/introduction?spm=5176.12281949.1003.2.493e2448KgHsEd" target="_blank" rel="noopener">二手车交易价格预测</a></p>
</li>
<li><p>赛题概括:赛题以预测二手车的交易价格为任务，数据集报名后可见并可下载，该数据来自某交易平台的二手车交易记录，总数据量超过40w，包含31列变量信息，其中15列为匿名变量。为了保证比赛的公平性，将会从中抽取15万条作为训练集，5万条作为测试集A，5万条作为测试集B，同时会对name、model、brand和regionCode等信息进行脱敏。</p>
</li>
</ul>
<h1 id="赛题分析"><a href="#赛题分析" class="headerlink" title="赛题分析"></a>赛题分析</h1><h2 id="数据概括"><a href="#数据概括" class="headerlink" title="数据概括"></a>数据概括</h2><p>一般而言，对于数据在比赛界面都有对应的数据概况介绍（匿名特征除外），说明列的性质特征。了解列的性质会有助于我们对于数据的理解和后续分析。 Tip:匿名特征，就是未告知数据列所属的性质的特征列。</p>
<blockquote>
<p>train.csv</p>
<ul>
<li>SaleID - 销售样本ID</li>
<li>name - 汽车编码</li>
<li>regDate - 汽车注册时间</li>
<li>model - 车型编码</li>
<li>brand - 品牌</li>
<li>bodyType - 车身类型</li>
<li>fuelType - 燃油类型</li>
<li>gearbox - 变速箱</li>
<li>power - 汽车功率</li>
<li>kilometer - 汽车行驶公里</li>
<li>notRepairedDamage - 汽车有尚未修复的损坏</li>
<li>regionCode - 看车地区编码</li>
<li>seller - 销售方</li>
<li>offerType - 报价类型</li>
<li>creatDate - 广告发布时间</li>
<li>price - 汽车价格</li>
<li>‘v_0’, ‘v_1’, ‘v_2’, ‘v_3’, ‘v_4’, ‘v_5’, ‘v_6’, ‘v_7’, ‘v_8’, ‘v_9’, ‘v_10’, ‘v_11’, ‘v_12’, ‘v_13’,’v_14’ 【匿名特征，包含v0-14在内15个匿名特征】 　</li>
</ul>
</blockquote>
<h2 id="评测标准"><a href="#评测标准" class="headerlink" title="评测标准"></a>评测标准</h2><p>赛题评价目标为MAE(Mean Absolute Error):</p>
<blockquote>
<p><img src="https://s1.ax1x.com/2020/03/21/8hVCfs.png" alt=""><br>MAE越小，说明模型预测得越准确</p>
</blockquote>
<h2 id="预测建模"><a href="#预测建模" class="headerlink" title="预测建模"></a>预测建模</h2><ul>
<li>预测建模就是使用历史数据建立一个模型，去给没有答案的新数据做预测的问题</li>
</ul>
<p>关于预测建模，可以在下面这篇文章中了解更多信息:</p>
<blockquote>
<p>Gentle Introduction to Predictive Modeling: <a href="https://machinelearningmastery.com/gentle-introduction-to-predictive-modeling/" target="_blank" rel="noopener">https://machinelearningmastery.com/gentle-introduction-to-predictive-modeling/</a></p>
</blockquote>
<p>预测建模可以被描述成一个近似求取从输入变量（X）到输出变量（y）的映射函数的数学问题。这被称为函数逼近问题</p>
<p>建模算法的任务就是在给定的可用时间和资源的限制下，去寻找最佳映射函数。更多关于机器学习中应用逼近函数的内容，请参阅下面这篇文章：</p>
<blockquote>
<p>机器学习是如何运行的（how machine learning work,<a href="https://machinelearningmastery.com/how-machine-learning-algorithms-work/" target="_blank" rel="noopener">https://machinelearningmastery.com/how-machine-learning-algorithms-work/</a>)</p>
</blockquote>
<p>一般而言，我们可以将函数逼近任务划分为分类任务和回归任务</p>
<h3 id="分类预测建模"><a href="#分类预测建模" class="headerlink" title="分类预测建模"></a>分类预测建模</h3><p>分类预测建模是逼近一个从输入变量（X）到离散的输出变量（y）之间的映射函数（f）</p>
<p>输出变量经常被称作标签或者类别。映射函数会对一个给定的观察样本预测一个类别标签</p>
<p>例如，一个文本邮件可以被归为两类：「垃圾邮件」，和「非垃圾邮件」</p>
<ul>
<li>分类问题需要把样本分为两类或者多类</li>
<li>分类的输入可以是实数也可以有离散变量</li>
<li>只有两个类别的分类问题经常被称作两类问题或者二元分类问题</li>
<li>具有多于两类的问题经常被称作多分类问题</li>
<li>样本属于多个类别的问题被称作多标签分类问题</li>
</ul>
<p>分类模型经常为输入样本预测得到与每一类别对应的像概率一样的连续值。这些概率可以被解释为样本属于每个类别的似然度或者置信度。预测到的概率可以通过选择概率最高的类别转换成类别标签</p>
<p>例如，某封邮件可能以 0.1 的概率被分为「垃圾邮件」，以 0.9 的概率被分为「非垃圾邮件」。因为非垃圾邮件的标签的概率最大，所以我们可以将概率转换成「非垃圾邮件」的标签</p>
<p>有很多用来衡量分类预测模型的性能的指标，但是分类准确率可能是最常用的一个</p>
<p>例如，如果一个分类预测模型做了 5 个预测，其中有 3 个是正确的，2 个这是错误的，那么这个模型的准确率就是 60%：</p>
<blockquote>
<p>accuracy = correct predictions / total predictions * 100<br>accuracy = 3 / 5 * 100<br>accuracy = 60%</p>
</blockquote>
<p>能够学习分类模型的算法就叫做分类算法</p>
<h3 id="回归预测模型"><a href="#回归预测模型" class="headerlink" title="回归预测模型"></a>回归预测模型</h3><p>回归预测建模是逼近一个从输入变量（X）到连续的输出变量（y）的函数映射</p>
<p>连续输出变量是一个实数，例如一个整数或者浮点数。这些变量通常是数量或者尺寸大小等等</p>
<p>例如，一座房子可能被预测到以 xx 美元出售，也许会在 $100,000 t 到$200,000 的范围内</p>
<ul>
<li>回归问题需要预测一个数量</li>
<li>回归的输入变量可以是连续的也可以是离散的</li>
<li>有多个输入变量的通常被称作多变量回归</li>
<li>输入变量是按照时间顺序的回归称为时间序列预测问题</li>
<li>因为回归预测问题预测的是一个数量，所以模型的性能可以用预测结果中的错误来评价</li>
</ul>
<p>有很多评价回归预测模型的方式，但是最常用的一个可能是计算误差值的均方根，即 RMSE</p>
<p>例如，如果回归预测模型做出了两个预测结果，一个是 1.5，对应的期望结果是 1.0；另一个是 3.3 对应的期望结果是 3.0. 那么，这两个回归预测的 RMSE 如下：</p>
<blockquote>
<p>RMSE = sqrt(average(error^2))<br>RMSE = sqrt(((1.0 - 1.5)^2 + (3.0 - 3.3)^2) / 2)<br>RMSE = sqrt((0.25 + 0.09) / 2)<br>RMSE = sqrt(0.17)<br>RMSE = 0.412</p>
</blockquote>
<p>使用 RMSE 的好处就是错误评分的单位与预测结果是一样的</p>
<p>一个能够学习回归预测模型的算法称作回归算法</p>
<p>有些算法的名字也有「regression,回归」一词，例如线性回归和 logistics 回归，这种情况有时候会让人迷惑因为线性回归确实是一个回归问题，但是 logistics 回归却是一个分类问题</p>
<h3 id="分类-vs-回归"><a href="#分类-vs-回归" class="headerlink" title="分类 vs 回归"></a>分类 vs 回归</h3><p>分类预测建模问题与回归预测建模问题是不一样的</p>
<ul>
<li>分类是预测一个离散标签的任务</li>
<li>回归是预测一个连续数量的任务</li>
</ul>
<p>分类和回归也有一些相同的地方：</p>
<ul>
<li>分类算法可能预测到一个连续的值，但是这些连续值对应的是一个类别的概率的形式</li>
<li>回归算法可以预测离散值，但是以整型量的形式预测离散值的</li>
</ul>
<p>有些算法既可以用来分类，也可以稍作修改就用来做回归问题，例如决策树和人工神经网络。但是一些算法就不行了——或者说是不太容易用于这两种类型的问题，例如线性回归是用来做回归预测建模的，logistics 回归是用来做分类预测建模的</p>
<p>重要的是，我们评价分类模型和预测模型的方式是不一样的，例如：</p>
<ul>
<li>分类预测可以使用准确率来评价，而回归问题则不能</li>
<li>回归预测可以使用均方根误差来评价，但是分类问题则不能</li>
</ul>
<h3 id="分类问题和回归问题之间的转换"><a href="#分类问题和回归问题之间的转换" class="headerlink" title="分类问题和回归问题之间的转换"></a>分类问题和回归问题之间的转换</h3><p>在一些情况中是可以将回归问题转换成分类问题的。例如，被预测的数量是可以被转换成离散数值的范围的</p>
<p>例如，在$0 到$100 之间的金额可以被分为两个区间：</p>
<ul>
<li>class 0：$0 到$49</li>
<li>class 1: $50 到$100</li>
</ul>
<p>这通常被称作离散化，结果中的输出变量是一个分类，分类的标签是有顺序的（称为叙序数）</p>
<p>在一些情况中，分类是可以转换成回归问题的。例如，一个标签可以被转换成一个连续的范围</p>
<p>一些算法早已通过为每一个类别预测一个概率，这个概率反过来又可以被扩展到一个特定的数值范围：</p>
<blockquote>
<p>quantity = min + probability * range</p>
</blockquote>
<p>与此对应，一个类别值也可以被序数化，并且映射到一个连续的范围中：</p>
<ul>
<li>$0 到 $49 是类别 1</li>
<li>$0 到 $49 是类别 2</li>
</ul>
<p>如果分类问题中的类别标签没有自然顺序的关系，那么从分类问题到回归问题的转换也许会导致奇怪的结果或者很差的性能，因为模型可能学到一个并不存在于从输入到连续输出之间的映射函数</p>
<p><em>原文链接</em><a href="https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/" target="_blank" rel="noopener">https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/</a></p>
<h2 id="关于评价指标"><a href="#关于评价指标" class="headerlink" title="关于评价指标"></a>关于评价指标</h2><ul>
<li>评估指标即是我们对于一个模型效果的数值型量化。（有点类似与对于一个商品评价打分，而这是针对于模型效果和理想效果之间的一个打分）</li>
</ul>
<p>一般来说分类和回归问题的评价指标有如下一些形式：</p>
<h3 id="分类算法常见的评估指标如下："><a href="#分类算法常见的评估指标如下：" class="headerlink" title="分类算法常见的评估指标如下："></a>分类算法常见的评估指标如下：</h3><blockquote>
<ul>
<li>对于二类分类器/分类算法，评价指标主要有accuracy， Precision，Recall，F-score，Pr曲线，ROC-AUC曲线</li>
<li>对于多类分类器/分类算法，评价指标主要有accuracy， 宏平均和微平均，F-score</li>
</ul>
</blockquote>
<h3 id="对于回归预测类常见的评估指标如下"><a href="#对于回归预测类常见的评估指标如下" class="headerlink" title="对于回归预测类常见的评估指标如下:"></a>对于回归预测类常见的评估指标如下:</h3><blockquote>
<ul>
<li>平均绝对误差（Mean Absolute Error，MAE），均方误差（Mean Squared Error，MSE），平均绝对百分误差（Mean Absolute Percentage Error，MAPE），均方根误差（Root Mean Squared Error）， R2（R-Square）</li>
</ul>
</blockquote>
<h4 id="平均绝对误差"><a href="#平均绝对误差" class="headerlink" title="平均绝对误差"></a>平均绝对误差</h4><ul>
<li>平均绝对误差（Mean Absolute Error，MAE）:其能更好地反映预测值与真实值误差的实际情况，其计算公式如下：<br>$$MAE=\frac{1}{N} \sum_{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|$$</li>
</ul>
<h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><ul>
<li>均方误差（Mean Squared Error，MSE）,均方误差,其计算公式为：<br>$$MSE=\frac{1}{N} \sum_{i}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}$$</li>
</ul>
<h3 id="R2（R-Square）"><a href="#R2（R-Square）" class="headerlink" title="R2（R-Square）"></a>R2（R-Square）</h3><ul>
<li>残差平方和:<br>$$SS_{res}=\sum\left(y_{i}-\hat{y}_{i}\right)^{2}$$</li>
<li>总平均值:<br>$$SS_{tot}=\sum\left(y_{i}-\overline{y}_{i}\right)^{2}$$</li>
<li>其中$\overline{y}$表示$y$的平均值得到$R^2$的表达式为:</li>
</ul>
<p>$$R^{2}=1-\frac{SS_{res}}{SS_{tot}}$$</p>
<p>$R^2$用于度量因变量的变异中可由自变量解释部分所占的比例，取值范围是 0~1，$R^2$越接近1,表明回归平方和占总平方和的比例越大,回归线与各观测点越接近，用x的变化来解释y值变化的部分就越多,回归的拟合程度就越好。所以$R^2$也称为拟合优度（Goodness of Fit）的统计量</p>
<p>$y_{i}$表示真实值,</p>
<p>$\hat{y}_{i}$表示预测值,</p>
<p>$\overline{y}_{i}$表示样本均值。得分越高拟合效果越好</p>
<h3 id="几何解释"><a href="#几何解释" class="headerlink" title="几何解释"></a>几何解释</h3><p><img src="https://s1.ax1x.com/2020/03/21/8hqPgA.png" alt=""><br>上图红色点是incoming自变量与Consuming因变量对应的散点图，蓝色线是回归方程线（最小二乘法得到）；<br>这里红色点$y_{i}$表示一个响应观测值点（共4个），蓝色点$f_{i}$是响应观测值对应的回归曲线上的点，两个的差值就是残差，残差值共有4个,$\overline{y}$是响应变量的平均值。</p>
<p>根据平方和分解公式:<br><img src="https://s1.ax1x.com/2020/03/21/8hquCQ.jpg" alt=""><br>即：SS 总体=SS 回归 + SS 残差 (观测值与平均值的差值平方和被残差平方和以及回归差值平方和之和解释)</p>
<h1 id="分析结果"><a href="#分析结果" class="headerlink" title="分析结果"></a>分析结果</h1><ol>
<li>此题为传统的数据挖掘问题，通过数据科学以及机器学习深度学习的办法来进行建模得到结果。</li>
<li>此题是一个典型的回归问题。</li>
<li>主要应用xgb、lgb、catboost，以及pandas、numpy、matplotlib、seabon、sklearn、keras等等数据挖掘常用库或者框架来进行数据挖掘任务。</li>
<li>通过EDA来挖掘数据的联系和自我熟悉数据</li>
</ol>
<h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"># 1) 载入训练集和测试集</span><br><span class="line">Train_data &#x3D; pd.read_csv(&#39;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&#39;,sep&#x3D;(&#39; &#39;))</span><br><span class="line">Test_data &#x3D; pd.read_csv(&#39;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&#39;,sep&#x3D;(&#39; &#39;))</span><br><span class="line"></span><br><span class="line">print(Train_data.shape) # 返回行,列数</span><br><span class="line">print(Test_data.shape)</span><br><span class="line">out_put &#x3D; Train_data.head(4) # 返回前四行字段数据</span><br><span class="line">print(out_put)</span><br><span class="line"></span><br><span class="line"># 2)分类指标评价计算</span><br><span class="line"></span><br><span class="line">## accuracy</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line">y_pred &#x3D; [0, 1, 0, 1] # 预测标签</span><br><span class="line">y_true &#x3D; [0, 1, 1, 1] # 正确标签</span><br><span class="line">print(&#39;ACC:&#39;,accuracy_score(y_true, y_pred)) # 返回正确样本所占比例（float）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">TP：真正例：即将正样本预测为正样本</span><br><span class="line">TN：真反例：即将负样本预测为负样本</span><br><span class="line">FP：假正例：将负样本预测为了正样本</span><br><span class="line">FN：假反例：将正样本预测为了负样本</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">## Percision,Recall,F1-score</span><br><span class="line">from sklearn import metrics</span><br><span class="line">y_pred &#x3D; [0, 1, 0, 0]</span><br><span class="line">y_true &#x3D; [0, 1, 0, 1]</span><br><span class="line">print(&#39;Precision&#39;,metrics.precision_score(y_true, y_pred)) # 返回 TP &#x2F; (TP + FP)</span><br><span class="line">print(&#39;Recall&#39;,metrics.recall_score(y_true, y_pred)) # 返回 TP &#x2F; (TP + FN)</span><br><span class="line">print(&#39;F1-score:&#39;,metrics.f1_score(y_true, y_pred)) # 返回 2*(P*R)&#x2F;(P+R)</span><br><span class="line"></span><br><span class="line">## AUC</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line">y_true &#x3D; np.array([0, 0, 1, 1]) # True labels or binary label indicators</span><br><span class="line">y_scores &#x3D; np.array([0.1, 0.4, 0.35, 0.8]) # Target scores</span><br><span class="line">print(&#39;AUC socre:&#39;,roc_auc_score(y_true, y_scores))</span><br><span class="line"></span><br><span class="line"># coding&#x3D;utf-8</span><br><span class="line"># import numpy as np</span><br><span class="line"># from sklearn import metrics</span><br><span class="line"></span><br><span class="line"># MAPE需要自己实现</span><br><span class="line">def mape(y_true, y_pred):</span><br><span class="line">    return np.mean(np.abs((y_pred - y_true) &#x2F; y_true))</span><br><span class="line"></span><br><span class="line">y_true &#x3D; np.array([1.0, 5.0, 4.0, 3.0, 2.0, 5.0, -3.0])</span><br><span class="line">y_pred &#x3D; np.array([1.0, 4.5, 3.8, 3.2, 3.0, 4.8, -2.2])</span><br><span class="line"></span><br><span class="line"># MSE</span><br><span class="line">print(&#39;MSE:&#39;,metrics.mean_squared_error(y_true, y_pred))</span><br><span class="line"># RMSE</span><br><span class="line">print(&#39;RMSE:&#39;,np.sqrt(metrics.mean_squared_error(y_true, y_pred)))</span><br><span class="line"># MAE</span><br><span class="line">print(&#39;MAE:&#39;,metrics.mean_absolute_error(y_true, y_pred))</span><br><span class="line"># MAPE</span><br><span class="line">print(&#39;MAPE:&#39;,mape(y_true, y_pred))</span><br><span class="line"></span><br><span class="line">## R2-score</span><br><span class="line">from sklearn.metrics import r2_score</span><br><span class="line">y_true &#x3D; [3, -0.5, 2, 7]</span><br><span class="line">y_pred &#x3D; [2.5, 0.0, 2, 8]</span><br><span class="line">print(&#39;R2-score:&#39;,r2_score(y_true, y_pred))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据挖掘及机器学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>如何使用github创建博客</title>
    <url>/2020/03/19/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8github%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>-利用 Github 搭建博客需要熟悉git方便管理.操作<a href="http://iissnan.com/progit/" target="_blank" rel="noopener">如果对git感兴趣请参考</a></p>
<h1 id="搭建环境"><a href="#搭建环境" class="headerlink" title="搭建环境"></a>搭建环境</h1><h2 id="安装-node"><a href="#安装-node" class="headerlink" title="安装 node"></a>安装 node</h2><ul>
<li>因为 hexo 是基于 node 框架的,先下载安装 node ,查看<code>node -v</code>版本,没有的话就根据提示操作</li>
</ul>
<h2 id="安装-npm"><a href="#安装-npm" class="headerlink" title="安装 npm"></a>安装 npm</h2><ul>
<li>安装 nodejs 肯定要安装 npm ,Ubuntu下载可能会很慢,建议换成国内源,参考<a href="https://www.cnblogs.com/vipstone/p/9038023.html" target="_blank" rel="noopener">Ubuntu apt-get和pip源更换</a></li>
</ul>
<h2 id="初始化-blog"><a href="#初始化-blog" class="headerlink" title="初始化 blog"></a>初始化 blog</h2><ol>
<li>安装 hexo ,在 终端 中输入:<code>npm install hexo-cli -g</code>(<a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">参考Hexo文档</a>)</li>
<li>初始化 blog 目录:<code>hexo init happybear1234.github.io</code>(这里的 happybear1234 换成你自己的英文名,我这里就是github的用户名)</li>
<li>初始化之后,进入到 blog 目录下:<code>cd happybear1234.github.io</code>(以后对博客的所以操作都是在这)</li>
<li>安装<code>npm install</code></li>
<li>clean一下:<code>hexo clean</code></li>
<li>生成静态页面:<code>hexo g</code></li>
<li>运行起来:<code>hexo s</code></li>
</ol>
<ul>
<li>打开浏览器,输入 终端 里网址 localhost:4000 就能看到了(如果提示服务端口被占用,可以换个端口,<code>hexo server -p 5000</code>)</li>
</ul>
<h1 id="选一个Hexo主题"><a href="#选一个Hexo主题" class="headerlink" title="选一个Hexo主题"></a>选一个Hexo主题</h1><ul>
<li>这里提供<a href="https://www.zhihu.com/question/24422335" target="_blank" rel="noopener">知乎答主们推荐的hexo主题大全</a>,刚开始为了熟悉各种配置建议使用 NexT 主题,因为文档比较详细,界面也很简洁,如果安装 NexT 主题和配置可以参考<a href="https://theme-next.org/docs/getting-started/" target="_blank" rel="noopener">文档</a></li>
</ul>
<h1 id="部署到网上"><a href="#部署到网上" class="headerlink" title="部署到网上"></a>部署到网上</h1><ul>
<li>现在的 blog 只能自己本地访问,可以使用 Github Pages 免费部署</li>
</ul>
<h2 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h2><ul>
<li>创建一个 xxx.github.io 的 public 仓库,这里 xxx 写你的名字,我这里写的 happybear1234.github.io,那么之后我就可以用 happybear1234.github.io 来访问了</li>
</ul>
<h2 id="安装-hexo-deployer-git"><a href="#安装-hexo-deployer-git" class="headerlink" title="安装 hexo-deployer-git"></a>安装 hexo-deployer-git</h2><ul>
<li>在 blog 目录下输入下面命令,这样本地的文章才能 push 到 Github 上面去<br>  <code>npm install hexo-deployer-git --save</code></li>
</ul>
<h2 id="配置Git"><a href="#配置Git" class="headerlink" title="配置Git"></a>配置Git</h2><ul>
<li><p>打开 blog 目录下配置文件:<code>vi _config.yml</code>,输入你的 git 地址:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  	    type: git</span><br><span class="line">  	    repo: https:&#x2F;&#x2F;github.com&#x2F;xxx&#x2F;xxxx.github.io.git</span><br></pre></td></tr></table></figure>
<h2 id="推送网站到-Github-上"><a href="#推送网站到-Github-上" class="headerlink" title="推送网站到 Github 上"></a>推送网站到 Github 上</h2></li>
<li><p>直接在 blog 目录下输入:<code>hexo d</code></p>
</li>
<li><p>push 上去以后你就可以输入 xxx.github.io 进行访问啦</p>
</li>
</ul>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
