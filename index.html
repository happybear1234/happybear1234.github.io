<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"happybear1234.github.io","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
<meta property="og:type" content="website">
<meta property="og:title" content="happybear">
<meta property="og:url" content="https://happybear1234.github.io/index.html">
<meta property="og:site_name" content="happybear">
<meta property="og:description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="happybear">
<meta property="article:tag" content="happybear">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="C+++">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://happybear1234.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>happybear</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">happybear</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-主页"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-标签"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-分类"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-档案"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="search-pop-overlay">
  <div class="popup search-popup">
      <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

  </div>
</div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/06/15/latex%E8%AE%BA%E6%96%87%E6%A8%A1%E6%9D%BF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/15/latex%E8%AE%BA%E6%96%87%E6%A8%A1%E6%9D%BF/" class="post-title-link" itemprop="url">LaTex论文模板</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-15 22:14:50 / 修改时间：22:17:12" itemprop="dateCreated datePublished" datetime="2020-06-15T22:14:50+08:00">2020-06-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;ctexart&#125;%中文</span><br><span class="line">\pagestyle&#123;empty&#125;%去掉页眉</span><br><span class="line">\usepackage&#123;graphicx&#125;%插图</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">%封面</span><br><span class="line">\begin&#123;flushright&#125;</span><br><span class="line">	&#123;\Large 分\hspace&#123;2cm&#125;数&#125;：\underline&#123;\quad\qquad\qquad&#125;</span><br><span class="line">	\vskip 0.5cm</span><br><span class="line">	&#123;\Large 任课教师签字&#125;：\underline&#123;\quad\qquad\qquad&#125;</span><br><span class="line">\end&#123;flushright&#125; </span><br><span class="line">\begin&#123;center&#125;</span><br><span class="line">	\quad \\</span><br><span class="line">	\quad \\</span><br><span class="line">	\heiti \fontsize&#123;30&#125;&#123;17&#125; 华\quad 北\quad 电\quad 力\quad 大\quad 学\quad 研\quad 究\quad 生\quad 结\quad 课\quad 作\quad 业</span><br><span class="line">	\vskip 6cm</span><br><span class="line">	%\heiti \zihao&#123;2&#125; 在此打印论文题目，二号黑体	</span><br><span class="line">\end&#123;center&#125;</span><br><span class="line">\vskip 3cm</span><br><span class="line">\begin&#123;quotation&#125;</span><br><span class="line">	\songti \fontsize&#123;15&#125;&#123;15&#125;</span><br><span class="line">	\par\setlength\parindent&#123;8em&#125;</span><br><span class="line">	\quad </span><br><span class="line">	</span><br><span class="line">	学\hspace&#123;0.2cm&#125; 年\hspace&#123;0.2cm&#125;学\hspace&#123;0.2cm&#125;期：\underline&#123;&#123;\Large 2019-2020&#125;学年第二学期&#125;</span><br><span class="line">	\vskip 0.5cm</span><br><span class="line">	课\hspace&#123;0.2cm&#125; 程\hspace&#123;0.2cm&#125;名\hspace&#123;0.2cm&#125;称：\underline&#123;数据仓库与数据挖掘\qquad&#125;</span><br><span class="line">	\vskip 0.5cm</span><br><span class="line">	学\hspace&#123;0.2cm&#125; 生\hspace&#123;0.1cm&#125; 姓\hspace&#123;0.1cm&#125; 名：\underline&#123;\qquad\qquad\qquad 肖雄\qquad\qquad\qquad &#125;</span><br><span class="line">	\vskip 0.5cm</span><br><span class="line">	学\hspace&#123;1.7cm&#125; 号：\underline&#123;\quad\qquad &#123;\Large 2192221067&#125;\qquad\qquad\quad&#125;</span><br><span class="line">	\vskip 0.5cm</span><br><span class="line">	提\hspace&#123;0.3cm&#125;交\hspace&#123;0.3cm&#125;时\hspace&#123;0.2cm&#125;间：\underline&#123;\qquad &#123;\Large 2020&#125;年&#123;\Large 06&#125;月&#123;\Large 15&#125;日\qquad&#125;</span><br><span class="line">	\vskip 2cm</span><br><span class="line">	\centering</span><br><span class="line">\end&#123;quotation&#125;</span><br><span class="line">\title&#123;逻辑回归分类预测的分析与应用&#125;</span><br><span class="line">\date&#123;&#125;</span><br><span class="line">\maketitle</span><br><span class="line"></span><br><span class="line">%中文摘要</span><br><span class="line">\begin&#123;abstract&#125;</span><br><span class="line">	基于逻辑回归模型,对二分类问题进行分类预测;通过sklearn逻辑回归库与梯度下降算法实现做对比;并分别采用留一法与十折交叉验证法对Iris数据集和Blood Transfusion Service Center数据集进行分类;实验结果表明，留一法与十折交叉验证法精度相差不大，但十折交叉验证更加高效。</span><br><span class="line">	\newline%另起一行</span><br><span class="line">	</span><br><span class="line">	\centering%使得关键字居中</span><br><span class="line">	\textbf&#123;关键字：&#125;逻辑回归，梯度下降，留一法，十折交叉验证法</span><br><span class="line">\end&#123;abstract&#125;</span><br><span class="line">%英文摘要</span><br><span class="line">\newcommand&#123;\enabstractname&#125;&#123;Abstract&#125;</span><br><span class="line">\newenvironment&#123;enabstract&#125;&#123;%</span><br><span class="line">	\par\small</span><br><span class="line">	\noindent\mbox&#123;&#125;\hfill&#123;\bfseries \enabstractname&#125;\hfill\mbox&#123;&#125;\par</span><br><span class="line">	\vskip 2.5ex&#125;&#123;\par\vskip 2.5ex&#125;  </span><br><span class="line">\begin&#123;enabstract&#125;</span><br><span class="line">	Based on logistic regression model, the classification prediction of dichotomy problem is carried out.The sklearn logistic regression library is compared with the implementation of gradient descent algorithm.And one method and one thousand one hundred percent cross validation method were used respectively to Iris data set and Blood Transfusion Service Center data set classification;The experimental results show that the accuracy of the retention method is not different from that of the ten fold cross validation method, but the ten fold cross validation method is more efficient.</span><br><span class="line">	</span><br><span class="line">	\centering</span><br><span class="line">	\textbf&#123;Keywords:&#125; Logistic regression, gradient descent, retention method, ten fold cross validation</span><br><span class="line">\end&#123;enabstract&#125;</span><br><span class="line"></span><br><span class="line">\section&#123;引言&#125;</span><br><span class="line">	随着信息化社会的高速发展，信息多元化成为主要发展模式。人们使用更多的特征属性描述数据信息，通常某一数据记录使用成千上万的特征描述。在数据挖掘领域中，有众多算法模型对数据进行特征提取分类,针对不同的评估方法，对模型的要求和测试的结果也不一样，逻辑回归模型作为一种高效、易实现的模型应用十分广泛。本文基于逻辑回归模型分别用sklearn逻辑回归库和梯度下降实现二分类问题；对留一法和十折交叉验证法分别评估做比较。实验结果表明，十折交叉验证法在满足精度的同一条件下，耗时更小。</span><br><span class="line">\section&#123;逻辑回归模型&#125;</span><br><span class="line">	逻辑回归是比较常用的机器学习方法，用于估计某种事物的可能性。比如某用户购买某商品的可能性，某病人患有某种疾病的可能性，以及某广告被用户点击的可能性等\cite&#123;1&#125;。逻辑回归延伸了多元线性回归思想，即因变量是二值的情形，自变量为$x_&#123;1&#125;$,$x_&#123;2&#125;$，$x_&#123;3&#125;$,…,$x_&#123;k&#125;$。逻辑回归是用来测量分类结果与因变量之间的关系。逻辑回归模型的最终结果为0,1分类结果。其中1表示属于该类，0表示不属于该类别。</span><br><span class="line">	\par&#123;一般线性模型：&#125;$$f(x)&#x3D;\omega_&#123;1&#125;x_&#123;1&#125;+\omega_&#123;2&#125;x_&#123;2&#125;…+\omega_&#123;d&#125;x_&#123;d&#125;+b$$</span><br><span class="line">	\par&#123;其中$x_&#123;1&#125;...x_&#123;d&#125;$表示d个特征属性值，$\omega_&#123;1&#125;...\omega_&#123;d&#125;,b$表示特征属性参数值。用向量简化为：$$f(x)&#x3D;&#123;\omega&#125;x^T+b$$&#125;</span><br><span class="line">	\par&#123;也可表示为：$$f(x)&#x3D;&#123;\beta&#125;\hat&#123;x&#125;^T$$&#125;</span><br><span class="line">	\par&#123;其中$\beta&#x3D;(&#123;\omega&#125;;b)$,$\hat&#123;x&#125;&#x3D;(x;1)$,线性回归只能预测连续的值，对于离散的二分类问题需要转化为逻辑回归，即将线性结果映射到\&#123;0,1\&#125;上:$$\ln\frac&#123;f(x)&#125;&#123;1-f(x)&#125;&#x3D;&#123;\beta&#125;\hat&#123;x&#125;^T$$&#125;</span><br><span class="line">	\par&#123;因此，逻辑回归又称为对数几率回归：$$f(x)&#x3D;\frac&#123;1&#125;&#123;1+e^&#123;-\beta\hat&#123;x&#125;^T&#125;&#125;$$&#125;</span><br><span class="line">	\par&#123;对于逻辑回归模型，关键在于如何求得$\beta$的值，最常用的方法就是梯度下降法。&#125;</span><br><span class="line">	%文献索引</span><br><span class="line">	\cite&#123;2&#125;</span><br><span class="line">\section&#123;梯度下降&#125;</span><br><span class="line">	梯度下降法是求解无约束优化问题的方法之一，有计算过程简单、初始收敛较快等优点，因此也常作为其他算法的核心算法，例如人工神经网络和逻辑回归，广泛应用于数据挖掘、模式识别等领域\cite&#123;3&#125;。</span><br><span class="line">	\par&#123;对于一阶无约束优化问题$min_&#123;x&#125;f(x)$,若能找到$x^&#123;0&#125;,x^&#123;1&#125;,x^&#123;2&#125;...$满足：$$f(x^&#123;t+1&#125;)&lt;f(x^&#123;t&#125;),t&#x3D;0,1,2...$$&#125;</span><br><span class="line">	\par&#123;不断执行此过程可收敛到局部极小点，根据泰勒展开式：$$f(x+\Delta x)\approx f(x)+\Delta x\nabla f(x)$$&#125;</span><br><span class="line">	\par&#123;于是，欲满足$f(x+\Delta x)&lt;f(x)$，可选择$$\Delta x&#x3D;-\gamma\nabla f(x)$$&#125;</span><br><span class="line">	\par&#123;其中$\gamma$是小常数。这就是梯度下降&#125;\cite&#123;4&#125;。</span><br><span class="line">	\par&#123;在逻辑回归模型中，可通过极大似然法来估计$\beta$的值\cite&#123;5&#125;：$$\psi(\beta)&#x3D;\sum_&#123;i&#x3D;1&#125;^&#123;m&#125;(y_&#123;i&#125;p(y&#x3D;1|\beta\hat&#123;x&#125;^T_&#123;i&#125;)+(1-y_&#123;i&#125;)p(y&#x3D;0|\beta\hat&#123;x&#125;^T_&#123;i&#125;))&#x3D;\sum_&#123;i&#x3D;1&#125;^&#123;m&#125;(y_&#123;i&#125;\beta\hat&#123;x&#125;^T_&#123;i&#125;-ln(1+e^&#123;\beta\hat&#123;x&#125;^T_&#123;i&#125;&#125;))$$&#125;</span><br><span class="line">	\par&#123;该函数为连续可导凸函数，因此可采用梯度下降来求解，因此将上式转化为最小化：$$\psi(\beta)&#x3D;\sum_&#123;i&#x3D;1&#125;^&#123;m&#125;(-y_&#123;i&#125;\beta\hat&#123;x&#125;^T_&#123;i&#125;+ln(1+e^&#123;\beta\hat&#123;x&#125;^T_&#123;i&#125;&#125;))$$&#125;</span><br><span class="line">	\par&#123;迭代过程：$$\beta^&#123;t+1&#125;&#x3D;\beta^&#123;t&#125;-\gamma\nabla \psi(\beta)$$&#125;</span><br><span class="line">\section&#123;应用&#125;</span><br><span class="line">	本文使用文献2中西瓜数据集$3.0\alpha$，分布情况如图\ref&#123;fig:xigua&#125;</span><br><span class="line">	\begin&#123;figure&#125;[h]</span><br><span class="line">		\centering</span><br><span class="line">		\includegraphics[width&#x3D;0.7\linewidth]&#123;1&#125;</span><br><span class="line">		\caption&#123;西瓜数据集$3.0\alpha$散点图\label&#123;fig:xigua&#125;&#125;</span><br><span class="line">		\label&#123;fig:1&#125;</span><br><span class="line">	\end&#123;figure&#125;</span><br><span class="line">	\par&#123;特征属性为密度和含糖量，样本标签“1”表示好瓜，“0”表示坏瓜；使用留出法选择相同的训练集和测试集，通过sklearn逻辑回归库和批量梯度下降法分别进行分类训练和测试，其中梯度下降设置固定步长为0.1，在迭代15000次后趋于稳定，如图\ref&#123;fig:diedai&#125;&#125;</span><br><span class="line">	\begin&#123;figure&#125;</span><br><span class="line">		\centering</span><br><span class="line">		\includegraphics[width&#x3D;0.7\linewidth]&#123;2&#125;</span><br><span class="line">		\caption&#123;批量梯度下降迭代曲线\label&#123;fig:diedai&#125;&#125;</span><br><span class="line">		\label&#123;fig:2&#125;</span><br><span class="line">	\end&#123;figure&#125;。</span><br><span class="line">	\par&#123;经测试两者精度相差不大，测试结果如下表:&#125;</span><br><span class="line">	\par&#123;&#125;</span><br><span class="line">	%表格</span><br><span class="line">	\begin&#123;tabular&#125;&#123;|c|c|&#125;</span><br><span class="line">		\hline </span><br><span class="line">		方法&amp;精度  \\ </span><br><span class="line">		\hline </span><br><span class="line">		sklearn逻辑回归库&amp;67\%  \\ </span><br><span class="line">		\hline </span><br><span class="line">		批量梯度下降法&amp;66.67\%  \\ </span><br><span class="line">		\hline </span><br><span class="line">	\end&#123;tabular&#125; </span><br><span class="line">	\par&#123;考虑到该数据样本过小，使用留出法拟合效果一般，因此在UCI选择Iris数据集和Blood Transfusion Service Center数据集，同样以逻辑回归模型分别对留一法和十折交叉验证法评估做比较，结果如下：&#125;</span><br><span class="line">	\par&#123;&#125;</span><br><span class="line">	\begin&#123;tabular&#125;&#123;|c|c|c|&#125;</span><br><span class="line">		\hline </span><br><span class="line">		数据集&amp;留一法&amp;十折交叉验证法  \\ </span><br><span class="line">		\hline </span><br><span class="line">		Iris&amp;96.66\%&amp;97.33\%\\ </span><br><span class="line">		\hline </span><br><span class="line">		Blood Transfusion Service Center&amp;76.87\%&amp;77.01\%\\ </span><br><span class="line">		\hline </span><br><span class="line">	\end&#123;tabular&#125; </span><br><span class="line">	\par&#123;Iris数据集因为数据类间分散情况比较好，广泛被引用，因此拟合效果比Blood Transfusion Service Center数据集要好，从图\ref&#123;fig:i&#125;,\ref&#123;fig:t&#125;就能看出：&#125;</span><br><span class="line">	\begin&#123;figure&#125;</span><br><span class="line">		\centering</span><br><span class="line">		\includegraphics[width&#x3D;0.7\linewidth]&#123;i&#125;</span><br><span class="line">		\caption&#123;Iris数据集散点图\label&#123;fig:i&#125;&#125;</span><br><span class="line">		\label&#123;fig:i&#125;</span><br><span class="line">	\end&#123;figure&#125;</span><br><span class="line">	\begin&#123;figure&#125;</span><br><span class="line">		\centering</span><br><span class="line">		\includegraphics[width&#x3D;0.7\linewidth]&#123;t&#125;</span><br><span class="line">		\caption&#123;Blood Transfusion Service Center数据集散点图\label&#123;fig:t&#125;&#125;</span><br><span class="line">		\label&#123;fig:t&#125;</span><br><span class="line">	\end&#123;figure&#125;</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	\par&#123;实验结果表明，留一法和十折交叉验证法的精度相差不大。值得注意的是，十折交叉验证更加高效，耗时更少，对于数据量越大，这种现象越明显。因此选择十折交叉验证即可满足精度要求，又减少运行成本。&#125;</span><br><span class="line">\section&#123;总结&#125;</span><br><span class="line">	逻辑回归对于二分类问题，不仅将预测值映射到$\&#123;0,1\&#125;$之间的值，而且还能评估出概率值，这使得在许多领域都具有广泛的应用。一个好的模型通常是由测试结果来判定，因此对训练集、测试集进行划分的评估方法起着决定性作用，本文对留一法和十折交叉验证法评估结果比较，结果显示在数据量足够的情况，选择十折交叉验证法更加高效。</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">\begin&#123;thebibliography&#125;&#123;&#125;</span><br><span class="line">	\bibitem&#123;1&#125;毛林,陆全华,程涛.\emph&#123;基于高维数据的集成逻辑回归分类算法的研究与应用[J]&#125;,</span><br><span class="line">	\texttt&#123;科技通报,2013,29(12):64-66&#125;</span><br><span class="line">	\bibitem&#123;2&#125;周志华.\emph&#123;机器学习[M]&#125;,</span><br><span class="line">	\texttt&#123;2016:53-59&#125;</span><br><span class="line">	\bibitem&#123;3&#125;郭跃东,宋旭东.\emph&#123;梯度下降法的分析和改进[J]&#125;,</span><br><span class="line">	\texttt&#123;科技展望,2016,26(15):115+117&#125;</span><br><span class="line">	\bibitem&#123;4&#125;周志华.\emph&#123;机器学习[M]&#125;,</span><br><span class="line">	\texttt&#123;2016:407-408&#125;</span><br><span class="line">	\bibitem&#123;5&#125;周志华.\emph&#123;机器学习[M]&#125;,</span><br><span class="line">	\texttt&#123;2016:59-60&#125;</span><br><span class="line">\end&#123;thebibliography&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/05/24/%E8%A5%BF%E7%93%9C%E4%B9%A6%E4%B9%A0%E9%A2%983-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/24/%E8%A5%BF%E7%93%9C%E4%B9%A6%E4%B9%A0%E9%A2%983-4/" class="post-title-link" itemprop="url">西瓜书习题3-4</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-05-24 21:49:45 / 修改时间：22:33:15" itemprop="dateCreated datePublished" datetime="2020-05-24T21:49:45+08:00">2020-05-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>参考:<a href="https://blog.csdn.net/Snoopy_Yuan/article/details/64131129" target="_blank" rel="noopener">https://blog.csdn.net/Snoopy_Yuan/article/details/64131129</a></p>
<p>完整代码:<a href="https://github.com/happybear1234/The-Watermelon-book-exercises/blob/master/Practical_3.4/code/Practical_3.4.py" target="_blank" rel="noopener">https://github.com/happybear1234/The-Watermelon-book-exercises/blob/master/Practical_3.4/code/Practical_3.4.py</a></p>
<blockquote>
<p>习题3.4 选择两个UCI数据集，比较10折交叉验证法和留一法所估计出的对率回归的错误率</p>
</blockquote>
<p>这里从UCI分别选择了数据集<a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="noopener">Iris Data Set </a>和<a href="http://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center" target="_blank" rel="noopener"> Blood Transfusion Service Center Data Set</a>;通过sklearn库实现,seaborns进行可视化,另外seaborns自带iris的数据集,可以直接拿来用</p>
<h1 id="载入数据-预处理"><a href="#载入数据-预处理" class="headerlink" title="载入数据,预处理"></a>载入数据,预处理</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import seaborn as sns</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">iris &#x3D; sns.load_dataset(&#39;iris&#39;) # 在线载入自带的 iris 数据集</span><br><span class="line">X &#x3D; iris.values[:, 0 : 4]</span><br><span class="line">y &#x3D; iris.values[:, 4]</span><br><span class="line"></span><br><span class="line">sns.set(style&#x3D;&#39;white&#39;) # 风格设置</span><br><span class="line">g &#x3D; sns.pairplot(iris, hue&#x3D;&#39;species&#39;, markers&#x3D;[&#39;o&#39;, &#39;s&#39;, &#39;D&#39;]) # 变量关系组图</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/05/24/tSXph6.png" alt="1"></p>
<p>可以看到iris数据类间比较分散,也是后面测试结果比较好的原因之一</p>
<h1 id="sklearn库"><a href="#sklearn库" class="headerlink" title="sklearn库"></a>sklearn库</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn import metrics</span><br><span class="line">from sklearn import model_selection</span><br><span class="line"></span><br><span class="line">log_model &#x3D; LogisticRegression(max_iter&#x3D;1000) # 增加最大迭代次数,也可以减少数据量</span><br><span class="line">m, n &#x3D; np.shape(X)</span><br></pre></td></tr></table></figure>
<h2 id="十折交叉训练"><a href="#十折交叉训练" class="headerlink" title="十折交叉训练"></a>十折交叉训练</h2><p>model_selection.cross_val_predict 指定模型直接就返回测试结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_pred_10_fold &#x3D; model_selection.cross_val_predict(log_model, X, y, cv&#x3D;10)</span><br><span class="line"></span><br><span class="line"># 打印精度</span><br><span class="line">accuracy_10_fold &#x3D; metrics.accuracy_score(y, y_pred_10_fold)</span><br><span class="line">print(&#39;The accuracy of 10-fold cross-validation:&#39;, accuracy_10_fold)</span><br></pre></td></tr></table></figure>
<pre><code>The accuracy of 10-fold cross-validation: 0.9733333333333334</code></pre><h2 id="留一法"><a href="#留一法" class="headerlink" title="留一法"></a>留一法</h2><p>留一法相当于k折交叉训练中,把k取为所有的样例数m,因此要经过m次训练,用循环来实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">accuracy_LOO &#x3D; 0</span><br><span class="line"># 计算 m 次测试的结果</span><br><span class="line">for train_index, test_index in model_selection.LeaveOneOut().split(X):</span><br><span class="line">    X_train, X_test &#x3D; X[train_index], X[test_index] # 训练集样本,测试集样本</span><br><span class="line">    y_train, y_test &#x3D; y[train_index], y[test_index] # 训练集标签, 测试集标签</span><br><span class="line">    log_model.fit(X_train, y_train) # 训练模型</span><br><span class="line">    y_pred_LOO &#x3D; log_model.predict(X_test) # 测试</span><br><span class="line">    if y_pred_LOO &#x3D;&#x3D; y_test:</span><br><span class="line">        accuracy_LOO +&#x3D; 1</span><br><span class="line">print(&#39;The accuracy of Leave-One-Out:&#39;, accuracy_LOO &#x2F; m)</span><br></pre></td></tr></table></figure>
<pre><code>The accuracy of Leave-One-Out: 0.9666666666666667</code></pre><p>对于iris数据集,精度比较高,相应错误率较低</p>
<p>类似的,对Transfusion数据集可视化:<br><img src="https://s1.ax1x.com/2020/05/24/tSXljg.png" alt="2"></p>
<p>类间分散比较紧凑</p>
<p>相应精度:<br>    The accuracy of 10-fold cross-validation: 0.7687165775401069<br>    The accuracy of Leave-One-Out: 0.7700534759358288</p>
<p>通过以上对比,十折交叉验证法与留一法精度相差不大;而且通过实验,留一法代码跑的时间更长,对于数据越大,这种现象越明显.<br>因此往后,选择十折交叉验证即可满足精度要求,也节约运行成本</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/05/23/%E8%A5%BF%E7%93%9C%E4%B9%A6%E4%B9%A0%E9%A2%983-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/23/%E8%A5%BF%E7%93%9C%E4%B9%A6%E4%B9%A0%E9%A2%983-3/" class="post-title-link" itemprop="url">西瓜书习题3-3</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-23 16:06:17" itemprop="dateCreated datePublished" datetime="2020-05-23T16:06:17+08:00">2020-05-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-25 13:03:41" itemprop="dateModified" datetime="2020-05-25T13:03:41+08:00">2020-05-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>参考:<br>[1]<a href="https://www.cnblogs.com/judejie/p/8999832.html" target="_blank" rel="noopener">https://www.cnblogs.com/judejie/p/8999832.html</a><br>[2]<a href="https://blog.csdn.net/zouxy09/article/details/20319673" target="_blank" rel="noopener">https://blog.csdn.net/zouxy09/article/details/20319673</a><br>[3]<a href="https://blog.csdn.net/Snoopy_Yuan/article/details/63684219?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase" target="_blank" rel="noopener">https://blog.csdn.net/Snoopy_Yuan/article/details/63684219?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase</a></p>
<p>完整代码:<br>(sklearn库实现)<a href="https://github.com/happybear1234/The-Watermelon-book-exercises/blob/master/Practical_3.3/code/Practical_3.3.py" target="_blank" rel="noopener">https://github.com/happybear1234/The-Watermelon-book-exercises/blob/master/Practical_3.3/code/Practical_3.3.py</a><br>(梯度下降实现)<a href="https://github.com/happybear1234/The-Watermelon-book-exercises/blob/master/Practical_3.3/code/Practical_3.3_self_def.py" target="_blank" rel="noopener">https://github.com/happybear1234/The-Watermelon-book-exercises/blob/master/Practical_3.3/code/Practical_3.3_self_def.py</a></p>
<blockquote>
<p>习题3.3 编程实现对率回归,并给出西瓜数据集(如下)上的结果.</p>
</blockquote>
<table>
<thead>
<tr>
<th>密度</th>
<th>含糖率</th>
<th>好瓜</th>
</tr>
</thead>
<tbody><tr>
<td>0.697</td>
<td>0.46</td>
<td>是</td>
</tr>
<tr>
<td>0.774</td>
<td>0.376</td>
<td>是</td>
</tr>
<tr>
<td>0.634</td>
<td>0.264</td>
<td>是</td>
</tr>
<tr>
<td>0.608</td>
<td>0.318</td>
<td>是</td>
</tr>
<tr>
<td>0.556</td>
<td>0.215</td>
<td>是</td>
</tr>
<tr>
<td>0.403</td>
<td>0.237</td>
<td>是</td>
</tr>
<tr>
<td>0.481</td>
<td>0.149</td>
<td>是</td>
</tr>
<tr>
<td>0.437</td>
<td>0.211</td>
<td>是</td>
</tr>
<tr>
<td>0.666</td>
<td>0.091</td>
<td>否</td>
</tr>
<tr>
<td>0.243</td>
<td>0.267</td>
<td>否</td>
</tr>
<tr>
<td>0.245</td>
<td>0.057</td>
<td>否</td>
</tr>
<tr>
<td>0.343</td>
<td>0.099</td>
<td>否</td>
</tr>
<tr>
<td>0.639</td>
<td>0.161</td>
<td>否</td>
</tr>
<tr>
<td>0.657</td>
<td>0.198</td>
<td>否</td>
</tr>
<tr>
<td>0.36</td>
<td>0.37</td>
<td>否</td>
</tr>
<tr>
<td>0.593</td>
<td>0.042</td>
<td>否</td>
</tr>
<tr>
<td>0.719</td>
<td>0.103</td>
<td>否</td>
</tr>
</tbody></table>
<p>因此将以上数据中好瓜表示为”1”,不好的瓜表示为”0”,转换成csv文件便于读取</p>
<h1 id="公式说明"><a href="#公式说明" class="headerlink" title="公式说明"></a>公式说明</h1><p>基础线性模型:<br>$$f(x)=\omega_{1}x_{1}+\omega_{2}x_{2}…+\omega_{d}x_{d}+b$$<br>可转化为向量:<br>$$f(x)={\omega}x^T+b$$<br>继而令$\beta=({\omega},b)$,$\hat{x}=(x,1)$,那么(注:此处均作为行向量,与西瓜书上相反,):<br>$$f(x)={\omega}x^T+b={\beta}\hat{x}^T$$<br>为了解决此处的二分类问题,将预测值映射成$y\in{0,1}$的值,即将线性回归转化为逻辑回归,常常采用以下的对数几率函数(sigmoid函数)代替:<br>$$y=\frac{1}{1+e^{-\beta\hat{x}^T}}$$<br>因此,只要求得$\omega$和$b$的值即可,以下通过极大似然法来估计$\omega$和$b$的值:<br>$$\psi(\beta)=\sum_{i=1}^{m}(y_{i}\beta\hat{x}^T_{i}-ln(1+e^{\beta\hat{x}^T_{i}}))$$<br>将上式最大化转化为最小化,便于后面梯度下降求解,如西瓜书P59公式3.27:<br>$$\psi(\beta)=\sum_{i=1}^{m}(-y_{i}\beta\hat{x}^T_{i}+ln(1+e^{\beta\hat{x}^T_{i}}))$$</p>
<p>该函数为连续可导凸函数(对应海塞矩阵正定),因此可用梯度下降求得最优解,梯度为:<br>$$\frac{\varphi\psi(\beta)}{\varphi\beta}=\sum_{i=1}^{m}(-y_{i}+\frac{1}{1+e^{-\beta\hat{x}^T}})\hat{x}_{i}$$<br>迭代过程(其中$\lambda$为步长)为:<br>$$\beta^{t+1}=\beta^{t}-\lambda\frac{\varphi\psi(\beta)}{\varphi\beta}$$</p>
<h1 id="sklearn库实现"><a href="#sklearn库实现" class="headerlink" title="sklearn库实现"></a>sklearn库实现</h1><h2 id="载入数据-预处理"><a href="#载入数据-预处理" class="headerlink" title="载入数据,预处理"></a>载入数据,预处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset &#x3D; np.loadtxt(&#39;&#x2F;home&#x2F;data&#x2F;watermelon_3a.csv&#39;, delimiter&#x3D;&#39;,&#39;)</span><br><span class="line"></span><br><span class="line">X &#x3D; dataset[:, 1 : 3]</span><br><span class="line">y &#x3D; dataset[:, 3]</span><br><span class="line">print(np.shape(X))</span><br></pre></td></tr></table></figure>
<pre><code>(17, 2)</code></pre><p>绘制分散图,查看数据分散情况:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">f1 &#x3D; plt.figure(1)</span><br><span class="line">plt.title(&#39;watermelon_3a&#39;)</span><br><span class="line">plt.xlabel(&#39;density&#39;)</span><br><span class="line">plt.ylabel(&#39;rate_sugar&#39;)</span><br><span class="line">plt.scatter(X[y &#x3D;&#x3D; 0, 0], X[y &#x3D;&#x3D; 0, 1], marker &#x3D; &#39;o&#39;, color &#x3D; &#39;k&#39;, s &#x3D; 100, label&#x3D; &#39;bad&#39;)</span><br><span class="line">plt.scatter(X[y &#x3D;&#x3D; 1, 0], X[y &#x3D;&#x3D;1, 1], marker&#x3D; &#39;o&#39;, color &#x3D; &#39;g&#39;, s &#x3D; 100, label &#x3D; &#39;good&#39;)</span><br><span class="line">plt.legend(loc &#x3D; &#39;upper right&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/05/23/YxdnFe.png" alt="1"></p>
<h2 id="sklearn逻辑回归库拟合"><a href="#sklearn逻辑回归库拟合" class="headerlink" title="sklearn逻辑回归库拟合"></a>sklearn逻辑回归库拟合</h2><p>调用sklearn中的逻辑回归模型进行训练和预测</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import model_selection</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn import metrics</span><br><span class="line">import matplotlib.pylab as pl</span><br><span class="line"></span><br><span class="line"># 切分数据集:留出法 返回 划分好的训练集测试集样本和训练集测试集标签</span><br><span class="line">X_train, X_test, y_train, y_test &#x3D; model_selection.train_test_split(X, y, test_size&#x3D;0.5, random_state&#x3D;0)</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">log_model &#x3D; LogisticRegression()</span><br><span class="line">log_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 模型测试</span><br><span class="line">y_pred &#x3D; log_model.predict(X_test)</span><br></pre></td></tr></table></figure>
<p>打印混淆矩阵和相关度量,结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(metrics.confusion_matrix(y_test, y_pred))</span><br><span class="line">print(metrics.classification_report(y_test, y_pred))</span><br><span class="line">precision, recall, thresholds &#x3D; metrics.precision_recall_curve(y_test, y_pred)</span><br></pre></td></tr></table></figure>
<pre><code>[[3 2]
 [1 3]]

precision    recall  f1-score   support

     0.0       0.75      0.60      0.67         5
     1.0       0.60      0.75      0.67         4

    accuracy                           0.67         9
   macro avg       0.68      0.68      0.67         9
weighted avg       0.68      0.67      0.67         9</code></pre><p>这里选择的留出法抽取样本,因为样本比较少,拟合效果一般,预测精度只有67%,可以采用自助法或者交叉验证法重新抽样,进一步选择最优模型</p>
<h2 id="绘制决策边界"><a href="#绘制决策边界" class="headerlink" title="绘制决策边界"></a>绘制决策边界</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">f2 &#x3D; plt.figure(2)</span><br><span class="line">h &#x3D; 0.01</span><br><span class="line">x0_min, x0_max &#x3D; X[:, 0].min() - 0.1, X[:, 0].max() + 0.1</span><br><span class="line">x1_min, x1_max &#x3D; X[:, 1].min() - 0.1, X[:, 1].max() +0.1</span><br><span class="line">x0, x1&#x3D; np.meshgrid(np.arange(x0_min, x0_max, h), np.arange(x1_min, x1_max, h)) # 生成笛卡尔积坐标矩阵</span><br><span class="line"></span><br><span class="line">z &#x3D; log_model.predict(np.c_[x0.ravel(), x1.ravel()]) # c_ 按列合并, ravel 降成一维</span><br><span class="line"></span><br><span class="line">z &#x3D; z.reshape(x0.shape)</span><br><span class="line">plt.contourf(x0, x1, z, cmap &#x3D; pl.cm.Paired)# 等高线</span><br><span class="line"></span><br><span class="line">plt.title(&#39;watermelon_3a&#39;)</span><br><span class="line">plt.xlabel(&#39;density&#39;)</span><br><span class="line">plt.ylabel(&#39;rate_sugar&#39;)</span><br><span class="line">plt.scatter(X[y&#x3D;&#x3D;0, 0], X[y&#x3D;&#x3D;0, 1], marker&#x3D;&#39;o&#39;, color&#x3D;&#39;k&#39;, s&#x3D;100, label&#x3D;&#39;bad&#39;)</span><br><span class="line">plt.scatter(X[y&#x3D;&#x3D;1, 0], X[y&#x3D;&#x3D;1, 1], marker&#x3D;&#39;o&#39;, color&#x3D;&#39;g&#39;, s&#x3D;100, label&#x3D;&#39;good&#39;)</span><br><span class="line">plt.legend(loc&#x3D;&#39;upper right&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/05/23/YxaIIg.png" alt="2"><br>可以看出训练出来的分类器还是可以分类出大多数示例</p>
<h1 id="梯度下降法实现"><a href="#梯度下降法实现" class="headerlink" title="梯度下降法实现"></a>梯度下降法实现</h1><h2 id="实现以上公式"><a href="#实现以上公式" class="headerlink" title="实现以上公式"></a>实现以上公式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># 1)实现 P59 公式3.27极大似然法</span><br><span class="line"></span><br><span class="line">def likelihood_sub(x, y, beta):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    :param x: 一个示例变量(行向量)</span><br><span class="line">    :param y:一个样品标签(行向量)</span><br><span class="line">    :param beta:3.27中矢量参数(行向量)</span><br><span class="line">    :return: 单个对数似然 3.27</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return -y * np.dot(beta, x.T) + np.math.log(1 + np.math.exp(np.dot(beta, x.T)))</span><br><span class="line"></span><br><span class="line">def likelihood(X, y, beta):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    公式 3.27 :对数似然函数(交叉熵损失函数)</span><br><span class="line">    :param X: 示例变量矩阵</span><br><span class="line">    :param y:样本标签矩阵</span><br><span class="line">    :param beta:3.27中的矢量参数</span><br><span class="line">    :return: beta 的似然值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sum &#x3D; 0</span><br><span class="line">    m, n &#x3D; np.shape(X)</span><br><span class="line"></span><br><span class="line">    for i in range(m):</span><br><span class="line">        sum +&#x3D; likelihood_sub(X[i], y[i], beta)</span><br><span class="line">    return sum</span><br><span class="line"></span><br><span class="line"># 2)实现似然公式一阶偏导</span><br><span class="line">def sigmoid(x, beta):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    基础模型 S 形函数</span><br><span class="line">    P59 对数几率回归(逻辑回归)公式 3.23</span><br><span class="line">    :param x: 预测变量</span><br><span class="line">    :param beta: beta 变量</span><br><span class="line">    :return:S 形函数</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return 1 &#x2F; (1 + np.math.exp(- np.dot(beta, x.T)))</span><br><span class="line">def partial_derivative(X, y, beta):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    P60 似然公式一阶偏导3.30</span><br><span class="line">    :param X:示例变量矩阵</span><br><span class="line">    :param y:样本标签矩阵</span><br><span class="line">    :param beta:3.27 中矢量参数</span><br><span class="line">    :return: beta 的偏导数,梯度</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    m, n &#x3D; np.shape(X)</span><br><span class="line">    pd &#x3D; np.zeros(n)</span><br><span class="line"></span><br><span class="line">    for i in range(m):</span><br><span class="line">        tmp &#x3D; -y[i] + sigmoid(X[i], beta)</span><br><span class="line">        for j in range(n):</span><br><span class="line">            pd[j] +&#x3D; X[i][j] * tmp</span><br><span class="line">    return pd</span><br></pre></td></tr></table></figure>
<p>这里采用批量梯度下降法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def gradDscent(X, y, alpha, iterations, n):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    :param X:变量矩阵</span><br><span class="line">    :param y:样本标签数组</span><br><span class="line">    :return:3.27中beta参数最优解</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    cost &#x3D; np.zeros(iterations) # 构建 max_times 个 0 的数组</span><br><span class="line">    beta &#x3D; np.mat(np.zeros(n)) # 初始化 beta</span><br><span class="line"></span><br><span class="line">    for i in range(iterations):</span><br><span class="line">        # 梯度下降</span><br><span class="line">        output &#x3D; partial_derivative(X, y, beta)</span><br><span class="line">        beta &#x3D; beta - alpha * output</span><br><span class="line">        cost[i] &#x3D; likelihood(X, y, beta)</span><br><span class="line"></span><br><span class="line">    return beta, cost</span><br></pre></td></tr></table></figure>
<h2 id="绘制收敛曲线"><a href="#绘制收敛曲线" class="headerlink" title="绘制收敛曲线"></a>绘制收敛曲线</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def showConvergCurve(Iterations, Cost):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    :param Iterations: 迭代次数</span><br><span class="line">    :param Cost: 损失值数组</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    f1 &#x3D; plt.figure(1)</span><br><span class="line">    t &#x3D; np.arange(Iterations)</span><br><span class="line">    p1 &#x3D; plt.subplot(1,1,1)</span><br><span class="line">    p1.plot(t, Cost, &#39;r&#39;)</span><br><span class="line">    p1.set_xlabel(&#39;Iterations&#39;)</span><br><span class="line">    p1.set_ylabel(&#39;cost&#39;)</span><br><span class="line">    p1.set_title(&#39;The Gradient Descent Convergence Curve&#39;)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>这里步长取得0.1,迭代次数1500,在800次迭代后趋于稳定<br><img src="https://s1.ax1x.com/2020/05/23/Yxa7Gj.png" alt="3"></p>
<h2 id="绘制决策边界-1"><a href="#绘制决策边界-1" class="headerlink" title="绘制决策边界"></a>绘制决策边界</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def showLogRegression(X, y, Beta, N):</span><br><span class="line">    f2 &#x3D; plt.figure(2)</span><br><span class="line"></span><br><span class="line">    plt.title(&#39;The Logistic Regression Fitted Curve&#39;)</span><br><span class="line">    plt.xlabel(&#39;density&#39;)</span><br><span class="line">    plt.ylabel(&#39;rate_sugar&#39;)</span><br><span class="line">    # f &#x3D; Beta * X.transpose()</span><br><span class="line">    # plt.plot(X[:, 2], f.tolist()[0], &#39;r&#39;, label &#x3D; &#39;Prediction&#39;)</span><br><span class="line">    min_x &#x3D; min(X[:, 0])</span><br><span class="line">    max_x &#x3D; max(X[:, 0])</span><br><span class="line">    y_min_x &#x3D; (- Beta.tolist()[0][2] - Beta.tolist()[0][0] * min_x) &#x2F; Beta.tolist()[0][1] # 由线性模型 y &#x3D;  w1 * x1 + w2 * x2 +b</span><br><span class="line">    y_max_x &#x3D; (- Beta.tolist()[0][2] - Beta.tolist()[0][0] * max_x) &#x2F; Beta.tolist()[0][1]</span><br><span class="line">    plt.plot([min_x, max_x], [y_min_x, y_max_x], &#39;-g&#39;)</span><br><span class="line">    plt.scatter(X[y &#x3D;&#x3D; 0, 0], X[y &#x3D;&#x3D; 0, 1], marker&#x3D;&#39;o&#39;, color&#x3D;&#39;k&#39;, s&#x3D;100, label&#x3D;&#39;bad&#39;)</span><br><span class="line">    plt.scatter(X[y &#x3D;&#x3D; 1, 0], X[y &#x3D;&#x3D; 1, 1], marker&#x3D;&#39;o&#39;, color&#x3D;&#39;g&#39;, s&#x3D;100, label&#x3D;&#39;good&#39;)</span><br><span class="line">    plt.legend(loc&#x3D;&#39;upper right&#39;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/05/23/Yxabzn.png" alt="4"><br>看上去并没有sklearn库中逻辑回归分类器效果好,但是计算出来的精度却比分类器要高(这里为了方便没有划分数据集,可以重新用sklearn分好的数据再做对比)</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def testLogRegres(Beta, test_x, test_y):</span><br><span class="line">    m, n &#x3D; np.shape(test_x)</span><br><span class="line">    matchCount &#x3D; 0</span><br><span class="line">    for i in range(m):</span><br><span class="line">        predict &#x3D; sigmoid(test_x[i], Beta) &gt; 0.5</span><br><span class="line">        if predict &#x3D;&#x3D; bool(test_y[i]):</span><br><span class="line">            matchCount +&#x3D; 1</span><br><span class="line">    accuracy &#x3D; float(matchCount) &#x2F; m</span><br><span class="line">    return accuracy</span><br><span class="line"></span><br><span class="line">def loadData():</span><br><span class="line">    dataset &#x3D; np.loadtxt(&#39;&#x2F;home&#x2F;data&#x2F;watermelon_3a.csv&#39;, delimiter&#x3D;&#39;,&#39;)</span><br><span class="line"></span><br><span class="line">    X &#x3D; dataset[:, 1: 3]</span><br><span class="line">    tmp &#x3D; np.ones(X.shape[0])</span><br><span class="line">    X &#x3D; np.insert(X, 2, values&#x3D;tmp, axis&#x3D;1) # 在最后一列插入全是 1 的列</span><br><span class="line">    y &#x3D; dataset[:, 3]</span><br><span class="line">    return X, y</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    alpha &#x3D; 0.1  # 迭代步长</span><br><span class="line">    iterations &#x3D; 1500  # 迭代次数上限</span><br><span class="line">    X, y &#x3D; loadData()</span><br><span class="line">    test_x &#x3D; X</span><br><span class="line">    test_y &#x3D; y</span><br><span class="line">    m, n &#x3D; np.shape(X)</span><br><span class="line">    beta, cost &#x3D; gradDscent(X, y, alpha, iterations, n)</span><br><span class="line">    print(beta)</span><br><span class="line">    showConvergCurve(iterations, cost)</span><br><span class="line">    showLogRegression(X, y, beta, n)</span><br><span class="line">    accuracy &#x3D; testLogRegres(beta, test_x, test_y)</span><br><span class="line">    print(&#39;The classify accuracy is: %.3f%%&#39; %(accuracy * 100))</span><br></pre></td></tr></table></figure>
<pre><code>The classify accuracy is: 70.588%</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/04/28/%E8%A5%BF%E7%93%9C%E4%B9%A6%E4%B9%A0%E9%A2%981-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/28/%E8%A5%BF%E7%93%9C%E4%B9%A6%E4%B9%A0%E9%A2%981-2/" class="post-title-link" itemprop="url">西瓜书习题1.2</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-28 15:24:48 / 修改时间：16:21:27" itemprop="dateCreated datePublished" datetime="2020-04-28T15:24:48+08:00">2020-04-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>参考<a href="https://blog.csdn.net/yuzeyuan12/article/details/83113461" target="_blank" rel="noopener">https://blog.csdn.net/yuzeyuan12/article/details/83113461</a></p>
<ul>
<li>1.2 与使用单个合取式来进行假设表示相比，使用“析合范式”将使得假设空间具有更强的表示能力。若使用最多包含k个合取式的析合范式来表达表1.1的西瓜分类问题的假设空间，试估算有多少种可能的假设。</li>
</ul>
<p>表1.1显示 色泽:二种,根蒂:三种,敲声:三种;因此析取范式共有(不含空集):3*4*4=48;特征集共有:2*3*3=18.<br>此题关键点是去冗余操作,因为特征集最多为18,析合范式里去冗余后的析取范式也不会超过18;因此用一个18维向量就可以全部表示出去冗余后的所有取值,其中18维向量中全为1时,表示至多的情况;全为0时为初始值</p>
<ul>
<li>代码:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import itertools as it</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;&quot;</span><br><span class="line">1) 从 0-47 中抽取 k 个组合 sample_combin</span><br><span class="line">2) 将 sample_combin 中的元素依次转换成三维 (3,4,4) 中的对应坐标 coord_3</span><br><span class="line">3) 将 coord_3 再依次转换成 0&#x2F;1 二值形式的 18 维向量 vector_18,并依次添加到列表 vector 做去冗余操作</span><br><span class="line">4) 把 vector 映射到 1-2^18 对应数值 num,并依次添加到集合 num_set 筛选重复的数</span><br><span class="line">5) 最后 num_set 的长度即为最终要求的结果</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># 数值转换成三维(3,4,4)</span><br><span class="line">def turn_48_to_coord_3(num):</span><br><span class="line">    for i in range(3):</span><br><span class="line">        for j in range(4):</span><br><span class="line">            for k in range(4):</span><br><span class="line">                if i * 16 + j * 4 + k &#x3D;&#x3D; num:</span><br><span class="line">                    return [i + 1,j + 1,k + 1]</span><br><span class="line"></span><br><span class="line"># 三维(3,4,4)转换成 18 维向量</span><br><span class="line">def coord_3_to_18(coord_3):</span><br><span class="line">    vector_18 &#x3D; np.zeros([2,3,3])</span><br><span class="line">    # 如果色泽为 *</span><br><span class="line">    if coord_3[0] &#x3D;&#x3D; 3:</span><br><span class="line">        coord_3[0] &#x3D; [1, 2]</span><br><span class="line">    else:</span><br><span class="line">        coord_3[0] &#x3D; [coord_3[0]]</span><br><span class="line">    # 如果根蒂为 *</span><br><span class="line">    if coord_3[1] &#x3D;&#x3D; 4:</span><br><span class="line">        coord_3[1] &#x3D; [1, 2, 3]</span><br><span class="line">    else:</span><br><span class="line">        coord_3[1] &#x3D; [coord_3[1]]</span><br><span class="line">    # 如果敲声为 *</span><br><span class="line">    if coord_3[2] &#x3D;&#x3D; 4:</span><br><span class="line">        coord_3[2] &#x3D; [1, 2, 3]</span><br><span class="line">    else:</span><br><span class="line">        coord_3[2] &#x3D; [coord_3[2]]</span><br><span class="line">    for x in coord_3[0]:</span><br><span class="line">        for y in coord_3[1]:</span><br><span class="line">            for z in coord_3[2]:</span><br><span class="line">                # 映射到 18 维向量的值为 1 表示相应特征</span><br><span class="line">                vector_18[x-1][y-1][z-1] &#x3D; 1</span><br><span class="line">    return vector_18</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获得 0-48 数值转换成 18 维向量的结果</span><br><span class="line">def get_48_to_18(num):</span><br><span class="line">    coord_3 &#x3D; turn_48_to_coord_3(num)</span><br><span class="line">    vector_18 &#x3D; coord_3_to_18(coord_3)</span><br><span class="line">    return vector_18</span><br><span class="line">def main(k):</span><br><span class="line">    num_set &#x3D; []</span><br><span class="line">    # 从 0-47 中抽取 k 个组合</span><br><span class="line">    for sample_combin in it.combinations(range(48),k):</span><br><span class="line">        vector &#x3D; []</span><br><span class="line">        for i in range(k):</span><br><span class="line">            vector_18 &#x3D; get_48_to_18(sample_combin[i])</span><br><span class="line">            vector.append(vector_18)</span><br><span class="line">        vector &#x3D; np.array(vector)</span><br><span class="line">        vector &#x3D; vector.any(axis&#x3D;0) # 去冗余操作:按第一个轴方向取或</span><br><span class="line">        vector &#x3D; np.reshape(vector,[18])</span><br><span class="line">        vector &#x3D; vector.tolist()</span><br><span class="line">        num &#x3D; 0</span><br><span class="line">        for i in range(18):</span><br><span class="line">            num +&#x3D; 2 ** i * vector[i] # 0&#x2F;1 二值 18 维映射成 1-2^18 十进制</span><br><span class="line">        num_set.append(num)</span><br><span class="line">        if len(num_set) &gt; 5000000:</span><br><span class="line">            num_set &#x3D; list(set(num_set)) # 长度大于 500W 时取一次集合,防止数组太长导致程序崩溃</span><br><span class="line">    # 最后取一次集合</span><br><span class="line">    num_set &#x3D; list(set(num_set))</span><br><span class="line">    end_time1 &#x3D; datetime.datetime.now()</span><br><span class="line">    print(&#39;k&#x3D;%d时： %d examples&#39; %(k, len(num_set)))</span><br><span class="line">    print(&#39;   用时:&#39;, end_time1 - start_time1)</span><br><span class="line"></span><br><span class="line">start_time0 &#x3D; datetime.datetime.now()</span><br><span class="line">for k in range(1,18):</span><br><span class="line">    start_time1 &#x3D; datetime.datetime.now()</span><br><span class="line">    main(k)</span><br><span class="line">end_time0 &#x3D; datetime.datetime.now()</span><br><span class="line">print(&#39;一共用时&#39;,end_time0 - start_time0)</span><br></pre></td></tr></table></figure></li>
<li>运行结果:<br>  k=1时： 48 examples<pre><code>用时: 0:00:00.001208</code></pre>  k=2时： 879 examples<pre><code>用时: 0:00:00.034517</code></pre>  k=3时： 8223 examples<pre><code>用时: 0:00:00.668103</code></pre>  k=4时： 40911 examples<pre><code>用时: 0:00:09.143752</code></pre>  k=5时： 112962 examples<pre><code>用时: 0:01:35.084796</code></pre>  k=6时： 193998 examples<pre><code>用时: 0:13:07.760253</code></pre>  k=7时： 233640 examples<pre><code>用时: 1:28:37.023360</code></pre></li>
</ul>
<p>由于是穷举法:不去冗余穷举次数$\sum_{i=1}^{k}C^{k}_{48}=(1+1)^k$(二项式定理),随着k越大,计算量也更大,从运行耗时就能看出</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/04/04/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/04/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task5/" class="post-title-link" itemprop="url">Datawhale零基础入门数据挖掘-Task5</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-04 18:25:29 / 修改时间：18:43:14" itemprop="dateCreated datePublished" datetime="2020-04-04T18:25:29+08:00">2020-04-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">数据挖掘及机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>对于多种调参完成的模型进行模型融合</li>
</ul>
<ol>
<li>简单加权融合:</li>
</ol>
<ul>
<li>回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）；</li>
<li>分类：投票（Voting)</li>
<li>综合：排序融合(Rank averaging)，log融合</li>
</ul>
<ol start="2">
<li>stacking/blending:</li>
</ol>
<ul>
<li>构建多层模型，并利用预测结果再拟合预测。</li>
</ul>
<ol start="3">
<li>boosting/bagging（在xgboost，Adaboost,GBDT中已经用到）:</li>
</ol>
<ul>
<li>多树的提升方法</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">## 定义结果的加权平均函数</span><br><span class="line">def Weighted_method(test_pre1,test_pre2,test_pre3,w&#x3D;[1&#x2F;3,1&#x2F;3,1&#x2F;3]):</span><br><span class="line">    Weighted_result &#x3D; w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)</span><br><span class="line">    return Weighted_result</span><br><span class="line"></span><br><span class="line">from sklearn import metrics</span><br><span class="line"># 各模型的预测结果计算MAE</span><br><span class="line">print(&#39;Pred1 MAE:&#39;,metrics.mean_absolute_error(y_test_true, test_pre1))</span><br><span class="line">print(&#39;Pred2 MAE:&#39;,metrics.mean_absolute_error(y_test_true, test_pre2))</span><br><span class="line">print(&#39;Pred3 MAE:&#39;,metrics.mean_absolute_error(y_test_true, test_pre3))</span><br><span class="line"></span><br><span class="line">## 根据加权计算MAE</span><br><span class="line">w &#x3D; [0.3,0.4,0.3] # 定义比重权值</span><br><span class="line">Weighted_pre &#x3D; Weighted_method(test_pre1,test_pre2,test_pre3,w)</span><br><span class="line">print(&#39;Weighted_pre MAE:&#39;,metrics.mean_absolute_error(y_test_true, Weighted_pre))</span><br><span class="line"></span><br><span class="line">## 定义结果的加权平均函数</span><br><span class="line">def Mean_method(test_pre1,test_pre2,test_pre3):</span><br><span class="line">    Mean_result &#x3D; pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis&#x3D;1).mean(axis&#x3D;1)</span><br><span class="line">    return Mean_result</span><br><span class="line"></span><br><span class="line">Mean_pre &#x3D; Mean_method(test_pre1,test_pre2,test_pre3)</span><br><span class="line">print(&#39;Mean_pre MAE:&#39;,metrics.mean_absolute_error(y_test_true, Mean_pre))</span><br><span class="line"></span><br><span class="line">## 定义结果的加权平均函数</span><br><span class="line">def Median_method(test_pre1,test_pre2,test_pre3):</span><br><span class="line">    Median_result &#x3D; pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis&#x3D;1).median(axis&#x3D;1)</span><br><span class="line">    return Median_result</span><br><span class="line"></span><br><span class="line">Median_pre &#x3D; Median_method(test_pre1,test_pre2,test_pre3)</span><br><span class="line">print(&#39;Median_pre MAE:&#39;,metrics.mean_absolute_error(y_test_true, Median_pre))</span><br><span class="line"></span><br><span class="line">from sklearn import linear_model</span><br><span class="line"></span><br><span class="line">def Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2&#x3D; linear_model.LinearRegression()):</span><br><span class="line">    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis&#x3D;1).values,y_train_true)</span><br><span class="line">    Stacking_result &#x3D; model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis&#x3D;1).values)</span><br><span class="line">    return Stacking_result</span><br><span class="line"></span><br><span class="line">## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值</span><br><span class="line">train_reg1 &#x3D; [3.2, 8.2, 9.1, 5.2]</span><br><span class="line">train_reg2 &#x3D; [2.9, 8.1, 9.0, 4.9]</span><br><span class="line">train_reg3 &#x3D; [3.1, 7.9, 9.2, 5.0]</span><br><span class="line"># y_test_true 代表第模型的真实值</span><br><span class="line">y_train_true &#x3D; [3, 8, 9, 5] </span><br><span class="line"></span><br><span class="line">test_pre1 &#x3D; [1.2, 3.2, 2.1, 6.2]</span><br><span class="line">test_pre2 &#x3D; [0.9, 3.1, 2.0, 5.9]</span><br><span class="line">test_pre3 &#x3D; [1.1, 2.9, 2.2, 6.0]</span><br><span class="line"></span><br><span class="line"># y_test_true 代表第模型的真实值</span><br><span class="line">y_test_true &#x3D; [1, 3, 2, 6] </span><br><span class="line"></span><br><span class="line">model_L2&#x3D; linear_model.LinearRegression()</span><br><span class="line">Stacking_pre &#x3D; Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,</span><br><span class="line">                               test_pre1,test_pre2,test_pre3,model_L2)</span><br><span class="line">print(&#39;Stacking_pre MAE:&#39;,metrics.mean_absolute_error(y_test_true, Stacking_pre))</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/04/01/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/01/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task4/" class="post-title-link" itemprop="url">Datawhale零基础入门数据挖掘-Task4</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-01 18:36:04" itemprop="dateCreated datePublished" datetime="2020-04-01T18:36:04+08:00">2020-04-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-03 22:23:40" itemprop="dateModified" datetime="2020-04-03T22:23:40+08:00">2020-04-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">数据挖掘及机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>了解常用的机器学习模型，并掌握机器学习模型的建模与调参流程</li>
</ul>
<h1 id="相关原理"><a href="#相关原理" class="headerlink" title="相关原理"></a>相关原理</h1><h2 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h2><p><a href="https://zhuanlan.zhihu.com/p/49480391" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49480391</a></p>
<h2 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h2><p><a href="https://zhuanlan.zhihu.com/p/65304798" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65304798</a></p>
<h2 id="GBDT模型"><a href="#GBDT模型" class="headerlink" title="GBDT模型"></a>GBDT模型</h2><p><a href="https://zhuanlan.zhihu.com/p/45145899" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/45145899</a></p>
<h2 id="XGBoost模型"><a href="#XGBoost模型" class="headerlink" title="XGBoost模型"></a>XGBoost模型</h2><p><a href="https://zhuanlan.zhihu.com/p/86816771" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/86816771</a></p>
<h2 id="LightGBM模型"><a href="#LightGBM模型" class="headerlink" title="LightGBM模型"></a>LightGBM模型</h2><p><a href="https://zhuanlan.zhihu.com/p/89360721" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/89360721</a></p>
<h2 id="教材推荐"><a href="#教材推荐" class="headerlink" title="教材推荐"></a>教材推荐</h2><ul>
<li>《机器学习》 <a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">https://book.douban.com/subject/26708119/</a></li>
<li>《统计学习方法》 <a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">https://book.douban.com/subject/10590856/</a></li>
<li>《Python大战机器学习》 <a href="https://book.douban.com/subject/26987890/" target="_blank" rel="noopener">https://book.douban.com/subject/26987890/</a></li>
<li>《面向机器学习的特征工程》 <a href="https://book.douban.com/subject/26826639/" target="_blank" rel="noopener">https://book.douban.com/subject/26826639/</a></li>
<li>《数据科学家访谈录》 <a href="https://book.douban.com/subject/30129410/" target="_blank" rel="noopener">https://book.douban.com/subject/30129410/</a></li>
</ul>
<h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间</span><br><span class="line">def reduce_mem_usage(df):</span><br><span class="line">    &quot;&quot;&quot; iterate through all the columns of a dataframe and modify the data type</span><br><span class="line">        to reduce memory usage.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    start_mem &#x3D; df.memory_usage().sum()</span><br><span class="line">    print(&#39;Memory usage of dataframe is &#123;:.2f&#125; MB&#39;.format(start_mem))</span><br><span class="line"></span><br><span class="line">    for col in df.columns:</span><br><span class="line">        col_type &#x3D; df[col].dtype</span><br><span class="line"></span><br><span class="line">        if col_type !&#x3D; object:</span><br><span class="line">            c_min &#x3D; df[col].min()</span><br><span class="line">            c_max &#x3D; df[col].max()</span><br><span class="line">            if str(col_type)[:3] &#x3D;&#x3D; &#39;int&#39;:</span><br><span class="line">                if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.int8)</span><br><span class="line">                elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.int16)</span><br><span class="line">                elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.int32)</span><br><span class="line">                elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.int64)</span><br><span class="line">            else:</span><br><span class="line">                if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.float16)</span><br><span class="line">                elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.float32)</span><br><span class="line">                else:</span><br><span class="line">                    df[col] &#x3D; df[col].astype(np.float64)</span><br><span class="line">        else:</span><br><span class="line">            df[col] &#x3D; df[col].astype(&#39;category&#39;)</span><br><span class="line"></span><br><span class="line">    end_mem &#x3D; df.memory_usage().sum()</span><br><span class="line">    print(&#39;Memory usage after optimization is: &#123;:.2f&#125; MB&#39;.format(end_mem))</span><br><span class="line">    print(&#39;Decreased by &#123;:.1f&#125;%&#39;.format(100 * (start_mem - end_mem) &#x2F; start_mem))</span><br><span class="line">    return df</span><br><span class="line"></span><br><span class="line">sample_feature &#x3D; reduce_mem_usage(pd.read_csv(&quot;data_for_tree.csv&quot;))</span><br></pre></td></tr></table></figure>

<pre><code>Memory usage of dataframe is 62099672.00 MB
Memory usage after optimization is: 16520303.00 MB
Decreased by 73.4%</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 返回 x in sample_feature.columns not include [&#39;price&#39;,&#39;brand&#39;,&#39;model&#39;,&#39;brand&#39;] 的列表</span><br><span class="line">continuous_feature_names &#x3D; [x for x in sample_feature.columns if x not in [&#39;price&#39;,&#39;brand&#39;,&#39;model&#39;,&#39;brand&#39;]]</span><br></pre></td></tr></table></figure>

<h1 id="线性回归-amp-五折交叉验证-amp-模拟真实业务情况"><a href="#线性回归-amp-五折交叉验证-amp-模拟真实业务情况" class="headerlink" title="线性回归 &amp; 五折交叉验证 &amp; 模拟真实业务情况"></a>线性回归 &amp; 五折交叉验证 &amp; 模拟真实业务情况</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sample_feature &#x3D; sample_feature.dropna().replace(&#39;-&#39;, 0).reset_index(drop&#x3D;True)</span><br><span class="line">sample_feature[&#39;notRepairedDamage&#39;] &#x3D; sample_feature[&#39;notRepairedDamage&#39;].astype(np.float32)</span><br><span class="line">train &#x3D; sample_feature[continuous_feature_names + [&#39;price&#39;]]</span><br><span class="line"></span><br><span class="line">train_X &#x3D; train[continuous_feature_names]</span><br><span class="line">train_y &#x3D; train[&#39;price&#39;]</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure>

<h2 id="简单建模"><a href="#简单建模" class="headerlink" title="简单建模"></a>简单建模</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">model &#x3D; LinearRegression(normalize&#x3D;True)</span><br><span class="line"></span><br><span class="line">model &#x3D; model.fit(train_X, train_y)</span><br><span class="line"></span><br><span class="line"># 查看训练的线性回归模型的截距（intercept）与权重(coef)</span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line"></span><br><span class="line">print(sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key&#x3D;lambda x:x[1], reverse&#x3D;True))</span><br></pre></td></tr></table></figure>

<pre><code>intercept:-110670.68277241504

[(&apos;v_6&apos;, 3367064.3416418717), 
(&apos;v_8&apos;, 700675.5609398251),
 (&apos;v_9&apos;, 170630.27723215616), 
(&apos;v_7&apos;, 32322.661931980558), 
(&apos;v_12&apos;, 20473.670796988994), 
(&apos;v_3&apos;, 17868.07954151303), 
(&apos;v_11&apos;, 11474.9389967116), 
(&apos;v_13&apos;, 11261.764560019501), 
(&apos;v_10&apos;, 2683.9200906064084), 
(&apos;gearbox&apos;, 881.8225039250154), 
(&apos;fuelType&apos;, 363.9042507216036), 
(&apos;bodyType&apos;, 189.60271012073036), 
(&apos;city&apos;, 44.94975120522736), 
(&apos;power&apos;, 28.553901616752857), 
(&apos;brand_price_median&apos;, 0.5103728134078609), 
(&apos;brand_price_std&apos;, 0.4503634709263256), 
(&apos;brand_amount&apos;, 0.14881120395065583), 
(&apos;brand_price_max&apos;, 0.0031910186703138638), 
(&apos;SaleID&apos;, 5.355989919860593e-05), 
(&apos;offerType&apos;, 4.397239536046982e-06), 
(&apos;train&apos;, 2.7939677238464355e-07), 
(&apos;seller&apos;, -2.873130142688751e-07), 
(&apos;brand_price_sum&apos;, -2.175006868187596e-05), 
(&apos;name&apos;, -0.0002980012713074109), 
(&apos;used_time&apos;, -0.002515894332880479), 
(&apos;brand_price_average&apos;, -0.404904845101148), 
(&apos;brand_price_min&apos;, -2.2467753486888244), 
(&apos;power_bin&apos;, -34.42064411727887), 
(&apos;v_14&apos;, -274.7841180777388), 
(&apos;kilometer&apos;, -372.89752666073025), 
(&apos;notRepairedDamage&apos;, -495.1903844628239), 
(&apos;v_0&apos;, -2045.0549573558887), 
(&apos;v_5&apos;, -11022.98624082137), 
(&apos;v_4&apos;, -15121.731109860013), 
(&apos;v_2&apos;, -26098.29992055148), 
(&apos;v_1&apos;, -45556.18929726381)]</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">subsample_index &#x3D; np.random.randint(low&#x3D;0, high&#x3D;len(train_y), size&#x3D;50)</span><br><span class="line">#绘制特征v_9的值与标签的散点图，图片发现模型的预测结果（蓝色点）与真实标签（黑色点）的分布差异较大，</span><br><span class="line"># 且部分预测值出现了小于0的情况，说明我们的模型存在一些问题</span><br><span class="line">plt.scatter(train_X[&#39;v_9&#39;][subsample_index], train_y[subsample_index], color&#x3D;&#39;black&#39;)</span><br><span class="line">plt.scatter(train_X[&#39;v_9&#39;][subsample_index], model.predict(train_X.loc[subsample_index]), color&#x3D;&#39;blue&#39;)</span><br><span class="line">plt.xlabel(&#39;v_9&#39;)</span><br><span class="line">plt.ylabel(&#39;price&#39;)</span><br><span class="line">plt.legend([&#39;True Price&#39;,&#39;Predicted Price&#39;],loc&#x3D;&#39;upper right&#39;)</span><br><span class="line">print(&#39;The predicted price is obvious different from true price&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/04/01/G8VrRI.png" alt="1"></p>
<ul>
<li>通过作图我们发现数据的标签（price）呈现长尾分布，不利于我们的建模预测。原因是很多模型都假设数据误差项符合正态分布，而长尾分布的数据违背了这一假设。参考博客：<a href="https://blog.csdn.net/Noob_daniel/article/details/76087829" target="_blank" rel="noopener">https://blog.csdn.net/Noob_daniel/article/details/76087829</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns</span><br><span class="line">print(&#39;It is clear to see the price shows a typical exponential distribution&#39;)</span><br><span class="line">plt.figure(figsize&#x3D;(15,5))</span><br><span class="line">plt.subplot(1,2,1)</span><br><span class="line">sns.distplot(train_y)</span><br><span class="line">plt.subplot(1,2,2)</span><br><span class="line">sns.distplot(train_y[train_y &lt; np.quantile(train_y, 0.9)])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="" alt="2"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 在这里我们对标签进行了 log(x+1) 变换，使标签贴近于正态分布</span><br><span class="line">train_y_ln &#x3D; np.log(train_y + 1)</span><br><span class="line">print(&#39;The transformed price seems like normal distribution&#39;)</span><br><span class="line">plt.figure(figsize&#x3D;(15,5))</span><br><span class="line">plt.subplot(1,2,1)</span><br><span class="line">sns.distplot(train_y_ln)</span><br><span class="line">plt.subplot(1,2,2)</span><br><span class="line">sns.distplot(train_y_ln[train_y_ln &lt; np.quantile(train_y_ln, 0.9)])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="" alt="3"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; model.fit(train_X, train_y_ln)</span><br><span class="line"></span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line">sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key&#x3D;lambda x:x[1], reverse&#x3D;True)</span><br></pre></td></tr></table></figure>

<pre><code>intercept:18.750748443060488

[(&apos;v_9&apos;, 8.052410408822315), 
(&apos;v_5&apos;, 5.764240780403914), 
(&apos;v_12&apos;, 1.618206098241706), 
(&apos;v_1&apos;, 1.479831064546508), 
(&apos;v_11&apos;, 1.166900417358536), 
(&apos;v_13&apos;, 0.9404706327194452), 
(&apos;v_7&apos;, 0.7137281645215736), 
(&apos;v_3&apos;, 0.6837863827349204), 
(&apos;v_0&apos;, 0.00850050520973589), 
(&apos;power_bin&apos;, 0.008497968353528977), 
(&apos;gearbox&apos;, 0.007922378343285602), 
(&apos;fuelType&apos;, 0.006684768936305926), 
(&apos;bodyType&apos;, 0.004523520651791603), 
(&apos;power&apos;, 0.0007161895389359644), 
(&apos;brand_price_min&apos;, 3.334354528992352e-05), 
(&apos;brand_amount&apos;, 2.897880289491835e-06), 
(&apos;brand_price_median&apos;, 1.2571187771074404e-06), 
(&apos;brand_price_std&apos;, 6.659170007178332e-07), 
(&apos;brand_price_max&apos;, 6.194957302457314e-07), 
(&apos;brand_price_average&apos;, 5.999348706659352e-07), 
(&apos;SaleID&apos;, 2.1194159119234957e-08), 
(&apos;seller&apos;, 1.6262902136077173e-10), 
(&apos;offerType&apos;, 1.1036149771825876e-10), 
(&apos;train&apos;, 6.707523425575346e-12), 
(&apos;brand_price_sum&apos;, -1.5126514245669237e-10), 
(&apos;name&apos;, -7.015511195846627e-08), 
(&apos;used_time&apos;, -4.122477016270915e-06), 
(&apos;city&apos;, -0.002218783709616053), 
(&apos;v_14&apos;, -0.004234189820672137), 
(&apos;kilometer&apos;, -0.013835867353556136), 
(&apos;notRepairedDamage&apos;, -0.27027942480393996), 
(&apos;v_4&apos;, -0.8315697362911634), 
(&apos;v_2&apos;, -0.9470821267759207), 
(&apos;v_10&apos;, -1.6261468392032863), 
(&apos;v_8&apos;, -40.34300817115224), 
(&apos;v_6&apos;, -238.79035497319248)]</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#再次进行可视化，发现预测结果与真实值较为接近，且未出现异常状况</span><br><span class="line">plt.scatter(train_X[&#39;v_9&#39;][subsample_index], train_y[subsample_index], color&#x3D;&#39;black&#39;)</span><br><span class="line">plt.scatter(train_X[&#39;v_9&#39;][subsample_index], np.exp(model.predict(train_X.loc[subsample_index])), color&#x3D;&#39;blue&#39;)</span><br><span class="line">plt.xlabel(&#39;v_9&#39;)</span><br><span class="line">plt.ylabel(&#39;price&#39;)</span><br><span class="line">plt.legend([&#39;True Price&#39;,&#39;Predicted Price&#39;],loc&#x3D;&#39;upper right&#39;)</span><br><span class="line">print(&#39;The predicted price seems normal after np.log transforming&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="" alt="4"></p>
<h2 id="五折交叉验证"><a href="#五折交叉验证" class="headerlink" title="五折交叉验证"></a>五折交叉验证</h2><blockquote>
<p>在使用训练集对参数进行训练的时候，经常会发现人们通常会将一整个训练集分为三个部分（比如mnist手写训练集）。一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。这其实是为了保证训练效果而特意设置的。其中测试集很好理解，其实就是完全不参与训练的数据，仅仅用来观测测试效果的数&gt;&gt;据。而训练集和评估集则牵涉到下面的知识了。</p>
</blockquote>
<blockquote>
<p>因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这一部分不参加训练）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证（Cross Validation）</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">##使用线性回归模型，对未处理标签的特征数据进行五折交叉验证</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.metrics import mean_absolute_error,  make_scorer</span><br><span class="line">def log_transfer(func):</span><br><span class="line">    def wrapper(y, yhat):</span><br><span class="line">        result &#x3D; func(np.log(y), np.nan_to_num(np.log(yhat)))</span><br><span class="line">        return result</span><br><span class="line">    return wrapper</span><br><span class="line"></span><br><span class="line">scores &#x3D; cross_val_score(model, X&#x3D;train_X, y&#x3D;train_y, verbose&#x3D;1, cv &#x3D; 5, scoring&#x3D;make_scorer(log_transfer(mean_absolute_error)))</span><br></pre></td></tr></table></figure>

<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.7s finished</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&#39;AVG:&#39;, np.mean(scores))</span><br></pre></td></tr></table></figure>

<pre><code>AVG: 1.3658024027748357</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#使用线性回归模型，对处理过标签的特征数据进行五折交叉验证（</span><br><span class="line">scores &#x3D; cross_val_score(model, X&#x3D;train_X, y&#x3D;train_y_ln, verbose&#x3D;1, cv &#x3D; 5, scoring&#x3D;make_scorer(mean_absolute_error))</span><br></pre></td></tr></table></figure>

<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.8s finished</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&#39;AVG:&#39;, np.mean(scores))</span><br></pre></td></tr></table></figure>

<pre><code>AVG: 0.19325301753940502</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scores &#x3D; pd.DataFrame(scores.reshape(1,-1))</span><br><span class="line">scores.columns &#x3D; [&#39;cv&#39; + str(x) for x in range(1, 6)]</span><br><span class="line">scores.index &#x3D; [&#39;MAE&#39;]</span><br><span class="line">print(scores)</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th></th>
<th>cv1</th>
<th>cv2</th>
<th>cv3</th>
<th>cv4</th>
<th>cv5</th>
</tr>
</thead>
<tbody><tr>
<td>MAE</td>
<td>0.190792</td>
<td>0.193758</td>
<td>0.194132</td>
<td>0.191825</td>
<td>0.195758</td>
</tr>
</tbody></table>
<h2 id="模拟真实业务情况"><a href="#模拟真实业务情况" class="headerlink" title="模拟真实业务情况"></a>模拟真实业务情况</h2><blockquote>
<p>但在事实上，由于我们并不具有预知未来的能力，五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况。通过2018年的二手车价格预测2017年的二手车价格，这显然是不合理的，因此我们还可以采用时间顺序对数据集进行分隔。在本例中，我们选用靠前时间的4/5样本当作训练集，靠后时间的1/5当作验证集，最终结果与五折交叉验证差距不大</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 采用时间顺序对数据集进行分隔 选用靠前时间的4&#x2F;5样本当作训练集，靠后时间的1&#x2F;5当作验证集</span><br><span class="line">import datetime</span><br><span class="line">sample_feature &#x3D; sample_feature.reset_index(drop&#x3D;True)</span><br><span class="line">split_point &#x3D; len(sample_feature) &#x2F;&#x2F; 5 * 4 # 取整除 - 返回商的整数部分（向下取整）</span><br><span class="line"></span><br><span class="line">train &#x3D; sample_feature.loc[:split_point].dropna()</span><br><span class="line">val &#x3D; sample_feature.loc[split_point:].dropna()</span><br><span class="line"></span><br><span class="line">train_X &#x3D; train[continuous_feature_names]</span><br><span class="line">train_y_ln &#x3D; np.log(train[&#39;price&#39;] + 1)</span><br><span class="line">val_X &#x3D; val[continuous_feature_names]</span><br><span class="line">val_y_ln &#x3D; np.log(val[&#39;price&#39;] + 1)</span><br><span class="line"></span><br><span class="line">model &#x3D; model.fit(train_X, train_y_ln)</span><br><span class="line">print(mean_absolute_error(val_y_ln, model.predict(val_X)))</span><br></pre></td></tr></table></figure>

<pre><code>0.19577667229471246</code></pre><h2 id="绘制学习率曲线与验证曲线"><a href="#绘制学习率曲线与验证曲线" class="headerlink" title="绘制学习率曲线与验证曲线"></a>绘制学习率曲线与验证曲线</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># 绘制学习率曲线与验证曲线</span><br><span class="line">from sklearn.model_selection import learning_curve, validation_curve</span><br><span class="line">def plot_learning_curve(estimator, title, X, y, ylim&#x3D;None, cv&#x3D;None,n_jobs&#x3D;1, train_size&#x3D;np.linspace(.1, 1.0, 5 )):</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(title)</span><br><span class="line">    if ylim is not None:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(&#39;Training example&#39;)</span><br><span class="line">    plt.ylabel(&#39;score&#39;)</span><br><span class="line">    train_sizes, train_scores, test_scores &#x3D; learning_curve(estimator, X, y, cv&#x3D;cv, n_jobs&#x3D;n_jobs, train_sizes&#x3D;train_size, scoring &#x3D; make_scorer(mean_absolute_error))</span><br><span class="line">    train_scores_mean &#x3D; np.mean(train_scores, axis&#x3D;1)</span><br><span class="line">    train_scores_std &#x3D; np.std(train_scores, axis&#x3D;1)</span><br><span class="line">    test_scores_mean &#x3D; np.mean(test_scores, axis&#x3D;1)</span><br><span class="line">    test_scores_std &#x3D; np.std(test_scores, axis&#x3D;1)</span><br><span class="line">    plt.grid()#区域</span><br><span class="line"></span><br><span class="line">    # x：第一个参数表示覆盖的区域，我直接复制为x，表示整个x都覆盖</span><br><span class="line">    # 0：表示覆盖的下限</span><br><span class="line">    # y：表示覆盖的上限是y这个曲线</span><br><span class="line">    # facecolor：覆盖区域的颜色</span><br><span class="line">    # alpha：覆盖区域的透明度[0,1],其值越大，表示越不透明</span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha&#x3D;0.1,</span><br><span class="line">                     color&#x3D;&quot;r&quot;)</span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, alpha&#x3D;0.1,</span><br><span class="line">                     color&#x3D;&quot;g&quot;)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, &#39;o-&#39;, color&#x3D;&#39;r&#39;,</span><br><span class="line">             label&#x3D;&quot;Training score&quot;)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean,&#39;o-&#39;,color&#x3D;&quot;g&quot;,</span><br><span class="line">             label&#x3D;&quot;Cross-validation score&quot;)</span><br><span class="line">    plt.legend(loc&#x3D;&quot;best&quot;)</span><br><span class="line">    return plt</span><br><span class="line"></span><br><span class="line">plot_learning_curve(LinearRegression(), &#39;Liner_model&#39;, train_X[:1000], train_y_ln[:1000], ylim&#x3D;(0.0, 0.5), cv&#x3D;5, n_jobs&#x3D;1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="" alt="5"></p>
<h1 id="多种模型对比"><a href="#多种模型对比" class="headerlink" title="多种模型对比"></a>多种模型对比</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train &#x3D; sample_feature[continuous_feature_names + [&#39;price&#39;]].dropna()</span><br><span class="line"></span><br><span class="line">train_X &#x3D; train[continuous_feature_names]</span><br><span class="line">train_y &#x3D; train[&#39;price&#39;]</span><br><span class="line">train_y_ln &#x3D; np.log(train_y + 1)</span><br></pre></td></tr></table></figure>

<h2 id="线性模型-amp-嵌入式特征选择"><a href="#线性模型-amp-嵌入式特征选择" class="headerlink" title="线性模型 &amp; 嵌入式特征选择"></a>线性模型 &amp; 嵌入式特征选择</h2><ul>
<li>本章节默认，学习者已经了解关于过拟合、模型复杂度、正则化等概念。否则请寻找相关资料或参考如下连接：</li>
</ul>
<blockquote>
<p>用简单易懂的语言描述「过拟合 overfitting」<a href="https://www.zhihu.com/question/32246256/answer/55320482" target="_blank" rel="noopener">https://www.zhihu.com/question/32246256/answer/55320482</a><br>模型复杂度与模型的泛化能力 <a href="http://yangyingming.com/article/434/" target="_blank" rel="noopener">http://yangyingming.com/article/434/</a><br>正则化的直观理解 <a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">https://blog.csdn.net/jinping_shi/article/details/52433975</a></p>
</blockquote>
<p>在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是L1正则化与L2正则化。在对线性回归模型加入两种正则化方法后，他们分别变成了Lasso回归与岭(Ridge)回归。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 线性模型 &amp; 嵌入式特征选择</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.linear_model import Ridge</span><br><span class="line">from sklearn.linear_model import Lasso</span><br><span class="line"></span><br><span class="line">models &#x3D; [LinearRegression(),</span><br><span class="line">          Ridge(),</span><br><span class="line">          Lasso()]</span><br><span class="line"></span><br><span class="line">result &#x3D; dict()</span><br><span class="line">for model in models:</span><br><span class="line">    model_name &#x3D; str(model).split(&#39;(&#39;)[0]</span><br><span class="line">    scores &#x3D; cross_val_score(model, X&#x3D;train_X, y&#x3D;train_y_ln, verbose&#x3D;0, cv &#x3D; 5, scoring&#x3D;make_scorer(mean_absolute_error))</span><br><span class="line">    result[model_name] &#x3D; scores</span><br><span class="line">    print(model_name + &#39; is finished&#39;)</span><br></pre></td></tr></table></figure>

<pre><code>LinearRegression is finished
Ridge is finished
Lasso is finished</code></pre><ul>
<li>对三种方法的效果对比</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 对三种方法的效果对比</span><br><span class="line">result &#x3D; pd.DataFrame(result)</span><br><span class="line">result.index &#x3D; [&#39;cv&#39; + str(x) for x in range(1, 6)]</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th></th>
<th>LinearRegression</th>
<th>Ridge</th>
<th>Lasso</th>
</tr>
</thead>
<tbody><tr>
<td>cv1</td>
<td>0.190792</td>
<td>0.194832</td>
<td>0.383899</td>
</tr>
<tr>
<td>cv2</td>
<td>0.193758</td>
<td>0.197632</td>
<td>0.381893</td>
</tr>
<tr>
<td>cv3</td>
<td>0.194132</td>
<td>0.198123</td>
<td>0.384090</td>
</tr>
<tr>
<td>cv4</td>
<td>0.191825</td>
<td>0.195670</td>
<td>0.380526</td>
</tr>
<tr>
<td>cv5</td>
<td>0.195758</td>
<td>0.199676</td>
<td>0.383611</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; LinearRegression().fit(train_X, train_y_ln)</span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line">sns.barplot(abs(model.coef_), continuous_feature_names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>intercept:18.75072374836874<br><img src="" alt="6"></p>
<p>L2正则化在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; Ridge().fit(train_X, train_y_ln)</span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line">sns.barplot(abs(model.coef_), continuous_feature_names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>intercept:4.671710763117783<br><img src="" alt="7"></p>
<p>L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。如下图，我们发现power与userd_time特征非常重要</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; Lasso().fit(train_X, train_y_ln)</span><br><span class="line">print(&#39;intercept:&#39;+ str(model.intercept_))</span><br><span class="line">sns.barplot(abs(model.coef_), continuous_feature_names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>intercept:8.672182470075398<br><img src="" alt="8"></p>
<p>除此之外，决策树通过信息熵或GINI指数选择分裂节点时，优先选择的分裂特征也更加重要，这同样是一种特征选择的方法。XGBoost与LightGBM模型中的model_importance指标正是基于此计算的</p>
<h2 id="非线性模型"><a href="#非线性模型" class="headerlink" title="非线性模型"></a>非线性模型</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/03/28/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/28/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task3/" class="post-title-link" itemprop="url">Datawhale零基础入门数据挖掘-Task3</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-28 17:51:42" itemprop="dateCreated datePublished" datetime="2020-03-28T17:51:42+08:00">2020-03-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-30 22:42:14" itemprop="dateModified" datetime="2020-03-30T22:42:14+08:00">2020-03-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">数据挖掘及机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>25k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>23 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>特征工程：对于特征进行进一步分析，并对于数据进行处理</li>
</ul>
<h1 id="常见的特征工程包括"><a href="#常见的特征工程包括" class="headerlink" title="常见的特征工程包括"></a>常见的特征工程包括</h1><ol>
<li>异常处理</li>
</ol>
<ul>
<li>通过箱线图（或 3-Sigma）分析删除异常值；</li>
<li>BOX-COX 转换（处理有偏分布）；</li>
<li>长尾截断；</li>
</ul>
<ol start="2">
<li>特征归一化/标准化：</li>
</ol>
<ul>
<li>标准化（转换为标准正态分布）；</li>
<li>归一化（抓换到 [0,1] 区间）；</li>
<li>针对幂律分布，可以采用公式：$log(\frac{1+x}{1+median})$</li>
</ul>
<ol start="3">
<li>数据分桶：</li>
</ol>
<ul>
<li>等频分桶；</li>
<li>等距分桶；</li>
<li>Best-KS 分桶（类似利用基尼指数进行二分类）；</li>
<li>卡方分桶；</li>
</ul>
<ol start="4">
<li>缺失值处理：</li>
</ol>
<ul>
<li>不处理（针对类似 XGBoost 等树模型）；</li>
<li>删除（缺失数据太多）；</li>
<li>插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等；</li>
<li>分箱，缺失值一个箱；</li>
</ul>
<ol start="5">
<li>特征构造：</li>
</ol>
<ul>
<li>构造统计量特征，报告计数、求和、比例、标准差等；</li>
<li>时间特征，包括相对时间和绝对时间，节假日，双休日等；</li>
<li>地理信息，包括分箱，分布编码等方法；</li>
<li>非线性变换，包括 log/ 平方/ 根号等；</li>
<li>特征组合，特征交叉；</li>
<li>仁者见仁，智者见智。</li>
</ul>
<ol start="6">
<li>特征筛选</li>
</ol>
<ul>
<li>过滤式（filter）：先对数据进行特征选择，然后在训练学习器，常见的方法有 Relief/方差选择发/相关系数法/卡方检验法/互信息法；</li>
<li>包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有 LVM（Las Vegas Wrapper） ；</li>
<li>嵌入式（embedding）：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有 lasso 回归；</li>
</ul>
<ol start="7">
<li>降维</li>
</ol>
<ul>
<li>PCA/ LDA/ ICA；</li>
<li>特征选择也是一种降维。</li>
</ul>
<h1 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">from operator import itemgetter</span><br><span class="line"></span><br><span class="line">Train_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&quot;, sep&#x3D;&quot; &quot;)</span><br><span class="line">Test_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&quot;, sep&#x3D;&quot; &quot;)</span><br><span class="line"></span><br><span class="line">print(Train_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(150000, 31)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data.head())</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th></th>
<th>SaleID</th>
<th>name</th>
<th>regDate</th>
<th>model</th>
<th>…</th>
<th>v_11</th>
<th>v_12</th>
<th>v_13</th>
<th>v_14</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>736</td>
<td>20040402</td>
<td>30.0</td>
<td>…</td>
<td>2.804097</td>
<td>-2.420821</td>
<td>0.795292</td>
<td>0.914762</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2262</td>
<td>20030301</td>
<td>40.0</td>
<td>…</td>
<td>2.096338</td>
<td>-1.030483</td>
<td>-1.722674</td>
<td>0.245522</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>14874</td>
<td>20040403</td>
<td>115.0</td>
<td>…</td>
<td>1.803559</td>
<td>1.565330</td>
<td>-0.832687</td>
<td>-0.229963</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>71865</td>
<td>19960908</td>
<td>109.0</td>
<td>…</td>
<td>1.285940</td>
<td>-0.501868</td>
<td>-2.438353</td>
<td>-0.478699</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
<td>111080</td>
<td>20120103</td>
<td>110.0</td>
<td>…</td>
<td>0.910783</td>
<td>0.931110</td>
<td>2.834518</td>
<td>1.923482</td>
</tr>
</tbody></table>
<p>[5 rows x 31 columns]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data.columns)</span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;SaleID&apos;, &apos;name&apos;, &apos;regDate&apos;, &apos;model&apos;, &apos;brand&apos;, &apos;bodyType&apos;, &apos;fuelType&apos;,
       &apos;gearbox&apos;, &apos;power&apos;, &apos;kilometer&apos;, &apos;notRepairedDamage&apos;, &apos;regionCode&apos;,
       &apos;seller&apos;, &apos;offerType&apos;, &apos;creatDate&apos;, &apos;price&apos;, &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;, &apos;v_3&apos;,
       &apos;v_4&apos;, &apos;v_5&apos;, &apos;v_6&apos;, &apos;v_7&apos;, &apos;v_8&apos;, &apos;v_9&apos;, &apos;v_10&apos;, &apos;v_11&apos;, &apos;v_12&apos;,
       &apos;v_13&apos;, &apos;v_14&apos;],
      dtype=&apos;object&apos;)</code></pre><h1 id="删除异常值"><a href="#删除异常值" class="headerlink" title="删除异常值"></a>删除异常值</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">## 删除异常值</span><br><span class="line"># 这里我包装了一个异常值处理的代码，可以随便调用</span><br><span class="line">def outliers_proc(data, col_name, scale&#x3D;3):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    用于清洗异常值，默认用box_plot(scale&#x3D;3)进行清洗</span><br><span class="line">    :param data:接受 pandas 数据格式</span><br><span class="line">    :param col_name:pandas 列名</span><br><span class="line">    :param scale:尺度</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def box_plot_outliers(data_ser, box_scale):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        利用箱线图去除异常值</span><br><span class="line">        :param data_ser:接收 pandas.Series 数据格式</span><br><span class="line">        :param box_scale: 箱线图尺度 （规定大于上四分位数1.5倍四分位数差 的值，或者小于下四分位数1.5倍四分位数差的值，划为异常值）</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        iqr &#x3D; box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25)) # 3倍四分位数差</span><br><span class="line">        val_low &#x3D; data_ser.quantile(0.25) - iqr # 下限&#x3D;Q1-3IQR</span><br><span class="line">        val_up &#x3D; data_ser.quantile(0.75) + iqr # 上限&#x3D;Q3+3IQR</span><br><span class="line">        rule_low &#x3D; (data_ser &lt; val_low)</span><br><span class="line">        rule_up &#x3D; (data_ser &gt; val_up) # 返回 pandas.Series 中对应值的bool</span><br><span class="line">        return (rule_low, rule_up), (val_low, val_up)</span><br><span class="line"></span><br><span class="line">    data_n &#x3D; data.copy() # copy 数据</span><br><span class="line">    data_series &#x3D; data_n[col_name] # 返回指定 col_name 数据</span><br><span class="line">    rule, value &#x3D; box_plot_outliers(data_series, box_scale&#x3D;scale)</span><br><span class="line">    index &#x3D; np.arange(data_series.shape[0])[rule[0]|rule[1]] # 返回rule_low, rule_up中为True的下标的列表</span><br><span class="line">    print(&quot;Delete number is:&#123;&#125;&quot;.format(len(index))) # 打印下标列表中个数</span><br><span class="line">    data_n &#x3D; data_n.drop(index) # 删除(删除后下标没变)</span><br><span class="line">    data_n.reset_index(drop&#x3D;True, inplace&#x3D;True) # 重置索引（drop&#x3D;True删除原来的索引;inplace&#x3D;True当前修改状态应用到原来Series中）</span><br><span class="line">    print(&quot;Now column number is:&#123;&#125;&quot;.format(data_n.shape[0])) # 查看删除后的数据个数</span><br><span class="line">    index_low &#x3D; np.arange(data_series.shape[0])[rule[0]]</span><br><span class="line">    outliers &#x3D; data_series.iloc[index_low] # ilco-按下标进行索引</span><br><span class="line">    print(&quot;Description of data larger than the lower bound is:&quot;)</span><br><span class="line">    print(pd.Series(outliers).describe())</span><br><span class="line">    index_up &#x3D; np.arange(data_series.shape[0])[rule[1]]</span><br><span class="line">    outliers &#x3D; data_series.iloc[index_up]</span><br><span class="line">    print(&quot;Description of data larger than the upper bound is:&quot;)</span><br><span class="line">    print(pd.Series(outliers).describe())</span><br><span class="line"></span><br><span class="line">    fig, ax &#x3D; plt.subplots(1, 2, figsize&#x3D;(10, 7)) # 创建子图:1行2列</span><br><span class="line">    sns.boxplot(y&#x3D;data[col_name], data&#x3D;data, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[0]) # 箱线图</span><br><span class="line">    sns.boxplot(y&#x3D;data_n[col_name], data&#x3D;data_n, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[1])</span><br><span class="line">    plt.show()</span><br><span class="line">    return data_n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 我们可以删掉一些异常数据,以 power 为例</span><br><span class="line">## 这里删不删可以自行判断</span><br><span class="line">## 但是注意 Test 的数据不能删</span><br><span class="line">Train_data &#x3D; outliers_proc(Train_data, &quot;power&quot;, scale&#x3D;3)</span><br></pre></td></tr></table></figure>

<pre><code>Delete number is:963
Now column number is:149037
Description of data larger than the lower bound is:
count    0.0
mean     NaN
std      NaN
min      NaN
25%      NaN
50%      NaN
75%      NaN
max      NaN
Name: power, dtype: float64
Description of data larger than the upper bound is:
count      963.000000
mean       846.836968
std       1929.418081
min        376.000000
25%        400.000000
50%        436.000000
75%        514.000000
max      19312.000000
Name: power, dtype: float64</code></pre><p><img src="https://s1.ax1x.com/2020/03/28/GAATiR.png" alt="1"></p>
<h1 id="特征构造"><a href="#特征构造" class="headerlink" title="特征构造"></a>特征构造</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 训练集和测试集放在一起，方便构造特征</span><br><span class="line">Train_data[&quot;train&quot;] &#x3D; 1 # 添加新字段，并设置值为1</span><br><span class="line">Test_data[&quot;train&quot;] &#x3D; 1</span><br><span class="line">data &#x3D; pd.concat([Train_data,Test_data],ignore_index&#x3D;True) # 连接函数 ignore_index&#x3D;True重置索引</span><br><span class="line"></span><br><span class="line"># 使用时间：data[&quot;createDate&quot;] - data[&quot;regDate&quot;], 反应汽车使用时间，一般来说价格与使用时间成反比</span><br><span class="line"># 不过要注意, 数据里有时间出错的格式, 所以我们需要 errors &#x3D; &quot;coerce&quot;</span><br><span class="line">data[&quot;used_time&quot;] &#x3D; (pd.to_datetime(data[&quot;creatDate&quot;], format&#x3D;&quot;%Y%m%d&quot;, errors&#x3D;&quot;coerce&quot;) -</span><br><span class="line">pd.to_datetime(data[&quot;regDate&quot;],format&#x3D;&quot;%Y%m%d&quot;,errors&#x3D;&quot;coerce&quot;)).dt.days # to_datetime将参数转换为日期 dt.days每个元素的天数</span><br><span class="line"></span><br><span class="line"># 看一下空数据, 有 15k 个样本的时间有问题的, 我们可以选择删除, 也可以选择放着</span><br><span class="line"># 但是这里不建议删除, 因为删除缺失数据占总样本量过大, 7.5%</span><br><span class="line"># 我们可以先放着, 因为如果我们 XGBoost 之类的决策树, 其本身就能处理缺失值, 所以可以不用管</span><br><span class="line">print(data[&quot;used_time&quot;].isnull().sum())</span><br></pre></td></tr></table></figure>

<pre><code>15072</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 从邮编中提取城市信息, 相当于加入了先验知识</span><br><span class="line">#print(data[&quot;regionCode&quot;])</span><br><span class="line"># 增加city 字段, 并从 regionCode 值的倒数第三位切片(apply 对regionCode每个元素运行指定运算 lambda 匿名函数)</span><br><span class="line">data[&quot;city&quot;] &#x3D; data[&quot;regionCode&quot;].apply(lambda x : str(x)[:-3])</span><br><span class="line"></span><br><span class="line"># 计算某品牌的销售统计量, 还可以计算其他特征的统计量</span><br><span class="line"># 这里以 train 的数据计算统计量</span><br><span class="line">Train_gb &#x3D; Train_data.groupby(&quot;brand&quot;) # 分组</span><br><span class="line">all_info &#x3D; &#123;&#125;</span><br><span class="line">for kind, kind_data in Train_gb:</span><br><span class="line">    info &#x3D; &#123;&#125;</span><br><span class="line">    kind_data &#x3D; kind_data[kind_data[&quot;price&quot;] &gt; 0]</span><br><span class="line">    info[&quot;brand_amount&quot;] &#x3D; len(kind_data)</span><br><span class="line">    info[&quot;brand_price_max&quot;] &#x3D; kind_data.price.max()</span><br><span class="line">    info[&quot;brand_price_median&quot;] &#x3D; kind_data.price.median()</span><br><span class="line">    info[&quot;brand_price_min&quot;] &#x3D; kind_data.price.min()</span><br><span class="line">    info[&quot;brand_price_sum&quot;] &#x3D; kind_data.price.sum()</span><br><span class="line">    info[&quot;brand_price_std&quot;] &#x3D; kind_data.price.std() # 样本方差</span><br><span class="line">    info[&quot;brand_price_average&quot;] &#x3D; round(kind_data.price.sum() &#x2F; (len(kind_data)+1), 2) # round(2)取近似值保留两位数</span><br><span class="line">    all_info[kind] &#x3D; info</span><br><span class="line"></span><br><span class="line">brand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;:&quot;brand&quot;&#125;) # T转置   reset_index还原索引  rename并修改列名</span><br><span class="line">data &#x3D; data.merge(brand_fe, how&#x3D;&quot;left&quot;, on&#x3D;&quot;brand&quot;) # 合并数据 data链接在brand_fe &quot;brand&quot;字段左边</span><br></pre></td></tr></table></figure>

<h2 id="数据分桶"><a href="#数据分桶" class="headerlink" title="数据分桶"></a>数据分桶</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 数据分桶 以 power 为例</span><br><span class="line"># 这时候缺失值也进桶了</span><br><span class="line"># 为什么要分桶：</span><br><span class="line"># 1. 离散后稀疏向量内积乘法运算速度更快, 计算结果也方便存储, 容易扩展</span><br><span class="line"># 2. 离散后的特征对异常值更具鲁棒性, 如 age&gt;30 为 1 否则为 0 , 对于年龄为 200 的也不会对模型造成很大的干扰</span><br><span class="line"># 3. LR 属于广义线性模型, 表达能力有限, 经过离散化后, 每个变量有单独的权重, 这相当于引入了非线性, 能够提升模型的表达能力, 加大拟合</span><br><span class="line"># 4. 离散后特征可以进行特征交叉, 提升表达能力, 由 M+N 个变量变成　Ｍ*N 个变量, 进一步引入非线性, 提升了表达能力</span><br><span class="line"># 5. 特征离散后模型更稳定, 如用户年龄区间, 不会因为用户年龄长了一岁就变化</span><br><span class="line"># 当然还有很多原因,　LightGBM 在改进 XGBoost 时就增加了数据分桶, 增强了模型的泛化性</span><br><span class="line"></span><br><span class="line">bin &#x3D; [i*10 for i in range(31)]</span><br><span class="line">data[&quot;power_bin&quot;] &#x3D; pd.cut(data[&quot;power&quot;], bin, labels&#x3D;False) # 分桶 cut切分数据(必须是一维的) bin定义区间 labels&#x3D;False返回第几个bin（从0开始）</span><br><span class="line">print(data[[&quot;power_bin&quot;, &quot;power&quot;]].head())</span><br></pre></td></tr></table></figure>

<pre><code> power_bin  power
0        5.0     60
1        NaN      0
2       16.0    163
3       19.0    193
4        6.0     68</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 删除不需要的数据</span><br><span class="line">data &#x3D; data.drop([&quot;creatDate&quot;, &quot;regDate&quot;, &quot;regionCode&quot;], axis&#x3D;1) # drop函数默认删除行，列需要加axis &#x3D; 1</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(199037, 39)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(data.columns)</span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;SaleID&apos;, &apos;name&apos;, &apos;model&apos;, &apos;brand&apos;, &apos;bodyType&apos;, &apos;fuelType&apos;, &apos;gearbox&apos;,
       &apos;power&apos;, &apos;kilometer&apos;, &apos;notRepairedDamage&apos;, &apos;seller&apos;, &apos;offerType&apos;,
       &apos;price&apos;, &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;, &apos;v_3&apos;, &apos;v_4&apos;, &apos;v_5&apos;, &apos;v_6&apos;, &apos;v_7&apos;, &apos;v_8&apos;,
       &apos;v_9&apos;, &apos;v_10&apos;, &apos;v_11&apos;, &apos;v_12&apos;, &apos;v_13&apos;, &apos;v_14&apos;, &apos;train&apos;, &apos;used_time&apos;,
       &apos;city&apos;, &apos;brand_amount&apos;, &apos;brand_price_max&apos;, &apos;brand_price_median&apos;,
       &apos;brand_price_min&apos;, &apos;brand_price_sum&apos;, &apos;brand_price_std&apos;,
       &apos;brand_price_average&apos;, &apos;power_bin&apos;],
      dtype=&apos;object&apos;)</code></pre><h2 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 目前的数据其实已经可以给树模型使用了, 所以我们导出一下</span><br><span class="line">data.to_csv(&quot;data_for_tree.csv&quot;, index&#x3D;0) # index&#x3D;0不保存行索引</span><br></pre></td></tr></table></figure>
<h2 id="特征构造-1"><a href="#特征构造-1" class="headerlink" title="特征构造"></a>特征构造</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 我们可以再构造一份特征给 LR NN 之类的模型用</span><br><span class="line"># 之所以分开构造是因为, 不同模型对数据的要求不同</span><br><span class="line"># 先看下数据分布：</span><br><span class="line">data[&quot;power&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/Guw0jH.png" alt="2"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 我们刚刚已经对 train 进行异常值处理了，但是现在还有这么奇怪的分布是因为 test 中的 power 异常值，</span><br><span class="line"># 所以我们其实刚刚 train 中的 power 异常值不删为好，可以用长尾分布截断来代替</span><br><span class="line">Train_data[&quot;power&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/Guw6Et.png" alt="3"></p>
<h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 我们对其取 log, 再做归一化</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line"># 将数据的每一个特征缩放到给定的范围，将数据的每一个属性值减去其最小值，然后除以其极差（最大值 - 最小值）</span><br><span class="line">min_max_scaler &#x3D; preprocessing.MinMaxScaler()</span><br><span class="line">data[&quot;power&quot;] &#x3D; np.log(data[&quot;power&quot;] + 1)</span><br><span class="line"># 归一化：(0,1)标准化</span><br><span class="line">data[&quot;power&quot;] &#x3D; ((data[&quot;power&quot;] - np.min(data[&quot;power&quot;])) &#x2F; (np.max(data[&quot;power&quot;]) - np.min(data[&quot;power&quot;])))</span><br><span class="line">data[&quot;power&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/GuwcUP.png" alt="4"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># km 的比较正常, 应该已经做过分桶了</span><br><span class="line">data[&quot;kilometer&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/GuwRC8.png" alt="5"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 所以可以直接作归一化</span><br><span class="line">data[&quot;kilometer&quot;] &#x3D; ((data[&quot;kilometer&quot;] - np.min(data[&quot;kilometer&quot;])) &#x2F;</span><br><span class="line">                     (np.max(data[&quot;kilometer&quot;]) - np.min(data[&quot;kilometer&quot;])))</span><br><span class="line">data[&quot;kilometer&quot;].plot.hist()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/Guwfgg.png" alt="6"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># 除此之外 还有我们刚刚构造的统计量特征：</span><br><span class="line"># &#39;brand_amount&#39;, &#39;brand_price_average&#39;, &#39;brand_price_max&#39;,</span><br><span class="line"># &#39;brand_price_median&#39;, &#39;brand_price_min&#39;, &#39;brand_price_std&#39;,</span><br><span class="line"># &#39;brand_price_sum&#39;</span><br><span class="line"># 这里不再一一举例分析了，直接做变换，</span><br><span class="line">def max_min(x):</span><br><span class="line">    return (x - np.min(x)) &#x2F; (np.max(x) - np.min(x))</span><br><span class="line"></span><br><span class="line">data[&#39;brand_amount&#39;] &#x3D; ((data[&#39;brand_amount&#39;] - np.min(data[&#39;brand_amount&#39;])) &#x2F;</span><br><span class="line">                        (np.max(data[&#39;brand_amount&#39;]) - np.min(data[&#39;brand_amount&#39;])))</span><br><span class="line">data[&#39;brand_price_average&#39;] &#x3D; ((data[&#39;brand_price_average&#39;] - np.min(data[&#39;brand_price_average&#39;])) &#x2F;</span><br><span class="line">                               (np.max(data[&#39;brand_price_average&#39;]) - np.min(data[&#39;brand_price_average&#39;])))</span><br><span class="line">data[&#39;brand_price_max&#39;] &#x3D; ((data[&#39;brand_price_max&#39;] - np.min(data[&#39;brand_price_max&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_max&#39;]) - np.min(data[&#39;brand_price_max&#39;])))</span><br><span class="line">data[&#39;brand_price_median&#39;] &#x3D; ((data[&#39;brand_price_median&#39;] - np.min(data[&#39;brand_price_median&#39;])) &#x2F;</span><br><span class="line">                              (np.max(data[&#39;brand_price_median&#39;]) - np.min(data[&#39;brand_price_median&#39;])))</span><br><span class="line">data[&#39;brand_price_min&#39;] &#x3D; ((data[&#39;brand_price_min&#39;] - np.min(data[&#39;brand_price_min&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_min&#39;]) - np.min(data[&#39;brand_price_min&#39;])))</span><br><span class="line">data[&#39;brand_price_std&#39;] &#x3D; ((data[&#39;brand_price_std&#39;] - np.min(data[&#39;brand_price_std&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_std&#39;]) - np.min(data[&#39;brand_price_std&#39;])))</span><br><span class="line">data[&#39;brand_price_sum&#39;] &#x3D; ((data[&#39;brand_price_sum&#39;] - np.min(data[&#39;brand_price_sum&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_sum&#39;]) - np.min(data[&#39;brand_price_sum&#39;])))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 对类别特征进行 OneEncoder</span><br><span class="line"></span><br><span class="line">data &#x3D; pd.get_dummies(data, columns&#x3D;[&#39;model&#39;, &#39;brand&#39;, &#39;bodyType&#39;, &#39;fuelType&#39;,</span><br><span class="line">                                     &#39;gearbox&#39;, &#39;notRepairedDamage&#39;, &#39;power_bin&#39;]) # 装换虚伪变量</span><br><span class="line"></span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(199037, 370)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(data.columns)</span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;SaleID&apos;, &apos;name&apos;, &apos;power&apos;, &apos;kilometer&apos;, &apos;seller&apos;, &apos;offerType&apos;, &apos;price&apos;,
       &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;,
       ...
       &apos;power_bin_20.0&apos;, &apos;power_bin_21.0&apos;, &apos;power_bin_22.0&apos;, &apos;power_bin_23.0&apos;,
       &apos;power_bin_24.0&apos;, &apos;power_bin_25.0&apos;, &apos;power_bin_26.0&apos;, &apos;power_bin_27.0&apos;,
       &apos;power_bin_28.0&apos;, &apos;power_bin_29.0&apos;],
      dtype=&apos;object&apos;, length=370)</code></pre><h2 id="导出数据-1"><a href="#导出数据-1" class="headerlink" title="导出数据"></a>导出数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 这份数据可以给 LR 用</span><br><span class="line">data.to_csv(&quot;data_for_lr.csv&quot;, index&#x3D;0)</span><br></pre></td></tr></table></figure>

<h1 id="特征筛选"><a href="#特征筛选" class="headerlink" title="特征筛选"></a>特征筛选</h1><h2 id="过滤式"><a href="#过滤式" class="headerlink" title="过滤式"></a>过滤式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 1）过滤式</span><br><span class="line"># 相关性分析</span><br><span class="line">print(data[&#39;power&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;)) #spearman：非线性的，非正太分析的数据的相关系数</span><br><span class="line">print(data[&#39;kilometer&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line">print(data[&#39;brand_amount&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line">print(data[&#39;brand_price_average&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line">print(data[&#39;brand_price_max&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line">print(data[&#39;brand_price_median&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br></pre></td></tr></table></figure>

<pre><code>0.5728285196051496
-0.4082569701616764
0.058156610025581514
0.3834909576057687
0.259066833880992
0.38691042393409447</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 当然也可以直接看图</span><br><span class="line">data_numeric &#x3D; data[[&#39;power&#39;, &#39;kilometer&#39;, &#39;brand_amount&#39;, &#39;brand_price_average&#39;,</span><br><span class="line">                     &#39;brand_price_max&#39;, &#39;brand_price_median&#39;]]</span><br><span class="line">correlation &#x3D; data_numeric.corr() #返回data_numeric 相关性矩阵</span><br><span class="line"></span><br><span class="line">f, ax &#x3D; plt.subplots(figsize&#x3D;(7,7))</span><br><span class="line">plt.title(&quot;Correlation of Numeric Features with Price&quot;, y&#x3D;1, size&#x3D;16)</span><br><span class="line">square&#x3D;True # 将坐标轴方向设置为“equal”，以使每个单元格为方形 , vmax:色彩映射的值</span><br><span class="line">sns.heatmap(correlation, square&#x3D;True, vmax&#x3D;0.8)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/30/GuwIDs.png" alt="7"></p>
<h2 id="包裹式"><a href="#包裹式" class="headerlink" title="包裹式"></a>包裹式</h2><ul>
<li>下面的代码运行错误，看不懂<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># # 2)包裹式</span><br><span class="line">from mlxtend.feature_selection import SequentialFeatureSelector as SFS #序列特征算法的实现——贪婪搜索算法</span><br><span class="line">from sklearn.linear_model import  LinearRegression # 基于最小二乘法的线性回归</span><br><span class="line">sfs &#x3D; SFS(LinearRegression(), # 分类器或回归矩阵</span><br><span class="line">          k_features&#x3D;10, # 要选择的特征数量</span><br><span class="line">          forward&#x3D;True, #  如果为True，则向前选择，否则为反向选择</span><br><span class="line">          floating&#x3D;False, # 如果为True，则添加条件排除&#x2F;包含。</span><br><span class="line">          scoring&#x3D;&quot;r2&quot;, # 对于sklearn回归变量使用“ r2”</span><br><span class="line">          cv&#x3D;0) # 如果cv为None、False或0，则不进行交叉验证</span><br><span class="line">x &#x3D; data.drop([&quot;price&quot;], axis&#x3D;1)</span><br><span class="line">x &#x3D; x.fillna(0)</span><br><span class="line">y &#x3D; data[&quot;price&quot;]</span><br><span class="line"></span><br><span class="line">sfs.fit(x, y) # 执行特征选择并从训练数据中学习模型 x训练样本 y目标值</span><br><span class="line">sfs.k_feature_names_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 画出来，可以看到边际效益</span><br><span class="line">from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig1 &#x3D; plot_sfs(sfs.get_metric_dict(), kind&#x3D;&#39;std_dev&#39;)</span><br><span class="line">plt.grid()</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="嵌入式"><a href="#嵌入式" class="headerlink" title="嵌入式"></a>嵌入式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 下一章介绍，Lasso 回归和决策树可以完成嵌入式特征选择</span><br><span class="line"># 大部分情况下都是用嵌入式做特征筛选</span><br></pre></td></tr></table></figure>

<h1 id="代码片段"><a href="#代码片段" class="headerlink" title="代码片段"></a>代码片段</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">from operator import itemgetter</span><br><span class="line"></span><br><span class="line"># %matplotlib inline 在终端中可以代替plt。show() 直接生成图</span><br><span class="line"></span><br><span class="line">Train_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&quot;, sep&#x3D;&quot; &quot;)</span><br><span class="line">Test_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&quot;, sep&#x3D;&quot; &quot;)</span><br><span class="line"></span><br><span class="line"># print(Train_data.shape)</span><br><span class="line"># print(Train_data.head())</span><br><span class="line"></span><br><span class="line">#print(Train_data.columns)</span><br><span class="line"></span><br><span class="line">## 删除异常值</span><br><span class="line"># 这里我包装了一个异常值处理的代码，可以随便调用</span><br><span class="line">def outliers_proc(data, col_name, scale&#x3D;3):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    用于清洗异常值，默认用box_plot(scale&#x3D;3)进行清洗</span><br><span class="line">    :param data:接受 pandas 数据格式</span><br><span class="line">    :param col_name:pandas 列名</span><br><span class="line">    :param scale:尺度</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def box_plot_outliers(data_ser, box_scale):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        利用箱线图去除异常值</span><br><span class="line">        :param data_ser:接收 pandas.Series 数据格式</span><br><span class="line">        :param box_scale: 箱线图尺度 （规定大于上四分位数1.5倍四分位数差 的值，或者小于下四分位数1.5倍四分位数差的值，划为异常值）</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        iqr &#x3D; box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25)) # 3倍四分位数差</span><br><span class="line">        val_low &#x3D; data_ser.quantile(0.25) - iqr # 下限&#x3D;Q1-3IQR</span><br><span class="line">        val_up &#x3D; data_ser.quantile(0.75) + iqr # 上限&#x3D;Q3+3IQR</span><br><span class="line">        rule_low &#x3D; (data_ser &lt; val_low)</span><br><span class="line">        rule_up &#x3D; (data_ser &gt; val_up) # 返回 pandas.Series 中对应值的bool</span><br><span class="line">        return (rule_low, rule_up), (val_low, val_up)</span><br><span class="line"></span><br><span class="line">    data_n &#x3D; data.copy() # copy 数据</span><br><span class="line">    data_series &#x3D; data_n[col_name] # 返回指定 col_name 数据</span><br><span class="line">    rule, value &#x3D; box_plot_outliers(data_series, box_scale&#x3D;scale)</span><br><span class="line">    index &#x3D; np.arange(data_series.shape[0])[rule[0]|rule[1]] # 返回rule_low, rule_up中为True的下标的列表</span><br><span class="line">    #print(&quot;Delete number is:&#123;&#125;&quot;.format(len(index))) # 打印下标列表中个数</span><br><span class="line">    data_n &#x3D; data_n.drop(index) # 删除(删除后下标没变)</span><br><span class="line">    data_n.reset_index(drop&#x3D;True, inplace&#x3D;True) # 重置索引（drop&#x3D;True删除原来的索引;inplace&#x3D;True当前修改状态应用到原来Series中）</span><br><span class="line">    #print(&quot;Now column number is:&#123;&#125;&quot;.format(data_n.shape[0])) # 查看删除后的数据个数</span><br><span class="line">    index_low &#x3D; np.arange(data_series.shape[0])[rule[0]]</span><br><span class="line">    outliers &#x3D; data_series.iloc[index_low] # ilco-按下标进行索引</span><br><span class="line">    #print(&quot;Description of data larger than the lower bound is:&quot;)</span><br><span class="line">    #print(pd.Series(outliers).describe())</span><br><span class="line">    index_up &#x3D; np.arange(data_series.shape[0])[rule[1]]</span><br><span class="line">    outliers &#x3D; data_series.iloc[index_up]</span><br><span class="line">    #print(&quot;Description of data larger than the upper bound is:&quot;)</span><br><span class="line">    #print(pd.Series(outliers).describe())</span><br><span class="line"></span><br><span class="line">    #fig, ax &#x3D; plt.subplots(1, 2, figsize&#x3D;(10, 7)) # 创建子图:1行2列</span><br><span class="line">    #sns.boxplot(y&#x3D;data[col_name], data&#x3D;data, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[0]) # 箱线图</span><br><span class="line">    #sns.boxplot(y&#x3D;data_n[col_name], data&#x3D;data_n, palette&#x3D;&quot;Set1&quot;, ax&#x3D;ax[1])</span><br><span class="line">    #plt.show()</span><br><span class="line">    return data_n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 我们可以删掉一些异常数据,以 power 为例</span><br><span class="line">## 这里删不删可以自行判断</span><br><span class="line">## 但是注意 Test 的数据不能删</span><br><span class="line">Train_data &#x3D; outliers_proc(Train_data, &quot;power&quot;, scale&#x3D;3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 特征构造</span><br><span class="line"># 训练集和测试集放在一起，方便构造特征</span><br><span class="line">Train_data[&quot;train&quot;] &#x3D; 1 # 添加新字段，并设置值为1</span><br><span class="line">Test_data[&quot;train&quot;] &#x3D; 1</span><br><span class="line">data &#x3D; pd.concat([Train_data,Test_data],ignore_index&#x3D;True) # 连接函数 ignore_index&#x3D;True重置索引</span><br><span class="line"></span><br><span class="line"># 使用时间：data[&quot;createDate&quot;] - data[&quot;regDate&quot;], 反应汽车使用时间，一般来说价格与使用时间成反比</span><br><span class="line"># 不过要注意, 数据里有时间出错的格式, 所以我们需要 errors &#x3D; &quot;coerce&quot;</span><br><span class="line">data[&quot;used_time&quot;] &#x3D; (pd.to_datetime(data[&quot;creatDate&quot;], format&#x3D;&quot;%Y%m%d&quot;, errors&#x3D;&quot;coerce&quot;) -</span><br><span class="line">pd.to_datetime(data[&quot;regDate&quot;],format&#x3D;&quot;%Y%m%d&quot;,errors&#x3D;&quot;coerce&quot;)).dt.days # to_datetime将参数转换为日期 dt.days每个元素的天数</span><br><span class="line"></span><br><span class="line"># 看一下空数据, 有 15k 个样本的时间有问题的, 我们可以选择删除, 也可以选择放着</span><br><span class="line"># 但是这里不建议删除, 因为删除缺失数据占总样本量过大, 7.5%</span><br><span class="line"># 我们可以先放着, 因为如果我们 XGBoost 之类的决策树, 其本身就能处理缺失值, 所以可以不用管</span><br><span class="line">#print(data[&quot;used_time&quot;].isnull().sum())</span><br><span class="line"></span><br><span class="line"># 从邮编中提取城市信息, 相当于加入了先验知识</span><br><span class="line">#print(data[&quot;regionCode&quot;])</span><br><span class="line"># 增加city 字段, 并从 regionCode 值的倒数第三位切片(apply 对regionCode每个元素运行指定运算 lambda 匿名函数)</span><br><span class="line">data[&quot;city&quot;] &#x3D; data[&quot;regionCode&quot;].apply(lambda x : str(x)[:-3])</span><br><span class="line"></span><br><span class="line"># 计算某品牌的销售统计量, 还可以计算其他特征的统计量</span><br><span class="line"># 这里以 train 的数据计算统计量</span><br><span class="line">Train_gb &#x3D; Train_data.groupby(&quot;brand&quot;) # 分组</span><br><span class="line">all_info &#x3D; &#123;&#125;</span><br><span class="line">for kind, kind_data in Train_gb:</span><br><span class="line">    info &#x3D; &#123;&#125;</span><br><span class="line">    kind_data &#x3D; kind_data[kind_data[&quot;price&quot;] &gt; 0]</span><br><span class="line">    info[&quot;brand_amount&quot;] &#x3D; len(kind_data)</span><br><span class="line">    info[&quot;brand_price_max&quot;] &#x3D; kind_data.price.max()</span><br><span class="line">    info[&quot;brand_price_median&quot;] &#x3D; kind_data.price.median()</span><br><span class="line">    info[&quot;brand_price_min&quot;] &#x3D; kind_data.price.min()</span><br><span class="line">    info[&quot;brand_price_sum&quot;] &#x3D; kind_data.price.sum()</span><br><span class="line">    info[&quot;brand_price_std&quot;] &#x3D; kind_data.price.std() # 样本方差</span><br><span class="line">    info[&quot;brand_price_average&quot;] &#x3D; round(kind_data.price.sum() &#x2F; (len(kind_data)+1), 2) # round(2)取近似值保留两位数</span><br><span class="line">    all_info[kind] &#x3D; info</span><br><span class="line"></span><br><span class="line">brand_fe &#x3D; pd.DataFrame(all_info).T.reset_index().rename(columns&#x3D;&#123;&quot;index&quot;:&quot;brand&quot;&#125;) # T转置   reset_index还原索引  rename并修改列名</span><br><span class="line">data &#x3D; data.merge(brand_fe, how&#x3D;&quot;left&quot;, on&#x3D;&quot;brand&quot;) # 合并数据 data链接在brand_fe &quot;brand&quot;字段左边</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据分桶 以 power 为例</span><br><span class="line"># 这时候缺失值也进桶了</span><br><span class="line"># 为什么要分桶：</span><br><span class="line"># 1. 离散后稀疏向量内积乘法运算速度更快, 计算结果也方便存储, 容易扩展</span><br><span class="line"># 2. 离散后的特征对异常值更具鲁棒性, 如 age&gt;30 为 1 否则为 0 , 对于年龄为 200 的也不会对模型造成很大的干扰</span><br><span class="line"># 3. LR 属于广义线性模型, 表达能力有限, 经过离散化后, 每个变量有单独的权重, 这相当于引入了非线性, 能够提升模型的表达能力, 加大拟合</span><br><span class="line"># 4. 离散后特征可以进行特征交叉, 提升表达能力, 由 M+N 个变量变成　Ｍ*N 个变量, 进一步引入非线性, 提升了表达能力</span><br><span class="line"># 5. 特征离散后模型更稳定, 如用户年龄区间, 不会因为用户年龄长了一岁就变化</span><br><span class="line"># 当然还有很多原因,　LightGBM 在改进 XGBoost 时就增加了数据分桶, 增强了模型的泛化性</span><br><span class="line"></span><br><span class="line">bin &#x3D; [i*10 for i in range(31)]</span><br><span class="line">data[&quot;power_bin&quot;] &#x3D; pd.cut(data[&quot;power&quot;], bin, labels&#x3D;False) # 分桶 cut切分数据(必须是一维的) bin定义区间 labels&#x3D;False返回第几个bin（从0开始）</span><br><span class="line">#print(data[[&quot;power_bin&quot;, &quot;power&quot;]].head())</span><br><span class="line"></span><br><span class="line"># 删除不需要的数据</span><br><span class="line">data &#x3D; data.drop([&quot;creatDate&quot;, &quot;regDate&quot;, &quot;regionCode&quot;], axis&#x3D;1) # drop函数默认删除行，列需要加axis &#x3D; 1</span><br><span class="line">#print(data.shape)</span><br><span class="line">#print(data.columns)</span><br><span class="line"></span><br><span class="line"># 目前的数据其实已经可以给树模型使用了, 所以我们导出一下</span><br><span class="line">#data.to_csv(&quot;data_for_tree.csv&quot;, index&#x3D;0) # index&#x3D;0不保存行索引</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 我们可以再构造一份特征给 LR NN 之类的模型用</span><br><span class="line"># 之所以分开构造是因为, 不同模型对数据的要求不同</span><br><span class="line"># 先看下数据分布：</span><br><span class="line">#data[&quot;power&quot;].plot.hist()</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line"># 我们刚刚已经对 train 进行异常值处理了，但是现在还有这么奇怪的分布是因为 test 中的 power 异常值，</span><br><span class="line"># 所以我们其实刚刚 train 中的 power 异常值不删为好，可以用长尾分布截断来代替</span><br><span class="line">#Train_data[&quot;power&quot;].plot.hist()</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line"># 我们对其取 log, 再做归一化</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line"># 将数据的每一个特征缩放到给定的范围，将数据的每一个属性值减去其最小值，然后除以其极差（最大值 - 最小值）</span><br><span class="line">min_max_scaler &#x3D; preprocessing.MinMaxScaler()</span><br><span class="line">data[&quot;power&quot;] &#x3D; np.log(data[&quot;power&quot;] + 1)</span><br><span class="line"># 归一化：(0,1)标准化</span><br><span class="line">data[&quot;power&quot;] &#x3D; ((data[&quot;power&quot;] - np.min(data[&quot;power&quot;])) &#x2F; (np.max(data[&quot;power&quot;]) - np.min(data[&quot;power&quot;])))</span><br><span class="line">#data[&quot;power&quot;].plot.hist()</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line"># km 的比较正常, 应该已经做过分桶了</span><br><span class="line"># data[&quot;kilometer&quot;].plot.hist()</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># 所以可以直接作归一化</span><br><span class="line">data[&quot;kilometer&quot;] &#x3D; ((data[&quot;kilometer&quot;] - np.min(data[&quot;kilometer&quot;])) &#x2F;</span><br><span class="line">                     (np.max(data[&quot;kilometer&quot;]) - np.min(data[&quot;kilometer&quot;])))</span><br><span class="line">#data[&quot;kilometer&quot;].plot.hist()</span><br><span class="line">#plt.show()</span><br><span class="line"></span><br><span class="line"># 除此之外 还有我们刚刚构造的统计量特征：</span><br><span class="line"># &#39;brand_amount&#39;, &#39;brand_price_average&#39;, &#39;brand_price_max&#39;,</span><br><span class="line"># &#39;brand_price_median&#39;, &#39;brand_price_min&#39;, &#39;brand_price_std&#39;,</span><br><span class="line"># &#39;brand_price_sum&#39;</span><br><span class="line"># 这里不再一一举例分析了，直接做变换，</span><br><span class="line">def max_min(x):</span><br><span class="line">    return (x - np.min(x)) &#x2F; (np.max(x) - np.min(x))</span><br><span class="line"></span><br><span class="line">data[&#39;brand_amount&#39;] &#x3D; ((data[&#39;brand_amount&#39;] - np.min(data[&#39;brand_amount&#39;])) &#x2F;</span><br><span class="line">                        (np.max(data[&#39;brand_amount&#39;]) - np.min(data[&#39;brand_amount&#39;])))</span><br><span class="line">data[&#39;brand_price_average&#39;] &#x3D; ((data[&#39;brand_price_average&#39;] - np.min(data[&#39;brand_price_average&#39;])) &#x2F;</span><br><span class="line">                               (np.max(data[&#39;brand_price_average&#39;]) - np.min(data[&#39;brand_price_average&#39;])))</span><br><span class="line">data[&#39;brand_price_max&#39;] &#x3D; ((data[&#39;brand_price_max&#39;] - np.min(data[&#39;brand_price_max&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_max&#39;]) - np.min(data[&#39;brand_price_max&#39;])))</span><br><span class="line">data[&#39;brand_price_median&#39;] &#x3D; ((data[&#39;brand_price_median&#39;] - np.min(data[&#39;brand_price_median&#39;])) &#x2F;</span><br><span class="line">                              (np.max(data[&#39;brand_price_median&#39;]) - np.min(data[&#39;brand_price_median&#39;])))</span><br><span class="line">data[&#39;brand_price_min&#39;] &#x3D; ((data[&#39;brand_price_min&#39;] - np.min(data[&#39;brand_price_min&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_min&#39;]) - np.min(data[&#39;brand_price_min&#39;])))</span><br><span class="line">data[&#39;brand_price_std&#39;] &#x3D; ((data[&#39;brand_price_std&#39;] - np.min(data[&#39;brand_price_std&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_std&#39;]) - np.min(data[&#39;brand_price_std&#39;])))</span><br><span class="line">data[&#39;brand_price_sum&#39;] &#x3D; ((data[&#39;brand_price_sum&#39;] - np.min(data[&#39;brand_price_sum&#39;])) &#x2F;</span><br><span class="line">                           (np.max(data[&#39;brand_price_sum&#39;]) - np.min(data[&#39;brand_price_sum&#39;])))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 对类别特征进行 OneEncoder</span><br><span class="line"></span><br><span class="line">data &#x3D; pd.get_dummies(data, columns&#x3D;[&#39;model&#39;, &#39;brand&#39;, &#39;bodyType&#39;, &#39;fuelType&#39;,</span><br><span class="line">                                     &#39;gearbox&#39;, &#39;notRepairedDamage&#39;, &#39;power_bin&#39;]) # 装换虚伪变量</span><br><span class="line"></span><br><span class="line">#print(data.shape)</span><br><span class="line">#print(data.columns)</span><br><span class="line"></span><br><span class="line"># 这份数据可以给 LR 用</span><br><span class="line">#data.to_csv(&quot;data_for_lr.csv&quot;, index&#x3D;0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 特征筛选</span><br><span class="line"># 1）过滤式</span><br><span class="line"># 相关性分析</span><br><span class="line"># print(data[&#39;power&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;)) #spearman：非线性的，非正太分析的数据的相关系数</span><br><span class="line"># print(data[&#39;kilometer&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"># print(data[&#39;brand_amount&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"># print(data[&#39;brand_price_average&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"># print(data[&#39;brand_price_max&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"># print(data[&#39;brand_price_median&#39;].corr(data[&#39;price&#39;], method&#x3D;&#39;spearman&#39;))</span><br><span class="line"></span><br><span class="line"># 当然也可以直接看图</span><br><span class="line"># data_numeric &#x3D; data[[&#39;power&#39;, &#39;kilometer&#39;, &#39;brand_amount&#39;, &#39;brand_price_average&#39;,</span><br><span class="line">#                      &#39;brand_price_max&#39;, &#39;brand_price_median&#39;]]</span><br><span class="line"># correlation &#x3D; data_numeric.corr() #返回data_numeric 相关性矩阵</span><br><span class="line">#</span><br><span class="line"># f, ax &#x3D; plt.subplots(figsize&#x3D;(7,7))</span><br><span class="line"># plt.title(&quot;Correlation of Numeric Features with Price&quot;, y&#x3D;1, size&#x3D;16)</span><br><span class="line"># square&#x3D;True # 将坐标轴方向设置为“equal”，以使每个单元格为方形 , vmax:色彩映射的值</span><br><span class="line"># sns.heatmap(correlation, square&#x3D;True, vmax&#x3D;0.8)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># # 2)包裹式</span><br><span class="line">from mlxtend.feature_selection import SequentialFeatureSelector as SFS #序列特征算法的实现——贪婪搜索算法</span><br><span class="line">from sklearn.linear_model import  LinearRegression # 基于最小二乘法的线性回归</span><br><span class="line">sfs &#x3D; SFS(LinearRegression(), # 分类器或回归矩阵</span><br><span class="line">          k_features&#x3D;10, # 要选择的特征数量</span><br><span class="line">          forward&#x3D;True, #  如果为True，则向前选择，否则为反向选择</span><br><span class="line">          floating&#x3D;False, # 如果为True，则添加条件排除&#x2F;包含。</span><br><span class="line">          scoring&#x3D;&quot;r2&quot;, # 对于sklearn回归变量使用“ r2”</span><br><span class="line">          cv&#x3D;0) # 如果cv为None、False或0，则不进行交叉验证</span><br><span class="line">x &#x3D; data.drop([&quot;price&quot;], axis&#x3D;1)</span><br><span class="line">x &#x3D; x.fillna(0)</span><br><span class="line">y &#x3D; data[&quot;price&quot;]</span><br><span class="line"></span><br><span class="line">sfs.fit(x, y) # 执行特征选择并从训练数据中学习模型 x训练样本 y目标值</span><br><span class="line">sfs.k_feature_names_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 画出来，可以看到边际效益</span><br><span class="line">from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig1 &#x3D; plot_sfs(sfs.get_metric_dict(), kind&#x3D;&#39;std_dev&#39;)</span><br><span class="line">plt.grid()</span><br></pre></td></tr></table></figure>

<h1 id="经验总结"><a href="#经验总结" class="headerlink" title="经验总结"></a>经验总结</h1><p>特征工程是比赛中最至关重要的的一块，特别的传统的比赛，大家的模型可能都差不多，调参带来的效果增幅是非常有限的，但特征工程的好坏往往会决定了最终的排名和成绩。</p>
<p>特征工程的主要目的还是在于将数据转换为能更好地表示潜在问题的特征，从而提高机器学习的性能。比如，异常值处理是为了去除噪声，填补缺失值可以加入先验知识等。</p>
<p>特征构造也属于特征工程的一部分，其目的是为了增强数据的表达。</p>
<p>有些比赛的特征是匿名特征，这导致我们并不清楚特征相互直接的关联性，这时我们就只有单纯基于特征进行处理，比如装箱，groupby，agg 等这样一些操作进行一些特征统计，此外还可以对特征进行进一步的 log，exp 等变换，或者对多个特征进行四则运算（如上面我们算出的使用时长），多项式组合等然后进行筛选。由于特性的匿名性其实限制了很多对于特征的处理，当然有些时候用 NN 去提取一些特征也会达到意想不到的良好效果。</p>
<p>对于知道特征含义（非匿名）的特征工程，特别是在工业类型比赛中，会基于信号处理，频域提取，峰度，偏度等构建更为有实际意义的特征，这就是结合背景的特征构建，在推荐系统中也是这样的，各种类型点击率统计，各时段统计，加用户属性的统计等等，这样一种特征构建往往要深入分析背后的业务逻辑或者说物理原理，从而才能更好的找到 magic。</p>
<p>当然特征工程其实是和模型结合在一起的，这就是为什么要为 LR NN 做分桶和特征归一化的原因，而对于特征的处理效果和特征重要性等往往要通过模型来验证。</p>
<p>总的来说，特征工程是一个入门简单，但想精通非常难的一件事。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/03/24/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/24/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task2/" class="post-title-link" itemprop="url">Datawhale零基础入门数据挖掘-Task2</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-24 17:27:07" itemprop="dateCreated datePublished" datetime="2020-03-24T17:27:07+08:00">2020-03-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-26 19:46:30" itemprop="dateModified" datetime="2020-03-26T19:46:30+08:00">2020-03-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">数据挖掘及机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>33k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>30 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>EDA的价值主要在于熟悉数据集，了解数据集，对数据集进行验证来确定所获得数据集可以用于接下来的机器学习或者深度学习使用</li>
<li>当了解了数据集之后我们下一步就是要去了解变量间的相互关系以及变量与预测值之间的存在关系</li>
<li>进行数据处理以及特征工程,使数据集的结构和特征集让接下来的预测问题更加可靠</li>
</ul>
<h1 id="载入各种数据科学以及可视化库"><a href="#载入各种数据科学以及可视化库" class="headerlink" title="载入各种数据科学以及可视化库"></a>载入各种数据科学以及可视化库</h1><h2 id="载入各种数据科学以及可视化库-1"><a href="#载入各种数据科学以及可视化库-1" class="headerlink" title="载入各种数据科学以及可视化库"></a>载入各种数据科学以及可视化库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 导入warnings包，利用过滤器来实现忽略警告语句</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import missingno as msno</span><br></pre></td></tr></table></figure>
<h1 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## pd.set_option(&#39;display.max_columns&#39;, None)# 显示所有列</span><br><span class="line">## pd.set_option(&#39;display.max_row&#39;, None)# 显示所有行</span><br><span class="line">## 1)载入训练集和测试集</span><br><span class="line">Train_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&quot;, sep &#x3D; &quot; &quot;)</span><br><span class="line">Test_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&quot;, sep &#x3D; &quot; &quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>以下主要以Train_data为例<h2 id="简略观察数据"><a href="#简略观察数据" class="headerlink" title="简略观察数据"></a>简略观察数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">## 2)简略观察数据（head()+shape)</span><br><span class="line">print(Train_data.head().append(Train_data.tail()))</span><br></pre></td></tr></table></figure>

</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>SaleID</th>
<th>name</th>
<th>regDate</th>
<th>model</th>
<th>…</th>
<th>v_11</th>
<th>v_12</th>
<th>v_13</th>
<th>v_14</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>736</td>
<td>20040402</td>
<td>30.0</td>
<td>…</td>
<td>2.804097</td>
<td>-2.420821</td>
<td>0.795292</td>
<td>0.914762</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2262</td>
<td>20030301</td>
<td>40.0</td>
<td>…</td>
<td>2.096338</td>
<td>-1.030483</td>
<td>-1.722674</td>
<td>0.245522</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>14874</td>
<td>20040403</td>
<td>115.0</td>
<td>…</td>
<td>1.803559</td>
<td>1.565330</td>
<td>-0.832687</td>
<td>-0.229963</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>71865</td>
<td>19960908</td>
<td>109.0</td>
<td>…</td>
<td>1.285940</td>
<td>-0.501868</td>
<td>-2.438353</td>
<td>-0.478699</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
<td>111080</td>
<td>20120103</td>
<td>110.0</td>
<td>…</td>
<td>0.910783</td>
<td>0.931110</td>
<td>2.834518</td>
<td>1.923482</td>
</tr>
<tr>
<td>149995</td>
<td>149995</td>
<td>163978</td>
<td>20000607</td>
<td>121.0</td>
<td>…</td>
<td>-2.983973</td>
<td>0.589167</td>
<td>-1.304370</td>
<td>-0.302592</td>
</tr>
<tr>
<td>149996</td>
<td>149996</td>
<td>184535</td>
<td>20091102</td>
<td>116.0</td>
<td>…</td>
<td>-2.774615</td>
<td>2.553994</td>
<td>0.924196</td>
<td>-0.272160</td>
</tr>
<tr>
<td>149997</td>
<td>149997</td>
<td>147587</td>
<td>20101003</td>
<td>60.0</td>
<td>…</td>
<td>-1.630677</td>
<td>2.290197</td>
<td>1.891922</td>
<td>0.414931</td>
</tr>
<tr>
<td>149998</td>
<td>149998</td>
<td>45907</td>
<td>20060312</td>
<td>34.0</td>
<td>…</td>
<td>-2.633719</td>
<td>1.414937</td>
<td>0.431981</td>
<td>-1.659014</td>
</tr>
<tr>
<td>149999</td>
<td>149999</td>
<td>177672</td>
<td>19990204</td>
<td>19.0</td>
<td>…</td>
<td>-3.179913</td>
<td>0.031724</td>
<td>-1.483350</td>
<td>-0.342674</td>
</tr>
</tbody></table>
<p>[10 rows x 31 columns]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(150000, 31)</code></pre><h1 id="总览数据概况"><a href="#总览数据概况" class="headerlink" title="总览数据概况"></a>总览数据概况</h1><ol>
<li>describe种有每列的统计量，个数count、平均值mean、方差std、最小值min、中位数25% 50% 75% 、以及最大值 看这个信息主要是瞬间掌握数据的大概的范围以及每个值的异常值的判断，比如有的时候会发现999 9999 -1 等值这些其实都是nan的另外一种表达方式，有的时候需要注意下</li>
<li>info 通过info来了解数据每列的type，有助于了解是否存在除了nan以外的特殊符号异常</li>
</ol>
<h2 id="通过describe-来熟悉相关统计量"><a href="#通过describe-来熟悉相关统计量" class="headerlink" title="通过describe()来熟悉相关统计量"></a>通过describe()来熟悉相关统计量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">## 3)通过describe()来熟悉相关统计量</span><br><span class="line">print(Train_data.describe())</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th></th>
<th>SaleID</th>
<th>name</th>
<th>…</th>
<th>v_13</th>
<th>v_14</th>
</tr>
</thead>
<tbody><tr>
<td>count</td>
<td>150000.000000</td>
<td>150000.000000</td>
<td>…</td>
<td>150000.000000</td>
<td>150000.000000</td>
</tr>
<tr>
<td>mean</td>
<td>74999.500000</td>
<td>68349.172873</td>
<td>…</td>
<td>0.000313</td>
<td>-0.000688</td>
</tr>
<tr>
<td>std</td>
<td>43301.414527</td>
<td>61103.875095</td>
<td>…</td>
<td>1.288988</td>
<td>1.038685</td>
</tr>
<tr>
<td>min</td>
<td>0.000000</td>
<td>0.000000</td>
<td>…</td>
<td>-4.153899</td>
<td>-6.546556</td>
</tr>
<tr>
<td>25%</td>
<td>37499.750000</td>
<td>11156.000000</td>
<td>…</td>
<td>-1.057789</td>
<td>-0.437034</td>
</tr>
<tr>
<td>50%</td>
<td>74999.500000</td>
<td>51638.000000</td>
<td>…</td>
<td>-0.036245</td>
<td>0.141246</td>
</tr>
<tr>
<td>75%</td>
<td>112499.250000</td>
<td>118841.250000</td>
<td>…</td>
<td>0.942813</td>
<td>0.680378</td>
</tr>
<tr>
<td>max</td>
<td>149999.000000</td>
<td>196812.000000</td>
<td>…</td>
<td>11.147669</td>
<td>8.658418</td>
</tr>
</tbody></table>
<p>[8 rows x 30 columns]</p>
<h2 id="通过info-来熟悉数据类型"><a href="#通过info-来熟悉数据类型" class="headerlink" title="通过info()来熟悉数据类型"></a>通过info()来熟悉数据类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">## 4)通过info()来熟悉数据类型</span><br><span class="line">print(Train_data.info())</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 150000 entries, 0 to 149999
Data columns (total 31 columns):
 #   Column             Non-Null Count   Dtype  
---  ------             --------------   -----  
 0   SaleID             150000 non-null  int64  
 1   name               150000 non-null  int64  
 2   regDate            150000 non-null  int64  
 3   model              149999 non-null  float64
 4   brand              150000 non-null  int64  
 5   bodyType           145494 non-null  float64
 6   fuelType           141320 non-null  float64
 7   gearbox            144019 non-null  float64
 8   power              150000 non-null  int64  
 9   kilometer          150000 non-null  float64
 10  notRepairedDamage  150000 non-null  object 
 11  regionCode         150000 non-null  int64  
 12  seller             150000 non-null  int64  
 13  offerType          150000 non-null  int64  
 14  creatDate          150000 non-null  int64  
 15  price              150000 non-null  int64  
 16  v_0                150000 non-null  float64
 17  v_1                150000 non-null  float64
 18  v_2                150000 non-null  float64
 19  v_3                150000 non-null  float64
 20  v_4                150000 non-null  float64
 21  v_5                150000 non-null  float64
 22  v_6                150000 non-null  float64
 23  v_7                150000 non-null  float64
 24  v_8                150000 non-null  float64
 25  v_9                150000 non-null  float64
 26  v_10               150000 non-null  float64
 27  v_11               150000 non-null  float64
 28  v_12               150000 non-null  float64
 29  v_13               150000 non-null  float64
 30  v_14               150000 non-null  float64
dtypes: float64(20), int64(10), object(1)
memory usage: 35.5+ MB
None</code></pre><h1 id="判断数据缺失和异常"><a href="#判断数据缺失和异常" class="headerlink" title="判断数据缺失和异常"></a>判断数据缺失和异常</h1><h2 id="查看每列的存在nan情况"><a href="#查看每列的存在nan情况" class="headerlink" title="查看每列的存在nan情况"></a>查看每列的存在nan情况</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">## 5) 查看每列的存在nan情况</span><br><span class="line">print(Train_data.isnull().sum())</span><br></pre></td></tr></table></figure>

<pre><code>SaleID                  0
name                    0
regDate                 0
model                   1
brand                   0
bodyType             4506
fuelType             8680
gearbox              5981
power                   0
kilometer               0
notRepairedDamage       0
regionCode              0
seller                  0
offerType               0
creatDate               0
price                   0
v_0                     0
v_1                     0
v_2                     0
v_3                     0
v_4                     0
v_5                     0
v_6                     0
v_7                     0
v_8                     0
v_9                     0
v_10                    0
v_11                    0
v_12                    0
v_13                    0
v_14                    0
dtype: int64</code></pre><h2 id="nan可视化"><a href="#nan可视化" class="headerlink" title="nan可视化"></a>nan可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#nan可视化</span><br><span class="line">missing &#x3D; Train_data.isnull().sum()</span><br><span class="line">missing &#x3D; missing[missing &gt; 0]</span><br><span class="line">missing.sort_values(inplace&#x3D;True) # 排序</span><br><span class="line">missing.plot.bar() # 绘柱状图</span><br><span class="line">plt.tight_layout() # 自动调整子图参数</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpS1ts.png" alt="1"></p>
<ul>
<li>通过以上可以很直观的了解哪些列存在 “nan”, 并可以把nan的个数打印，主要的目的在于 nan存在的个数是否真的很大，如果很小一般选择填充，如果使用lgb等树模型可以直接空缺，让树自己去优化，但如果nan存在的过多、可以考虑删掉</li>
</ul>
<h2 id="可视化看下缺省值"><a href="#可视化看下缺省值" class="headerlink" title="可视化看下缺省值"></a>可视化看下缺省值</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">## 可视化看下缺省值</span><br><span class="line">msno.matrix(Train_data.sample(250))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/Gp9H0S.png" alt="2"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">msno.bar(Train_data.sample(1000)) # 条形图</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/Gppugx.png" alt="3"></p>
<h2 id="查看异常值检测"><a href="#查看异常值检测" class="headerlink" title="查看异常值检测"></a>查看异常值检测</h2><ul>
<li><p>通过前面info()来熟悉数据类型，可以发现除了notRepairedDamage 为object类型其他都为数字 这里我们把他的几个不同的值都进行显示就知道了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data[&quot;notRepairedDamage&quot;].value_counts()) # 返回包含值和count</span><br></pre></td></tr></table></figure>

<p>  0.0    111361<br>  <code>-</code>       24324<br>  1.0     14315<br>  Name: notRepairedDamage, dtype: int64</p>
</li>
<li><p>可以看出来‘ - ’也为空缺值，因为很多模型对nan有直接的处理，这里我们先不做处理，先替换成nan</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train_data[&quot;notRepairedDamage&quot;].replace(&quot;-&quot;, np.nan, inplace&#x3D;True) # 将数据中‘-’替换成nan值</span><br><span class="line">print(Train_data[&quot;notRepairedDamage&quot;].value_counts())</span><br></pre></td></tr></table></figure>

<pre><code>0.0    111361
1.0     14315
Name: notRepairedDamage, dtype: int64</code></pre><ul>
<li>再查看nan值情况</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data.isnull().sum())</span><br></pre></td></tr></table></figure>

<pre><code>SaleID                   0
name                     0
regDate                  0
model                    1
brand                    0
bodyType              4506
fuelType              8680
gearbox               5981
power                    0
kilometer                0
notRepairedDamage    24324
regionCode               0
seller                   0
offerType                0
creatDate                0
price                    0
v_0                      0
v_1                      0
v_2                      0
v_3                      0
v_4                      0
v_5                      0
v_6                      0
v_7                      0
v_8                      0
v_9                      0
v_10                     0
v_11                     0
v_12                     0
v_13                     0
v_14                     0
dtype: int64</code></pre><ul>
<li>以下两个类别特征严重倾斜，一般不会对预测有什么帮助，故这边先删掉，当然你也可以继续挖掘，但是一般意义不大</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data[&quot;seller&quot;].value_counts())</span><br></pre></td></tr></table></figure>

<pre><code>0    149999
1         1
Name: seller, dtype: int64</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data[&quot;offerType&quot;].value_counts())</span><br></pre></td></tr></table></figure>

<pre><code>0    150000
Name: offerType, dtype: int64</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 删除严重倾斜的数据</span><br><span class="line">del Train_data[&quot;seller&quot;]</span><br><span class="line">del Train_data[&quot;offerType&quot;]</span><br><span class="line">print(Train_data.info())</span><br><span class="line">print(Train_data.shape)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 150000 entries, 0 to 149999
Data columns (total 29 columns):
 #   Column             Non-Null Count   Dtype  
---  ------             --------------   -----  
 0   SaleID             150000 non-null  int64  
 1   name               150000 non-null  int64  
 2   regDate            150000 non-null  int64  
 3   model              149999 non-null  float64
 4   brand              150000 non-null  int64  
 5   bodyType           145494 non-null  float64
 6   fuelType           141320 non-null  float64
 7   gearbox            144019 non-null  float64
 8   power              150000 non-null  int64  
 9   kilometer          150000 non-null  float64
 10  notRepairedDamage  150000 non-null  object 
 11  regionCode         150000 non-null  int64  
 12  creatDate          150000 non-null  int64  
 13  price              150000 non-null  int64  
 14  v_0                150000 non-null  float64
 15  v_1                150000 non-null  float64
 16  v_2                150000 non-null  float64
 17  v_3                150000 non-null  float64
 18  v_4                150000 non-null  float64
 19  v_5                150000 non-null  float64
 20  v_6                150000 non-null  float64
 21  v_7                150000 non-null  float64
 22  v_8                150000 non-null  float64
 23  v_9                150000 non-null  float64
 24  v_10               150000 non-null  float64
 25  v_11               150000 non-null  float64
 26  v_12               150000 non-null  float64
 27  v_13               150000 non-null  float64
 28  v_14               150000 non-null  float64
dtypes: float64(20), int64(8), object(1)
memory usage: 33.2+ MB
None

(150000, 29)</code></pre><h1 id="了解预测值的分布"><a href="#了解预测值的分布" class="headerlink" title="了解预测值的分布"></a>了解预测值的分布</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data[&quot;price&quot;])</span><br><span class="line">print(Train_data[&quot;price&quot;].value_counts())</span><br></pre></td></tr></table></figure>
<pre><code>0         1850
1         3600
2         6222
3         2400
4         5200
      ... 
149995    5900
149996    9500
149997    7500
149998    4999
149999    4700
Name: price, Length: 150000, dtype: int64

500      2337
1500     2158
1200     1922
1000     1850
2500     1821
     ... 
25321       1
8886        1
8801        1
37920       1
8188        1
Name: price, Length: 3763, dtype: int64</code></pre><h2 id="总体分布情况-无界约翰逊分布等）"><a href="#总体分布情况-无界约翰逊分布等）" class="headerlink" title="总体分布情况(无界约翰逊分布等）"></a>总体分布情况(无界约翰逊分布等）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">## 1)总体分布情况(无界约翰逊分布等）</span><br><span class="line">import scipy.stats as st</span><br><span class="line">y &#x3D; Train_data[&quot;price&quot;]</span><br><span class="line">plt.figure(1); plt.title(&quot;Johnson SU&quot;) # 创建新图</span><br><span class="line">sns.distplot(y, kde&#x3D;False, fit&#x3D;st.johnsonsu)</span><br><span class="line">plt.figure(2); plt.title(&quot;Normal&quot;)</span><br><span class="line">sns.distplot(y, kde&#x3D;False, fit&#x3D;st.norm)</span><br><span class="line">plt.figure(3); plt.title(&quot;Log Normal&quot;)</span><br><span class="line">sns.distplot(y, kde&#x3D;False, fit&#x3D;st.lognorm)</span><br><span class="line">plt.show() # 最佳拟合是无界约翰逊分布</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpmBL9.png" alt="4"><br><img src="https://s1.ax1x.com/2020/03/26/GpmfQe.png" alt="5"><br><img src="https://s1.ax1x.com/2020/03/26/Gpm4Ld.png" alt="6"></p>
<ul>
<li>价格不服从正态分布，所以在进行回归之前，它必须进行转换。虽然对数变换做得很好，但最佳拟合是无界约翰逊分布</li>
</ul>
<h2 id="查看skewness-and-kurtosis"><a href="#查看skewness-and-kurtosis" class="headerlink" title="查看skewness and kurtosis"></a>查看skewness and kurtosis</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 2)查看skewness and kurtosis</span><br><span class="line">sns.distplot(Train_data[&quot;price&quot;])</span><br><span class="line">print(&quot;Skewness: %f&quot; % Train_data[&quot;price&quot;].skew()) # 偏度</span><br><span class="line">print(&quot;Kurtosis: %f&quot; % Train_data[&quot;price&quot;].kurt()) # 峰度</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Skewness: 3.346487
Kurtosis: 18.995183</code></pre><p><img src="https://s1.ax1x.com/2020/03/26/Gpu55t.png" alt="7"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data.skew())</span><br><span class="line">print(Train_data.kurt())</span><br></pre></td></tr></table></figure>

<pre><code>SaleID               6.017846e-17
name                 5.576058e-01
regDate              2.849508e-02
model                1.484388e+00
brand                1.150760e+00
bodyType             9.915299e-01
fuelType             1.595486e+00
gearbox              1.317514e+00
power                6.586318e+01
kilometer           -1.525921e+00
notRepairedDamage    2.430640e+00
regionCode           6.888812e-01
creatDate           -7.901331e+01
price                3.346487e+00
v_0                 -1.316712e+00
v_1                  3.594543e-01
v_2                  4.842556e+00
v_3                  1.062920e-01
v_4                  3.679890e-01
v_5                 -4.737094e+00
v_6                  3.680730e-01
v_7                  5.130233e+00
v_8                  2.046133e-01
v_9                  4.195007e-01
v_10                 2.522046e-02
v_11                 3.029146e+00
v_12                 3.653576e-01
v_13                 2.679152e-01
v_14                -1.186355e+00
dtype: float64

SaleID                 -1.200000
name                   -1.039945
regDate                -0.697308
model                   1.740483
brand                   1.076201
bodyType                0.206937
fuelType                5.880049
gearbox                -0.264161
power                5733.451054
kilometer               1.141934
notRepairedDamage       3.908072
regionCode             -0.340832
creatDate            6881.080328
price                  18.995183
v_0                     3.993841
v_1                    -1.753017
v_2                    23.860591
v_3                    -0.418006
v_4                    -0.197295
v_5                    22.934081
v_6                    -1.742567
v_7                    25.845489
v_8                    -0.636225
v_9                    -0.321491
v_10                   -0.577935
v_11                   12.568731
v_12                    0.268937
v_13                   -0.438274
v_14                    2.393526
dtype: float64</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(Train_data.skew(), color&#x3D;&quot;blue&quot;, axlabel&#x3D;&quot;Skewness&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/GpQ1UA.png" alt="8"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(Train_data.kurt(), color&#x3D;&quot;orange&quot;, axlabel&#x3D;&quot;Kurtness&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/GpQJ8P.png" alt="9"></p>
<ul>
<li>skew、kurt说明参考<a href="https://www.cnblogs.com/wyy1480/p/10474046.html" target="_blank" rel="noopener">https://www.cnblogs.com/wyy1480/p/10474046.html</a></li>
</ul>
<h2 id="查看预测值的具体频数"><a href="#查看预测值的具体频数" class="headerlink" title="查看预测值的具体频数"></a>查看预测值的具体频数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 3)查看预测值的具体频数</span><br><span class="line">plt.hist(Train_data[&quot;price&quot;], orientation&#x3D;&quot;vertical&quot;, histtype&#x3D;&quot;bar&quot;, color&#x3D;&quot;red&quot;)</span><br><span class="line">plt.show() # 直方图</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/Gp1rcV.png" alt="10"></p>
<ul>
<li>查看频数, 大于20000得值极少，其实这里也可以把这些当作特殊得值（异常值）直接用填充或者删掉，在前面进行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># log变换之后的分布比较均匀，可以进行log变换进行预测，这也是预测问题常用的trick</span><br><span class="line">plt.hist(np.log(Train_data[&quot;price&quot;]), orientation&#x3D;&quot;vertical&quot;, histtype&#x3D;&quot;bar&quot;, color&#x3D;&quot;red&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/Gp3Z3q.png" alt="11"></p>
<ul>
<li>log变换之后的分布较均匀，可以进行log变换进行预测，这也是预测问题常用的trick</li>
</ul>
<h1 id="特征分为类别特征和数字特征，并对类别特征查看nunique分布"><a href="#特征分为类别特征和数字特征，并对类别特征查看nunique分布" class="headerlink" title="特征分为类别特征和数字特征，并对类别特征查看nunique分布"></a>特征分为类别特征和数字特征，并对类别特征查看nunique分布</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ul>
<li>name - 汽车编码</li>
<li>regDate - 汽车注册时间</li>
<li>model - 车型编码</li>
<li>brand - 品牌</li>
<li>bodyType - 车身类型</li>
<li>fuelType - 燃油类型</li>
<li>gearbox - 变速箱</li>
<li>power - 汽车功率</li>
<li>kilometer - 汽车行驶公里</li>
<li>notRepairedDamage - 汽车有尚未修复的损坏</li>
<li>regionCode - 看车地区编码</li>
<li>seller - 销售方 【以删】</li>
<li>offerType - 报价类型 【以删】</li>
<li>creatDate - 广告发布时间</li>
<li>price - 汽车价格</li>
<li>v_0’, ‘v_1’, ‘v_2’, ‘v_3’, ‘v_4’, ‘v_5’, ‘v_6’, ‘v_7’, ‘v_8’, ‘v_9’, ‘v_10’, ‘v_11’, ‘v_12’, ‘v_13’,’v_14’【匿名特征，包含v0-14在内15个匿名特征】</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 分离label即预测值</span><br><span class="line">Y_train &#x3D; Train_data[&#39;price&#39;]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 这个区别方式适用于没有直接label coding的数据</span><br><span class="line"># 这里不适用，需要人为根据实际含义来区分</span><br><span class="line"># 数字特征</span><br><span class="line"># numeric_features &#x3D; Train_data.select_dtypes(include&#x3D;[np.number])</span><br><span class="line"># numeric_features.columns</span><br><span class="line"># # 类型特征</span><br><span class="line"># categorical_features &#x3D; Train_data.select_dtypes(include&#x3D;[np.object])</span><br><span class="line"># categorical_features.columns</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 数字特征</span><br><span class="line">#numeric_features &#x3D; [&#39;power&#39;, &#39;kilometer&#39;, &#39;v_0&#39;, &#39;v_1&#39;, &#39;v_2&#39;, &#39;v_3&#39;, &#39;v_4&#39;, &#39;v_5&#39;, &#39;v_6&#39;, &#39;v_7&#39;, &#39;v_8&#39;, &#39;v_9&#39;, &#39;v_10&#39;, &#39;v_11&#39;, &#39;v_12&#39;, &#39;v_13&#39;,&#39;v_14&#39; ]</span><br><span class="line"># 类别特征</span><br><span class="line">#categorical_features &#x3D; [&#39;name&#39;, &#39;model&#39;, &#39;brand&#39;, &#39;bodyType&#39;, &#39;fuelType&#39;, &#39;gearbox&#39;, &#39;notRepairedDamage&#39;, &#39;regionCode&#39;]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 类别特征nunique分布——Train_data</span><br><span class="line">for cat_fea in categorical_features:</span><br><span class="line">    print(cat_fea+&quot;的特征分布如下：&quot;)</span><br><span class="line">    print(&quot;&#123;&#125;特征有&#123;&#125;个不同的值&quot;.format(cat_fea, Train_data[cat_fea].nunique()))</span><br><span class="line">    print(Train_data[cat_fea].value_counts())</span><br></pre></td></tr></table></figure>


<pre><code>name的特征分布如下：
name特征有99662个不同的值
708       282
387       282
55        280
1541      263
203       233
     ... 
5074        1
7123        1
11221       1
13270       1
174485      1
Name: name, Length: 99662, dtype: int64

model的特征分布如下：
model特征有248个不同的值
0.0      11762
19.0      9573
4.0       8445
1.0       6038
29.0      5186
     ...  
245.0        2
209.0        2
240.0        2
242.0        2
247.0        1
Name: model, Length: 248, dtype: int64

brand的特征分布如下：
brand特征有40个不同的值
0     31480
4     16737
14    16089
10    14249
1     13794
6     10217
9      7306
5      4665
13     3817
11     2945
3      2461
7      2361
16     2223
8      2077
25     2064
27     2053
21     1547
15     1458
19     1388
20     1236
12     1109
22     1085
26      966
30      940
17      913
24      772
28      649
32      592
29      406
37      333
2       321
31      318
18      316
36      228
34      227
33      218
23      186
35      180
38       65
39        9
Name: brand, dtype: int64

bodyType的特征分布如下：
bodyType特征有8个不同的值
0.0    41420
1.0    35272
2.0    30324
3.0    13491
4.0     9609
5.0     7607
6.0     6482
7.0     1289
Name: bodyType, dtype: int64

fuelType的特征分布如下：
fuelType特征有7个不同的值
0.0    91656
1.0    46991
2.0     2212
3.0      262
4.0      118
5.0       45
6.0       36
Name: fuelType, dtype: int64

gearbox的特征分布如下：
gearbox特征有2个不同的值
0.0    111623
1.0     32396
Name: gearbox, dtype: int64

notRepairedDamage的特征分布如下：
notRepairedDamage特征有2个不同的值
0.0    111361
1.0     14315
Name: notRepairedDamage, dtype: int64

regionCode的特征分布如下：
regionCode特征有7905个不同的值
419     369
764     258
125     137
176     136
462     134
       ... 
6414      1
7063      1
4239      1
5931      1
7267      1
Name: regionCode, Length: 7905, dtype: int64</code></pre><h1 id="数字特征分析"><a href="#数字特征分析" class="headerlink" title="数字特征分析"></a>数字特征分析</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numeric_features.append(&quot;price&quot;)</span><br><span class="line">print(numeric_features)</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;power&apos;, 
&apos;kilometer&apos;, 
&apos;v_0&apos;, 
&apos;v_1&apos;, 
&apos;v_2&apos;, 
&apos;v_3&apos;, 
&apos;v_4&apos;, 
&apos;v_5&apos;, 
&apos;v_6&apos;, 
&apos;v_7&apos;, 
&apos;v_8&apos;, 
&apos;v_9&apos;, 
&apos;v_10&apos;, 
&apos;v_11&apos;, 
&apos;v_12&apos;, 
&apos;v_13&apos;, 
&apos;v_14&apos;, 
&apos;price&apos;]</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data.head())</span><br></pre></td></tr></table></figure>


<table>
<thead>
<tr>
<th></th>
<th>SaleID</th>
<th>name</th>
<th>regDate</th>
<th>model</th>
<th>…</th>
<th>v_11</th>
<th>v_12</th>
<th>v_13</th>
<th>v_14</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>736</td>
<td>20040402</td>
<td>30.0</td>
<td>…</td>
<td>2.804097</td>
<td>-2.420821</td>
<td>0.795292</td>
<td>0.914762</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>2262</td>
<td>20030301</td>
<td>40.0</td>
<td>…</td>
<td>2.096338</td>
<td>-1.030483</td>
<td>-1.722674</td>
<td>0.245522</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>14874</td>
<td>20040403</td>
<td>115.0</td>
<td>…</td>
<td>1.803559</td>
<td>1.565330</td>
<td>-0.832687</td>
<td>-0.229963</td>
</tr>
<tr>
<td>3</td>
<td>3</td>
<td>71865</td>
<td>19960908</td>
<td>109.0</td>
<td>…</td>
<td>1.285940</td>
<td>-0.501868</td>
<td>-2.438353</td>
<td>-0.478699</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
<td>111080</td>
<td>20120103</td>
<td>110.0</td>
<td>…</td>
<td>0.910783</td>
<td>0.931110</td>
<td>2.834518</td>
<td>1.923482</td>
</tr>
</tbody></table>
<p>[5 rows x 29 columns]</p>
<h2 id="相关性分析"><a href="#相关性分析" class="headerlink" title="相关性分析"></a>相关性分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">## 1)相关性分析</span><br><span class="line">price_numeric &#x3D; Train_data[numeric_features]</span><br><span class="line">correlation &#x3D; price_numeric.corr() # 返回一个相关系数的矩阵</span><br><span class="line">print(correlation[&quot;price&quot;].sort_values(ascending&#x3D;False),&quot;\n&quot;) # 降序排序</span><br></pre></td></tr></table></figure>



<pre><code>price        1.000000
v_12         0.692823
v_8          0.685798
v_0          0.628397
power        0.219834
v_5          0.164317
v_2          0.085322
v_6          0.068970
v_1          0.060914
v_14         0.035911
v_13        -0.013993
v_7         -0.053024
v_4         -0.147085
v_9         -0.206205
v_10        -0.246175
v_11        -0.275320
kilometer   -0.440519
v_3         -0.730946
Name: price, dtype: float64 </code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">f , ax &#x3D; plt.subplots(figsize &#x3D; (7, 7))</span><br><span class="line">plt.title(&quot;Correlation of Numeric Features with Price&quot;)</span><br><span class="line">sns.heatmap(correlation, square&#x3D;True, vmax&#x3D;0.8) # 热图（显示相关系数）</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpNCD0.png" alt="12"></p>
<h2 id="查看几个特征的偏度和峰度"><a href="#查看几个特征的偏度和峰度" class="headerlink" title="查看几个特征的偏度和峰度"></a>查看几个特征的偏度和峰度</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 2)查看几个特征的偏度和峰度</span><br><span class="line">for col in numeric_features:</span><br><span class="line">    print(&quot;&#123;:15&#125;&quot;.format(col),&quot;Skewness:&#123;:05.2f&#125;&quot;.format(Train_data[col].skew()),</span><br><span class="line">    &quot;   &quot;,</span><br><span class="line">          &quot;Kurtosis:&#123;:06.2f&#125;&quot;.format(Train_data[col].kurt()))</span><br></pre></td></tr></table></figure>


<pre><code>power           Skewness:65.86     Kurtosis:5733.45
kilometer       Skewness:-1.53     Kurtosis:001.14
v_0             Skewness:-1.32     Kurtosis:003.99
v_1             Skewness:00.36     Kurtosis:-01.75
v_2             Skewness:04.84     Kurtosis:023.86
v_3             Skewness:00.11     Kurtosis:-00.42
v_4             Skewness:00.37     Kurtosis:-00.20
v_5             Skewness:-4.74     Kurtosis:022.93
v_6             Skewness:00.37     Kurtosis:-01.74
v_7             Skewness:05.13     Kurtosis:025.85
v_8             Skewness:00.20     Kurtosis:-00.64
v_9             Skewness:00.42     Kurtosis:-00.32
v_10            Skewness:00.03     Kurtosis:-00.58
v_11            Skewness:03.03     Kurtosis:012.57
v_12            Skewness:00.37     Kurtosis:000.27
v_13            Skewness:00.27     Kurtosis:-00.44
v_14            Skewness:-1.19     Kurtosis:002.39
price           Skewness:03.35     Kurtosis:019.00</code></pre><h2 id="每个数字特征得分布可视化"><a href="#每个数字特征得分布可视化" class="headerlink" title="每个数字特征得分布可视化"></a>每个数字特征得分布可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">## 3)每个数字特征得分布可视化</span><br><span class="line">f &#x3D; pd.melt(Train_data, value_vars&#x3D;numeric_features) # 转换</span><br><span class="line">g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2, sharex&#x3D;False,sharey&#x3D;False) # 以”variable“作“格子&quot;绘图</span><br><span class="line"># plt.show()</span><br><span class="line">g &#x3D; g.map(sns.distplot, &quot;value&quot;) # 以”value“绘制到”格子”图中</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpNhZV.png" alt="13"></p>
<ul>
<li>可以看出匿名特征相对分布均匀</li>
</ul>
<h2 id="数字特征相互之间的关系可视化"><a href="#数字特征相互之间的关系可视化" class="headerlink" title="数字特征相互之间的关系可视化"></a>数字特征相互之间的关系可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 4）数字特征相互之间的关系可视化</span><br><span class="line">sns.set() # 风格设置</span><br><span class="line">colunms &#x3D; [&quot;price&quot;, &quot;v_12&quot;, &quot;v_8&quot;, &quot;v_0&quot;, &quot;power&quot;, &quot;v_5&quot;, &quot;v_2&quot;, &quot;v_6&quot;, &quot;v_1&quot;, &quot;v_14&quot;]</span><br><span class="line">sns.pairplot(Train_data[colunms],size&#x3D;2, kind&#x3D;&quot;scatter&quot;, diag_kind&#x3D;&quot;kde&quot;) # 多变量图</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/GpaMtO.png" alt="14"></p>
<h2 id="多变量互相回归关系可视化"><a href="#多变量互相回归关系可视化" class="headerlink" title="多变量互相回归关系可视化"></a>多变量互相回归关系可视化</h2><ul>
<li><p>此处是多变量之间的关系可视化，可视化更多学习可参考很不错的文章<a href="https://www.jianshu.com/p/6e18d21a4cad" target="_blank" rel="noopener">https://www.jianshu.com/p/6e18d21a4cad</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data.columns)</span><br></pre></td></tr></table></figure>

<p>  Index([‘SaleID’, ‘name’, ‘regDate’, ‘model’, ‘brand’, ‘bodyType’, ‘fuelType’,</p>
<pre><code> &apos;gearbox&apos;, &apos;power&apos;, &apos;kilometer&apos;, &apos;notRepairedDamage&apos;, &apos;regionCode&apos;,
 &apos;creatDate&apos;, &apos;price&apos;, &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;, &apos;v_3&apos;, &apos;v_4&apos;, &apos;v_5&apos;, &apos;v_6&apos;,
 &apos;v_7&apos;, &apos;v_8&apos;, &apos;v_9&apos;, &apos;v_10&apos;, &apos;v_11&apos;, &apos;v_12&apos;, &apos;v_13&apos;, &apos;v_14&apos;],
dtype=&apos;object&apos;)</code></pre></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Y_train)</span><br></pre></td></tr></table></figure>
<pre><code>0         1850
1         3600
2         6222
3         2400
4         5200
      ... 
149995    5900
149996    9500
149997    7500
149998    4999
149999    4700
Name: price, Length: 150000, dtype: int64</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">## 5)多变量互相关系回归关系可视化</span><br><span class="line">fig,((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) &#x3D; plt.subplots(nrows&#x3D;5, ncols&#x3D;2, figsize&#x3D;(24, 20)) # 生成5行2列十个子图</span><br><span class="line"># [&#39;v_12&#39;, &#39;v_8&#39; , &#39;v_0&#39;, &#39;power&#39;, &#39;v_5&#39;,  &#39;v_2&#39;, &#39;v_6&#39;, &#39;v_1&#39;, &#39;v_14&#39;]</span><br><span class="line">v_12_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&quot;v_12&quot;]], axis&#x3D;1) # 合并成一列</span><br><span class="line">#print(v_12_scatter_plot)</span><br><span class="line">sns.regplot(x&#x3D;&quot;v_12&quot;, y&#x3D;&quot;price&quot;, data&#x3D;v_12_scatter_plot,scatter&#x3D;True,fit_reg&#x3D;True,ax&#x3D;ax1) # 数据与回归模型拟合</span><br><span class="line"></span><br><span class="line">v_8_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_8&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_8&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_8_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax2)</span><br><span class="line"></span><br><span class="line">v_0_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_0&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_0&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_0_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax3)</span><br><span class="line"></span><br><span class="line">power_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;power&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;power&#39;,y &#x3D; &#39;price&#39;,data &#x3D; power_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax4)</span><br><span class="line"></span><br><span class="line">v_5_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_5&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_5&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_5_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax5)</span><br><span class="line"></span><br><span class="line">v_2_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_2&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_2&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_2_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax6)</span><br><span class="line"></span><br><span class="line">v_6_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_6&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_6&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_6_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax7)</span><br><span class="line"></span><br><span class="line">v_1_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_1&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_1&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_1_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax8)</span><br><span class="line"></span><br><span class="line">v_14_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_14&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_14&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_14_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax9)</span><br><span class="line"></span><br><span class="line">v_13_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_13&#39;]],axis &#x3D; 1)</span><br><span class="line">sns.regplot(x&#x3D;&#39;v_13&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_13_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax10)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/Gp04C8.png" alt="15"></p>
<h1 id="类别特征分析"><a href="#类别特征分析" class="headerlink" title="类别特征分析"></a>类别特征分析</h1><h2 id="nunique分布"><a href="#nunique分布" class="headerlink" title="nunique分布"></a>nunique分布</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">## 1）nunique分布</span><br><span class="line">for fea in categorical_features:</span><br><span class="line">    print(Train_data[fea].nunique())</span><br></pre></td></tr></table></figure>
<pre><code>99662
248
40
8
7
2
2
7905</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(categorical_features)</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;name&apos;, 
&apos;model&apos;, 
&apos;brand&apos;, 
&apos;bodyType&apos;, 
&apos;fuelType&apos;, 
&apos;gearbox&apos;, 
&apos;notRepairedDamage&apos;, 
&apos;regionCode&apos;]</code></pre><h2 id="类别特征箱形图可视化"><a href="#类别特征箱形图可视化" class="headerlink" title="类别特征箱形图可视化"></a>类别特征箱形图可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">## 2)类别箱形图可视化</span><br><span class="line"># 因为 name和 regionCode的类别太稀疏了，这里我们把不稀疏的几类画一下</span><br><span class="line">categorical_features &#x3D; [&quot;model&quot;,</span><br><span class="line">                        &quot;brand&quot;,</span><br><span class="line">                        &quot;bodyType&quot;,</span><br><span class="line">                        &quot;fuelType&quot;,</span><br><span class="line">                        &quot;gearbox&quot;,</span><br><span class="line">                        &quot;notRepairedDamage&quot;]</span><br><span class="line">for c in categorical_features:</span><br><span class="line">    Train_data[c] &#x3D; Train_data[c].astype(&quot;category&quot;) # 强制转换数据类型</span><br><span class="line">    if Train_data[c].isnull().any(): # 检查字段缺失</span><br><span class="line">        Train_data[c] &#x3D; Train_data[c].cat.add_categories([&quot;MISSING&quot;]) # 添加新类别</span><br><span class="line">        Train_data[c] &#x3D; Train_data[c].fillna(&quot;MISSING&quot;) # 填充为NAN的值</span><br><span class="line">def boxplot(x, y, **kwargs):</span><br><span class="line">    sns.boxplot(x&#x3D;x, y&#x3D;y) # 箱形图</span><br><span class="line">    x&#x3D;plt.xticks(rotation&#x3D;90) #  设置坐标轴</span><br><span class="line"></span><br><span class="line">f &#x3D; pd.melt(Train_data, id_vars&#x3D;[&quot;price&quot;], value_vars&#x3D;categorical_features)</span><br><span class="line">g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2, sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line">g &#x3D; g.map(boxplot, &quot;value&quot;, &quot;price&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/GpDs0A.png" alt="16"></p>
<h2 id="类别特征的小提琴图可视化"><a href="#类别特征的小提琴图可视化" class="headerlink" title="类别特征的小提琴图可视化"></a>类别特征的小提琴图可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Train_data.columns)</span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;SaleID&apos;, &apos;name&apos;, &apos;regDate&apos;, &apos;model&apos;, &apos;brand&apos;, &apos;bodyType&apos;, &apos;fuelType&apos;,
       &apos;gearbox&apos;, &apos;power&apos;, &apos;kilometer&apos;, &apos;notRepairedDamage&apos;, &apos;regionCode&apos;,
       &apos;creatDate&apos;, &apos;price&apos;, &apos;v_0&apos;, &apos;v_1&apos;, &apos;v_2&apos;, &apos;v_3&apos;, &apos;v_4&apos;, &apos;v_5&apos;, &apos;v_6&apos;,
       &apos;v_7&apos;, &apos;v_8&apos;, &apos;v_9&apos;, &apos;v_10&apos;, &apos;v_11&apos;, &apos;v_12&apos;, &apos;v_13&apos;, &apos;v_14&apos;],
      dtype=&apos;object&apos;)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">## 3)类别特征的小提琴图可视化</span><br><span class="line">catg_list &#x3D; categorical_features</span><br><span class="line">target &#x3D; &quot;price&quot;</span><br><span class="line">for catg in catg_list:</span><br><span class="line">    sns.violinplot(x&#x3D;catg,y&#x3D;target,data&#x3D;Train_data)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/GprvPP.png" alt="17"><br><img src="https://s1.ax1x.com/2020/03/26/GpsmxU.png" alt="18"><br><img src="https://s1.ax1x.com/2020/03/26/Gps1aR.png" alt="19"><br><img src="https://s1.ax1x.com/2020/03/26/GpymFI.png" alt="20"><br><img src="https://s1.ax1x.com/2020/03/26/GpynYt.png" alt="21"><br><img src="https://s1.ax1x.com/2020/03/26/GpyQl8.png" alt="22"></p>
<h2 id="类别特征的柱形图可视化"><a href="#类别特征的柱形图可视化" class="headerlink" title="类别特征的柱形图可视化"></a>类别特征的柱形图可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(categorical_features)</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;model&apos;, 
&apos;brand&apos;, 
&apos;bodyType&apos;, 
&apos;fuelType&apos;, 
&apos;gearbox&apos;, 
&apos;notRepairedDamage&apos;]</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">## 4)类别特征的柱形图可视化</span><br><span class="line">def bar_plot(x,y,**kwargs): # 柱形图</span><br><span class="line">    sns.barplot(x&#x3D;x,y&#x3D;y)</span><br><span class="line">    x&#x3D;plt.xticks(rotation&#x3D;90)</span><br><span class="line">f &#x3D; pd.melt(Train_data, id_vars&#x3D;[&quot;price&quot;], value_vars&#x3D;categorical_features)</span><br><span class="line">g &#x3D; sns.FacetGrid(f, col&#x3D;&quot;variable&quot;,col_wrap&#x3D;2,sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line">g &#x3D; g.map(bar_plot, &quot;value&quot;, &quot;price&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/26/Gp6mB4.png" alt="23"></p>
<h2 id="类别特征的每个类别频数可视化"><a href="#类别特征的每个类别频数可视化" class="headerlink" title="类别特征的每个类别频数可视化"></a>类别特征的每个类别频数可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">## 5)类别特征的每个类别频数可视化</span><br><span class="line">def count_plot(x,**kwargs): # 计数直方图</span><br><span class="line">    sns.countplot(x&#x3D;x)</span><br><span class="line">    x&#x3D;plt.xticks(rotation&#x3D;90)</span><br><span class="line">f &#x3D; pd.melt(Train_data,value_vars&#x3D;categorical_features)</span><br><span class="line">g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2,sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line">g &#x3D; g.map(count_plot,&quot;value&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://s1.ax1x.com/2020/03/26/Gp6x8x.png" alt="24"></p>
<h2 id="用pandas-profiling生成数据报告"><a href="#用pandas-profiling生成数据报告" class="headerlink" title="用pandas_profiling生成数据报告"></a>用pandas_profiling生成数据报告</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 生成数据报告</span><br><span class="line">import pandas_profiling</span><br><span class="line"></span><br><span class="line">pfr &#x3D; pandas_profiling.ProfileReport(Train_data)</span><br><span class="line">pfr.to_file(&quot;.&#x2F;example.html&quot;)</span><br></pre></td></tr></table></figure>

<h1 id="代码片段"><a href="#代码片段" class="headerlink" title="代码片段"></a>代码片段</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br></pre></td><td class="code"><pre><span class="line"># 导入warnings包，利用过滤器来实现忽略警告语句</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import missingno as msno</span><br><span class="line"></span><br><span class="line">## pd.set_option(&#39;display.max_columns&#39;, None)# 显示所有列</span><br><span class="line">## pd.set_option(&#39;display.max_row&#39;, None)# 显示所有行</span><br><span class="line">## 1)载入训练集和测试集</span><br><span class="line">Train_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&quot;, sep &#x3D; &quot; &quot;)</span><br><span class="line">Test_data &#x3D; pd.read_csv(&quot;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&quot;, sep &#x3D; &quot; &quot;)</span><br><span class="line"></span><br><span class="line">## 2)简略观察数据（head()+shape)</span><br><span class="line">#print(Train_data.head().append(Train_data.tail()))</span><br><span class="line">#print(Train_data.shape)</span><br><span class="line">#</span><br><span class="line"># ## 3)通过describe()来熟悉相关统计量</span><br><span class="line"># print(Train_data.describe())</span><br><span class="line">#</span><br><span class="line"># ## 4)通过info()来熟悉数据类型</span><br><span class="line"># print(Train_data.info())</span><br><span class="line">#</span><br><span class="line"># ## 5)判断数据缺失和异常</span><br><span class="line"># print(Train_data.isnull().sum())</span><br><span class="line">#</span><br><span class="line">#nan可视化</span><br><span class="line"># missing &#x3D; Train_data.isnull().sum()</span><br><span class="line"># missing &#x3D; missing[missing &gt; 0]</span><br><span class="line"># missing.sort_values(inplace&#x3D;True) # 排序</span><br><span class="line"># missing.plot.bar() # 绘柱状图</span><br><span class="line"># plt.tight_layout() # 自动调整子图参数</span><br><span class="line"># plt.show()</span><br><span class="line"># # # 可视化看下缺省值</span><br><span class="line"># msno.matrix(Train_data.sample(250))</span><br><span class="line"># # plt.show()</span><br><span class="line"># msno.bar(Train_data.sample(1000)) # 条形图</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 6)查看异常值检测</span><br><span class="line"># Train_data.info()</span><br><span class="line">## print(Train_data[&quot;notRepairedDamage&quot;].value_counts()) # 返回包含值和count</span><br><span class="line">Train_data[&quot;notRepairedDamage&quot;].replace(&quot;-&quot;, np.nan, inplace&#x3D;True) # 将数据中‘-’替换成nan值</span><br><span class="line"># print(Train_data.isnull().sum())</span><br><span class="line"></span><br><span class="line">#print(Train_data[&quot;notRepairedDamage&quot;].value_counts())</span><br><span class="line">#Test_data.info()</span><br><span class="line">##print(Test_data[&quot;notRepairedDamage&quot;].value_counts())</span><br><span class="line">#Test_data[&quot;notRepairedDamage&quot;].replace(&quot;-&quot;, np.nan, inplace&#x3D;True)</span><br><span class="line">##print(Test_data[&quot;notRepairedDamage&quot;].value_counts())</span><br><span class="line"></span><br><span class="line"># 删除严重倾斜的数据</span><br><span class="line">#print(Train_data[&quot;seller&quot;].value_counts())</span><br><span class="line">#print(Train_data[&quot;offerType&quot;].value_counts())</span><br><span class="line"># print(Test_data[&quot;seller&quot;].value_counts())</span><br><span class="line"># print(Test_data[&quot;offerType&quot;].value_counts())</span><br><span class="line"></span><br><span class="line">del Train_data[&quot;seller&quot;]</span><br><span class="line">del Train_data[&quot;offerType&quot;]</span><br><span class="line"># print(Train_data.info())</span><br><span class="line"># print(Train_data.shape)</span><br><span class="line">#del Test_data[&quot;seller&quot;]</span><br><span class="line">#del Test_data[&quot;offerType&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 了解预测值的分布</span><br><span class="line"># print(Train_data[&quot;price&quot;])</span><br><span class="line"># print(Train_data[&quot;price&quot;].value_counts())</span><br><span class="line"></span><br><span class="line">## 1)总体分布情况(无界约翰逊分布等）</span><br><span class="line">import scipy.stats as st</span><br><span class="line"># y &#x3D; Train_data[&quot;price&quot;]</span><br><span class="line"># plt.figure(1); plt.title(&quot;Johnson SU&quot;) # 创建新图</span><br><span class="line"># sns.distplot(y, kde&#x3D;False, fit&#x3D;st.johnsonsu)</span><br><span class="line"># plt.figure(2); plt.title(&quot;Normal&quot;)</span><br><span class="line"># sns.distplot(y, kde&#x3D;False, fit&#x3D;st.norm)</span><br><span class="line"># plt.figure(3); plt.title(&quot;Log Normal&quot;)</span><br><span class="line"># sns.distplot(y, kde&#x3D;False, fit&#x3D;st.lognorm)</span><br><span class="line"># plt.show() # 最佳拟合是无界约翰逊分布</span><br><span class="line"></span><br><span class="line">## 2)查看skewness and kurtosis</span><br><span class="line"># sns.distplot(Train_data[&quot;price&quot;])</span><br><span class="line"># print(&quot;Skewness: %f&quot; % Train_data[&quot;price&quot;].skew()) # 偏度</span><br><span class="line"># print(&quot;Kurtosis: %f&quot; % Train_data[&quot;price&quot;].kurt()) # 峰度</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># print(Train_data.skew())</span><br><span class="line"># print(Train_data.kurt())</span><br><span class="line"># sns.distplot(Train_data.skew(), color&#x3D;&quot;blue&quot;, axlabel&#x3D;&quot;Skewness&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"># sns.distplot(Train_data.kurt(), color&#x3D;&quot;orange&quot;, axlabel&#x3D;&quot;Kurtness&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># 3)查看预测值的具体频数</span><br><span class="line"># plt.hist(Train_data[&quot;price&quot;], orientation&#x3D;&quot;vertical&quot;, histtype&#x3D;&quot;bar&quot;, color&#x3D;&quot;red&quot;)</span><br><span class="line"># plt.show() # 直方图</span><br><span class="line"># log变换之后的分布比较均匀，可以进行log变换进行预测，这也是预测问题常用的trick</span><br><span class="line"># plt.hist(np.log(Train_data[&quot;price&quot;]), orientation&#x3D;&quot;vertical&quot;, histtype&#x3D;&quot;bar&quot;, color&#x3D;&quot;red&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 查看特征</span><br><span class="line"># 分离label即预测值</span><br><span class="line">Y_train &#x3D; Train_data[&quot;price&quot;]</span><br><span class="line">## 这个区别方式适用于没有直接label coding的数据</span><br><span class="line">## 这里不适用，需要人为根据实际含义来区分</span><br><span class="line">## 数字特征</span><br><span class="line">## numeric_features &#x3D; Train_data.select_dtypes(include&#x3D;[np.number])</span><br><span class="line">## numeric_features.columns</span><br><span class="line">## # 类型特征</span><br><span class="line">## categorical_features &#x3D; Train_data.select_dtypes(include&#x3D;[np.object])</span><br><span class="line">## categorical_features.columns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数字特征</span><br><span class="line">numeric_features &#x3D; [&#39;power&#39;, &#39;kilometer&#39;, &#39;v_0&#39;, &#39;v_1&#39;, &#39;v_2&#39;, &#39;v_3&#39;, &#39;v_4&#39;, &#39;v_5&#39;, &#39;v_6&#39;, &#39;v_7&#39;, &#39;v_8&#39;, &#39;v_9&#39;, &#39;v_10&#39;, &#39;v_11&#39;, &#39;v_12&#39;, &#39;v_13&#39;,&#39;v_14&#39; ]</span><br><span class="line"># 类别特征</span><br><span class="line">categorical_features &#x3D; [&#39;name&#39;, &#39;model&#39;, &#39;brand&#39;, &#39;bodyType&#39;, &#39;fuelType&#39;, &#39;gearbox&#39;, &#39;notRepairedDamage&#39;, &#39;regionCode&#39;]</span><br><span class="line">## 类别特征nunique分布——Train_data</span><br><span class="line"># for cat_fea in categorical_features:</span><br><span class="line">#     print(cat_fea+&quot;的特征分布如下：&quot;)</span><br><span class="line">#     print(&quot;&#123;&#125;特征有&#123;&#125;个不同的值&quot;.format(cat_fea, Train_data[cat_fea].nunique()))</span><br><span class="line">#     print(Train_data[cat_fea].value_counts())</span><br><span class="line">## 类别特征nunique分布——Test_data</span><br><span class="line"># for cat_fea in categorical_features:</span><br><span class="line">#     print(cat_fea+&quot;的特征分布如下：&quot;)</span><br><span class="line">#     print(&quot;&#123;&#125;特征有&#123;&#125;个不同的值&quot;.format(cat_fea, Test_data[cat_fea].nunique()))</span><br><span class="line">#     print(Test_data[cat_fea].value_counts())</span><br><span class="line"></span><br><span class="line">## 数字特征分析</span><br><span class="line">numeric_features.append(&quot;price&quot;)</span><br><span class="line"># print(numeric_features)</span><br><span class="line">#print(Train_data.head())</span><br><span class="line">## 1)相关性分析</span><br><span class="line">price_numeric &#x3D; Train_data[numeric_features]</span><br><span class="line">correlation &#x3D; price_numeric.corr() # 返回一个相关系数的矩阵</span><br><span class="line"># print(correlation[&quot;price&quot;].sort_values(ascending&#x3D;False),&quot;\n&quot;) # 降序排序</span><br><span class="line"></span><br><span class="line"># f , ax &#x3D; plt.subplots(figsize &#x3D; (7, 7))</span><br><span class="line"># plt.title(&quot;Correlation of Numeric Features with Price&quot;)</span><br><span class="line"># sns.heatmap(correlation, square&#x3D;True, vmax&#x3D;0.8) # 热图（显示相关系数）</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 2)查看几个特征的偏度和峰度</span><br><span class="line"># for col in numeric_features:</span><br><span class="line">#     print(&quot;&#123;:15&#125;&quot;.format(col),&quot;Skewness:&#123;:05.2f&#125;&quot;.format(Train_data[col].skew()),</span><br><span class="line">#     &quot;   &quot;,</span><br><span class="line">#           &quot;Kurtosis:&#123;:06.2f&#125;&quot;.format(Train_data[col].kurt()))</span><br><span class="line"></span><br><span class="line">## 3)每个数字特征得分布可视化</span><br><span class="line"># f &#x3D; pd.melt(Train_data, value_vars&#x3D;numeric_features) # 转换</span><br><span class="line"># g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2, sharex&#x3D;False,sharey&#x3D;False) # 以”variable“作“格子&quot;绘图</span><br><span class="line"># # plt.show()</span><br><span class="line"># g &#x3D; g.map(sns.distplot, &quot;value&quot;) # 以”value“绘制到”格子”图中</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 4）数字特征相互之间的关系可视化</span><br><span class="line"># sns.set() # 风格设置</span><br><span class="line"># colunms &#x3D; [&quot;price&quot;, &quot;v_12&quot;, &quot;v_8&quot;, &quot;v_0&quot;, &quot;power&quot;, &quot;v_5&quot;, &quot;v_2&quot;, &quot;v_6&quot;, &quot;v_1&quot;, &quot;v_14&quot;]</span><br><span class="line"># sns.pairplot(Train_data[colunms],size&#x3D;2, kind&#x3D;&quot;scatter&quot;, diag_kind&#x3D;&quot;kde&quot;) # 多变量图</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># print(Train_data.columns)</span><br><span class="line"># print(Y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 5)多变量互相关系回归关系可视化</span><br><span class="line"># fig,((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) &#x3D; plt.subplots(nrows&#x3D;5, ncols&#x3D;2, figsize&#x3D;(24, 20)) # 生成5行2列十个子图</span><br><span class="line"># # [&#39;v_12&#39;, &#39;v_8&#39; , &#39;v_0&#39;, &#39;power&#39;, &#39;v_5&#39;,  &#39;v_2&#39;, &#39;v_6&#39;, &#39;v_1&#39;, &#39;v_14&#39;]</span><br><span class="line"># v_12_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&quot;v_12&quot;]], axis&#x3D;1) # 合并成一列</span><br><span class="line"># #print(v_12_scatter_plot)</span><br><span class="line"># sns.regplot(x&#x3D;&quot;v_12&quot;, y&#x3D;&quot;price&quot;, data&#x3D;v_12_scatter_plot,scatter&#x3D;True,fit_reg&#x3D;True,ax&#x3D;ax1) # 数据与回归模型拟合</span><br><span class="line">#</span><br><span class="line"># v_8_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_8&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_8&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_8_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax2)</span><br><span class="line">#</span><br><span class="line"># v_0_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_0&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_0&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_0_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax3)</span><br><span class="line">#</span><br><span class="line"># power_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;power&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;power&#39;,y &#x3D; &#39;price&#39;,data &#x3D; power_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax4)</span><br><span class="line">#</span><br><span class="line"># v_5_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_5&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_5&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_5_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax5)</span><br><span class="line">#</span><br><span class="line"># v_2_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_2&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_2&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_2_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax6)</span><br><span class="line">#</span><br><span class="line"># v_6_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_6&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_6&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_6_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax7)</span><br><span class="line">#</span><br><span class="line"># v_1_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_1&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_1&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_1_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax8)</span><br><span class="line">#</span><br><span class="line"># v_14_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_14&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_14&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_14_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax9)</span><br><span class="line">#</span><br><span class="line"># v_13_scatter_plot &#x3D; pd.concat([Y_train,Train_data[&#39;v_13&#39;]],axis &#x3D; 1)</span><br><span class="line"># sns.regplot(x&#x3D;&#39;v_13&#39;,y &#x3D; &#39;price&#39;,data &#x3D; v_13_scatter_plot,scatter&#x3D; True, fit_reg&#x3D;True, ax&#x3D;ax10)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line"># 类别特征分析</span><br><span class="line">## 1）nunique分布</span><br><span class="line"># for fea in categorical_features:</span><br><span class="line">#     print(Train_data[fea].nunique())</span><br><span class="line">#</span><br><span class="line"># print(categorical_features)</span><br><span class="line"></span><br><span class="line">## 2)类别箱形图可视化</span><br><span class="line"># 因为 name和 regionCode的类别太稀疏了，这里我们把不稀疏的几类画一下</span><br><span class="line">categorical_features &#x3D; [&quot;model&quot;,</span><br><span class="line">                        &quot;brand&quot;,</span><br><span class="line">                        &quot;bodyType&quot;,</span><br><span class="line">                        &quot;fuelType&quot;,</span><br><span class="line">                        &quot;gearbox&quot;,</span><br><span class="line">                        &quot;notRepairedDamage&quot;]</span><br><span class="line">for c in categorical_features:</span><br><span class="line">    Train_data[c] &#x3D; Train_data[c].astype(&quot;category&quot;) # 强制转换数据类型</span><br><span class="line">    if Train_data[c].isnull().any(): # 检查字段缺失</span><br><span class="line">        Train_data[c] &#x3D; Train_data[c].cat.add_categories([&quot;MISSING&quot;]) # 添加新类别</span><br><span class="line">        Train_data[c] &#x3D; Train_data[c].fillna(&quot;MISSING&quot;) # 填充为NAN的值</span><br><span class="line"># def boxplot(x, y, **kwargs):</span><br><span class="line">#     sns.boxplot(x&#x3D;x, y&#x3D;y) # 箱形图</span><br><span class="line">#     x&#x3D;plt.xticks(rotation&#x3D;90) #  设置坐标轴</span><br><span class="line">#</span><br><span class="line"># f &#x3D; pd.melt(Train_data, id_vars&#x3D;[&quot;price&quot;], value_vars&#x3D;categorical_features)</span><br><span class="line"># g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2, sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line"># g &#x3D; g.map(boxplot, &quot;value&quot;, &quot;price&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 3)类别特征的小提琴图可视化</span><br><span class="line">#print(Train_data.columns)</span><br><span class="line"># catg_list &#x3D; categorical_features</span><br><span class="line"># target &#x3D; &quot;price&quot;</span><br><span class="line"># for catg in catg_list:</span><br><span class="line">#     sns.violinplot(x&#x3D;catg,y&#x3D;target,data&#x3D;Train_data)</span><br><span class="line">#     plt.show()</span><br><span class="line"></span><br><span class="line"># print(categorical_features)</span><br><span class="line"></span><br><span class="line">## 4)类别特征的柱形图可视化</span><br><span class="line"># def bar_plot(x,y,**kwargs): # 柱形图</span><br><span class="line">#     sns.barplot(x&#x3D;x,y&#x3D;y)</span><br><span class="line">#     x&#x3D;plt.xticks(rotation&#x3D;90)</span><br><span class="line"># f &#x3D; pd.melt(Train_data, id_vars&#x3D;[&quot;price&quot;], value_vars&#x3D;categorical_features)</span><br><span class="line"># g &#x3D; sns.FacetGrid(f, col&#x3D;&quot;variable&quot;,col_wrap&#x3D;2,sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line"># g &#x3D; g.map(bar_plot, &quot;value&quot;, &quot;price&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 5)类别特征的每个类别频数可视化</span><br><span class="line"># def count_plot(x,**kwargs): # 计数直方图</span><br><span class="line">#     sns.countplot(x&#x3D;x)</span><br><span class="line">#     x&#x3D;plt.xticks(rotation&#x3D;90)</span><br><span class="line"># f &#x3D; pd.melt(Train_data,value_vars&#x3D;categorical_features)</span><br><span class="line"># g &#x3D; sns.FacetGrid(f,col&#x3D;&quot;variable&quot;, col_wrap&#x3D;2,sharex&#x3D;False,sharey&#x3D;False,size&#x3D;5)</span><br><span class="line"># g &#x3D; g.map(count_plot,&quot;value&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line"></span><br><span class="line">## 生成数据报告</span><br><span class="line">import pandas_profiling</span><br><span class="line">#</span><br><span class="line"># pfr &#x3D; pandas_profiling.ProfileReport(Train_data)</span><br><span class="line"># pfr.to_file(&quot;.&#x2F;example.html&quot;)</span><br></pre></td></tr></table></figure>

<h1 id="经验总结"><a href="#经验总结" class="headerlink" title="经验总结"></a>经验总结</h1><p>所给出的EDA步骤为广为普遍的步骤，在实际的不管是工程还是比赛过程中，这只是最开始的一步，也是最基本的一步。</p>
<p>接下来一般要结合模型的效果以及特征工程等来分析数据的实际建模情况，根据自己的一些理解，查阅文献，对实际问题做出判断和深入的理解。</p>
<h2 id="最后不断进行EDA与数据处理和挖掘，来到达更好的数据结构和分布以及较为强势相关的特征"><a href="#最后不断进行EDA与数据处理和挖掘，来到达更好的数据结构和分布以及较为强势相关的特征" class="headerlink" title="最后不断进行EDA与数据处理和挖掘，来到达更好的数据结构和分布以及较为强势相关的特征"></a>最后不断进行EDA与数据处理和挖掘，来到达更好的数据结构和分布以及较为强势相关的特征</h2><p>数据探索在机器学习中我们一般称为EDA（Exploratory Data Analysis）：</p>
<blockquote>
<p>是指对已有的数据（特别是调查或观察得来的原始数据）在尽量少的先验假定下进行探索，通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律的一种数据分析方&gt;法。</p>
</blockquote>
<p>数据探索有利于我们发现数据的一些特性，数据之间的关联性，对于后续的特征构建是很有帮助的。</p>
<ol>
<li><p>对于数据的初步分析（直接查看数据，或.sum(), .mean()，.descirbe()等统计函数）可以从：样本数量，训练集数量，是否有时间特征，是否是时许问题，特征所表示的含义（非匿名特征），特征类型（字符类似，int，float，time），特征的缺失情况（注意缺失的在数据中的表现形式，有些是空的有些是”NAN”符号等），特征的均值方差情况。</p>
</li>
<li><p>分析记录某些特征值缺失占比30%以上样本的缺失处理，有助于后续的模型验证和调节，分析特征应该是填充（填充方式是什么，均值填充，0填充，众数填充等），还是舍去，还是先做样本分类用不同的特征模型去预测。</p>
</li>
<li><p>对于异常值做专门的分析，分析特征异常的label是否为异常值（或者偏离均值较远或者是特殊符号）,异常值是否应该剔除，还是用正常值填充，是记录异常，还是机器本身异常等。</p>
</li>
<li><p>对于Label做专门的分析，分析标签的分布情况等。</p>
</li>
<li><p>进步分析可以通过对特征作图，特征和label联合做图（统计图，离散图），直观了解特征的分布情况，通过这一步也可以发现数据之中的一些异常值等，通过箱型图分析一些特征值的偏离情况，对于特征和特征联合作图，对于特征和label联合作图，分析其中的一些关联性。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/03/21/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/21/Datawhale%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-Task1/" class="post-title-link" itemprop="url">Datawhale零基础入门数据挖掘-Task1</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-21 18:25:46" itemprop="dateCreated datePublished" datetime="2020-03-21T18:25:46+08:00">2020-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-23 16:21:01" itemprop="dateModified" datetime="2020-03-23T16:21:01+08:00">2020-03-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">数据挖掘及机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li><p>学习背景:由Datawhale与天池开放的零基础入门数据挖掘赛事-<a href="https://tianchi.aliyun.com/competition/entrance/231784/introduction?spm=5176.12281949.1003.2.493e2448KgHsEd" target="_blank" rel="noopener">二手车交易价格预测</a></p>
</li>
<li><p>赛题概括:赛题以预测二手车的交易价格为任务，数据集报名后可见并可下载，该数据来自某交易平台的二手车交易记录，总数据量超过40w，包含31列变量信息，其中15列为匿名变量。为了保证比赛的公平性，将会从中抽取15万条作为训练集，5万条作为测试集A，5万条作为测试集B，同时会对name、model、brand和regionCode等信息进行脱敏。</p>
</li>
</ul>
<h1 id="赛题分析"><a href="#赛题分析" class="headerlink" title="赛题分析"></a>赛题分析</h1><h2 id="数据概括"><a href="#数据概括" class="headerlink" title="数据概括"></a>数据概括</h2><p>一般而言，对于数据在比赛界面都有对应的数据概况介绍（匿名特征除外），说明列的性质特征。了解列的性质会有助于我们对于数据的理解和后续分析。 Tip:匿名特征，就是未告知数据列所属的性质的特征列。</p>
<blockquote>
<p>train.csv</p>
<ul>
<li>SaleID - 销售样本ID</li>
<li>name - 汽车编码</li>
<li>regDate - 汽车注册时间</li>
<li>model - 车型编码</li>
<li>brand - 品牌</li>
<li>bodyType - 车身类型</li>
<li>fuelType - 燃油类型</li>
<li>gearbox - 变速箱</li>
<li>power - 汽车功率</li>
<li>kilometer - 汽车行驶公里</li>
<li>notRepairedDamage - 汽车有尚未修复的损坏</li>
<li>regionCode - 看车地区编码</li>
<li>seller - 销售方</li>
<li>offerType - 报价类型</li>
<li>creatDate - 广告发布时间</li>
<li>price - 汽车价格</li>
<li>‘v_0’, ‘v_1’, ‘v_2’, ‘v_3’, ‘v_4’, ‘v_5’, ‘v_6’, ‘v_7’, ‘v_8’, ‘v_9’, ‘v_10’, ‘v_11’, ‘v_12’, ‘v_13’,’v_14’ 【匿名特征，包含v0-14在内15个匿名特征】 　</li>
</ul>
</blockquote>
<h2 id="评测标准"><a href="#评测标准" class="headerlink" title="评测标准"></a>评测标准</h2><p>赛题评价目标为MAE(Mean Absolute Error):</p>
<blockquote>
<p><img src="https://s1.ax1x.com/2020/03/21/8hVCfs.png" alt=""><br>MAE越小，说明模型预测得越准确</p>
</blockquote>
<h2 id="预测建模"><a href="#预测建模" class="headerlink" title="预测建模"></a>预测建模</h2><ul>
<li>预测建模就是使用历史数据建立一个模型，去给没有答案的新数据做预测的问题</li>
</ul>
<p>关于预测建模，可以在下面这篇文章中了解更多信息:</p>
<blockquote>
<p>Gentle Introduction to Predictive Modeling: <a href="https://machinelearningmastery.com/gentle-introduction-to-predictive-modeling/" target="_blank" rel="noopener">https://machinelearningmastery.com/gentle-introduction-to-predictive-modeling/</a></p>
</blockquote>
<p>预测建模可以被描述成一个近似求取从输入变量（X）到输出变量（y）的映射函数的数学问题。这被称为函数逼近问题</p>
<p>建模算法的任务就是在给定的可用时间和资源的限制下，去寻找最佳映射函数。更多关于机器学习中应用逼近函数的内容，请参阅下面这篇文章：</p>
<blockquote>
<p>机器学习是如何运行的（how machine learning work,<a href="https://machinelearningmastery.com/how-machine-learning-algorithms-work/" target="_blank" rel="noopener">https://machinelearningmastery.com/how-machine-learning-algorithms-work/</a>)</p>
</blockquote>
<p>一般而言，我们可以将函数逼近任务划分为分类任务和回归任务</p>
<h3 id="分类预测建模"><a href="#分类预测建模" class="headerlink" title="分类预测建模"></a>分类预测建模</h3><p>分类预测建模是逼近一个从输入变量（X）到离散的输出变量（y）之间的映射函数（f）</p>
<p>输出变量经常被称作标签或者类别。映射函数会对一个给定的观察样本预测一个类别标签</p>
<p>例如，一个文本邮件可以被归为两类：「垃圾邮件」，和「非垃圾邮件」</p>
<ul>
<li>分类问题需要把样本分为两类或者多类</li>
<li>分类的输入可以是实数也可以有离散变量</li>
<li>只有两个类别的分类问题经常被称作两类问题或者二元分类问题</li>
<li>具有多于两类的问题经常被称作多分类问题</li>
<li>样本属于多个类别的问题被称作多标签分类问题</li>
</ul>
<p>分类模型经常为输入样本预测得到与每一类别对应的像概率一样的连续值。这些概率可以被解释为样本属于每个类别的似然度或者置信度。预测到的概率可以通过选择概率最高的类别转换成类别标签</p>
<p>例如，某封邮件可能以 0.1 的概率被分为「垃圾邮件」，以 0.9 的概率被分为「非垃圾邮件」。因为非垃圾邮件的标签的概率最大，所以我们可以将概率转换成「非垃圾邮件」的标签</p>
<p>有很多用来衡量分类预测模型的性能的指标，但是分类准确率可能是最常用的一个</p>
<p>例如，如果一个分类预测模型做了 5 个预测，其中有 3 个是正确的，2 个这是错误的，那么这个模型的准确率就是 60%：</p>
<blockquote>
<p>accuracy = correct predictions / total predictions * 100<br>accuracy = 3 / 5 * 100<br>accuracy = 60%</p>
</blockquote>
<p>能够学习分类模型的算法就叫做分类算法</p>
<h3 id="回归预测模型"><a href="#回归预测模型" class="headerlink" title="回归预测模型"></a>回归预测模型</h3><p>回归预测建模是逼近一个从输入变量（X）到连续的输出变量（y）的函数映射</p>
<p>连续输出变量是一个实数，例如一个整数或者浮点数。这些变量通常是数量或者尺寸大小等等</p>
<p>例如，一座房子可能被预测到以 xx 美元出售，也许会在 $100,000 t 到$200,000 的范围内</p>
<ul>
<li>回归问题需要预测一个数量</li>
<li>回归的输入变量可以是连续的也可以是离散的</li>
<li>有多个输入变量的通常被称作多变量回归</li>
<li>输入变量是按照时间顺序的回归称为时间序列预测问题</li>
<li>因为回归预测问题预测的是一个数量，所以模型的性能可以用预测结果中的错误来评价</li>
</ul>
<p>有很多评价回归预测模型的方式，但是最常用的一个可能是计算误差值的均方根，即 RMSE</p>
<p>例如，如果回归预测模型做出了两个预测结果，一个是 1.5，对应的期望结果是 1.0；另一个是 3.3 对应的期望结果是 3.0. 那么，这两个回归预测的 RMSE 如下：</p>
<blockquote>
<p>RMSE = sqrt(average(error^2))<br>RMSE = sqrt(((1.0 - 1.5)^2 + (3.0 - 3.3)^2) / 2)<br>RMSE = sqrt((0.25 + 0.09) / 2)<br>RMSE = sqrt(0.17)<br>RMSE = 0.412</p>
</blockquote>
<p>使用 RMSE 的好处就是错误评分的单位与预测结果是一样的</p>
<p>一个能够学习回归预测模型的算法称作回归算法</p>
<p>有些算法的名字也有「regression,回归」一词，例如线性回归和 logistics 回归，这种情况有时候会让人迷惑因为线性回归确实是一个回归问题，但是 logistics 回归却是一个分类问题</p>
<h3 id="分类-vs-回归"><a href="#分类-vs-回归" class="headerlink" title="分类 vs 回归"></a>分类 vs 回归</h3><p>分类预测建模问题与回归预测建模问题是不一样的</p>
<ul>
<li>分类是预测一个离散标签的任务</li>
<li>回归是预测一个连续数量的任务</li>
</ul>
<p>分类和回归也有一些相同的地方：</p>
<ul>
<li>分类算法可能预测到一个连续的值，但是这些连续值对应的是一个类别的概率的形式</li>
<li>回归算法可以预测离散值，但是以整型量的形式预测离散值的</li>
</ul>
<p>有些算法既可以用来分类，也可以稍作修改就用来做回归问题，例如决策树和人工神经网络。但是一些算法就不行了——或者说是不太容易用于这两种类型的问题，例如线性回归是用来做回归预测建模的，logistics 回归是用来做分类预测建模的</p>
<p>重要的是，我们评价分类模型和预测模型的方式是不一样的，例如：</p>
<ul>
<li>分类预测可以使用准确率来评价，而回归问题则不能</li>
<li>回归预测可以使用均方根误差来评价，但是分类问题则不能</li>
</ul>
<h3 id="分类问题和回归问题之间的转换"><a href="#分类问题和回归问题之间的转换" class="headerlink" title="分类问题和回归问题之间的转换"></a>分类问题和回归问题之间的转换</h3><p>在一些情况中是可以将回归问题转换成分类问题的。例如，被预测的数量是可以被转换成离散数值的范围的</p>
<p>例如，在$0 到$100 之间的金额可以被分为两个区间：</p>
<ul>
<li>class 0：$0 到$49</li>
<li>class 1: $50 到$100</li>
</ul>
<p>这通常被称作离散化，结果中的输出变量是一个分类，分类的标签是有顺序的（称为叙序数）</p>
<p>在一些情况中，分类是可以转换成回归问题的。例如，一个标签可以被转换成一个连续的范围</p>
<p>一些算法早已通过为每一个类别预测一个概率，这个概率反过来又可以被扩展到一个特定的数值范围：</p>
<blockquote>
<p>quantity = min + probability * range</p>
</blockquote>
<p>与此对应，一个类别值也可以被序数化，并且映射到一个连续的范围中：</p>
<ul>
<li>$0 到 $49 是类别 1</li>
<li>$0 到 $49 是类别 2</li>
</ul>
<p>如果分类问题中的类别标签没有自然顺序的关系，那么从分类问题到回归问题的转换也许会导致奇怪的结果或者很差的性能，因为模型可能学到一个并不存在于从输入到连续输出之间的映射函数</p>
<p><em>原文链接</em><a href="https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/" target="_blank" rel="noopener">https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/</a></p>
<h2 id="关于评价指标"><a href="#关于评价指标" class="headerlink" title="关于评价指标"></a>关于评价指标</h2><ul>
<li>评估指标即是我们对于一个模型效果的数值型量化。（有点类似与对于一个商品评价打分，而这是针对于模型效果和理想效果之间的一个打分）</li>
</ul>
<p>一般来说分类和回归问题的评价指标有如下一些形式：</p>
<h3 id="分类算法常见的评估指标如下："><a href="#分类算法常见的评估指标如下：" class="headerlink" title="分类算法常见的评估指标如下："></a>分类算法常见的评估指标如下：</h3><blockquote>
<ul>
<li>对于二类分类器/分类算法，评价指标主要有accuracy， Precision，Recall，F-score，Pr曲线，ROC-AUC曲线</li>
<li>对于多类分类器/分类算法，评价指标主要有accuracy， 宏平均和微平均，F-score</li>
</ul>
</blockquote>
<h3 id="对于回归预测类常见的评估指标如下"><a href="#对于回归预测类常见的评估指标如下" class="headerlink" title="对于回归预测类常见的评估指标如下:"></a>对于回归预测类常见的评估指标如下:</h3><blockquote>
<ul>
<li>平均绝对误差（Mean Absolute Error，MAE），均方误差（Mean Squared Error，MSE），平均绝对百分误差（Mean Absolute Percentage Error，MAPE），均方根误差（Root Mean Squared Error）， R2（R-Square）</li>
</ul>
</blockquote>
<h4 id="平均绝对误差"><a href="#平均绝对误差" class="headerlink" title="平均绝对误差"></a>平均绝对误差</h4><ul>
<li>平均绝对误差（Mean Absolute Error，MAE）:其能更好地反映预测值与真实值误差的实际情况，其计算公式如下：<br>$$MAE=\frac{1}{N} \sum_{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|$$</li>
</ul>
<h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><ul>
<li>均方误差（Mean Squared Error，MSE）,均方误差,其计算公式为：<br>$$MSE=\frac{1}{N} \sum_{i}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}$$</li>
</ul>
<h3 id="R2（R-Square）"><a href="#R2（R-Square）" class="headerlink" title="R2（R-Square）"></a>R2（R-Square）</h3><ul>
<li>残差平方和:<br>$$SS_{res}=\sum\left(y_{i}-\hat{y}_{i}\right)^{2}$$</li>
<li>总平均值:<br>$$SS_{tot}=\sum\left(y_{i}-\overline{y}_{i}\right)^{2}$$</li>
<li>其中$\overline{y}$表示$y$的平均值得到$R^2$的表达式为:</li>
</ul>
<p>$$R^{2}=1-\frac{SS_{res}}{SS_{tot}}$$</p>
<p>$R^2$用于度量因变量的变异中可由自变量解释部分所占的比例，取值范围是 0~1，$R^2$越接近1,表明回归平方和占总平方和的比例越大,回归线与各观测点越接近，用x的变化来解释y值变化的部分就越多,回归的拟合程度就越好。所以$R^2$也称为拟合优度（Goodness of Fit）的统计量</p>
<p>$y_{i}$表示真实值,</p>
<p>$\hat{y}_{i}$表示预测值,</p>
<p>$\overline{y}_{i}$表示样本均值。得分越高拟合效果越好</p>
<h3 id="几何解释"><a href="#几何解释" class="headerlink" title="几何解释"></a>几何解释</h3><p><img src="https://s1.ax1x.com/2020/03/21/8hqPgA.png" alt=""><br>上图红色点是incoming自变量与Consuming因变量对应的散点图，蓝色线是回归方程线（最小二乘法得到）；<br>这里红色点$y_{i}$表示一个响应观测值点（共4个），蓝色点$f_{i}$是响应观测值对应的回归曲线上的点，两个的差值就是残差，残差值共有4个,$\overline{y}$是响应变量的平均值。</p>
<p>根据平方和分解公式:<br><img src="https://s1.ax1x.com/2020/03/21/8hquCQ.jpg" alt=""><br>即：SS 总体=SS 回归 + SS 残差 (观测值与平均值的差值平方和被残差平方和以及回归差值平方和之和解释)</p>
<h1 id="分析结果"><a href="#分析结果" class="headerlink" title="分析结果"></a>分析结果</h1><ol>
<li>此题为传统的数据挖掘问题，通过数据科学以及机器学习深度学习的办法来进行建模得到结果。</li>
<li>此题是一个典型的回归问题。</li>
<li>主要应用xgb、lgb、catboost，以及pandas、numpy、matplotlib、seabon、sklearn、keras等等数据挖掘常用库或者框架来进行数据挖掘任务。</li>
<li>通过EDA来挖掘数据的联系和自我熟悉数据</li>
</ol>
<h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"># 1) 载入训练集和测试集</span><br><span class="line">Train_data &#x3D; pd.read_csv(&#39;.&#x2F;datalab&#x2F;used_car_train_20200313.csv&#39;,sep&#x3D;(&#39; &#39;))</span><br><span class="line">Test_data &#x3D; pd.read_csv(&#39;.&#x2F;datalab&#x2F;used_car_testA_20200313.csv&#39;,sep&#x3D;(&#39; &#39;))</span><br><span class="line"></span><br><span class="line">print(Train_data.shape) # 返回行,列数</span><br><span class="line">print(Test_data.shape)</span><br><span class="line">out_put &#x3D; Train_data.head(4) # 返回前四行字段数据</span><br><span class="line">print(out_put)</span><br><span class="line"></span><br><span class="line"># 2)分类指标评价计算</span><br><span class="line"></span><br><span class="line">## accuracy</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line">y_pred &#x3D; [0, 1, 0, 1] # 预测标签</span><br><span class="line">y_true &#x3D; [0, 1, 1, 1] # 正确标签</span><br><span class="line">print(&#39;ACC:&#39;,accuracy_score(y_true, y_pred)) # 返回正确样本所占比例（float）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">TP：真正例：即将正样本预测为正样本</span><br><span class="line">TN：真反例：即将负样本预测为负样本</span><br><span class="line">FP：假正例：将负样本预测为了正样本</span><br><span class="line">FN：假反例：将正样本预测为了负样本</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">## Percision,Recall,F1-score</span><br><span class="line">from sklearn import metrics</span><br><span class="line">y_pred &#x3D; [0, 1, 0, 0]</span><br><span class="line">y_true &#x3D; [0, 1, 0, 1]</span><br><span class="line">print(&#39;Precision&#39;,metrics.precision_score(y_true, y_pred)) # 返回 TP &#x2F; (TP + FP)</span><br><span class="line">print(&#39;Recall&#39;,metrics.recall_score(y_true, y_pred)) # 返回 TP &#x2F; (TP + FN)</span><br><span class="line">print(&#39;F1-score:&#39;,metrics.f1_score(y_true, y_pred)) # 返回 2*(P*R)&#x2F;(P+R)</span><br><span class="line"></span><br><span class="line">## AUC</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line">y_true &#x3D; np.array([0, 0, 1, 1]) # True labels or binary label indicators</span><br><span class="line">y_scores &#x3D; np.array([0.1, 0.4, 0.35, 0.8]) # Target scores</span><br><span class="line">print(&#39;AUC socre:&#39;,roc_auc_score(y_true, y_scores))</span><br><span class="line"></span><br><span class="line"># coding&#x3D;utf-8</span><br><span class="line"># import numpy as np</span><br><span class="line"># from sklearn import metrics</span><br><span class="line"></span><br><span class="line"># MAPE需要自己实现</span><br><span class="line">def mape(y_true, y_pred):</span><br><span class="line">    return np.mean(np.abs((y_pred - y_true) &#x2F; y_true))</span><br><span class="line"></span><br><span class="line">y_true &#x3D; np.array([1.0, 5.0, 4.0, 3.0, 2.0, 5.0, -3.0])</span><br><span class="line">y_pred &#x3D; np.array([1.0, 4.5, 3.8, 3.2, 3.0, 4.8, -2.2])</span><br><span class="line"></span><br><span class="line"># MSE</span><br><span class="line">print(&#39;MSE:&#39;,metrics.mean_squared_error(y_true, y_pred))</span><br><span class="line"># RMSE</span><br><span class="line">print(&#39;RMSE:&#39;,np.sqrt(metrics.mean_squared_error(y_true, y_pred)))</span><br><span class="line"># MAE</span><br><span class="line">print(&#39;MAE:&#39;,metrics.mean_absolute_error(y_true, y_pred))</span><br><span class="line"># MAPE</span><br><span class="line">print(&#39;MAPE:&#39;,mape(y_true, y_pred))</span><br><span class="line"></span><br><span class="line">## R2-score</span><br><span class="line">from sklearn.metrics import r2_score</span><br><span class="line">y_true &#x3D; [3, -0.5, 2, 7]</span><br><span class="line">y_pred &#x3D; [2.5, 0.0, 2, 8]</span><br><span class="line">print(&#39;R2-score:&#39;,r2_score(y_true, y_pred))</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://happybear1234.github.io/2020/03/19/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8github%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="happybear">
      <meta itemprop="description" content="happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="happybear">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/19/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8github%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/" class="post-title-link" itemprop="url">如何使用github创建博客</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-19 21:52:03" itemprop="dateCreated datePublished" datetime="2020-03-19T21:52:03+08:00">2020-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-21 12:05:36" itemprop="dateModified" datetime="2020-03-21T12:05:36+08:00">2020-03-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="firestore-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>990</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>-利用 Github 搭建博客需要熟悉git方便管理.操作<a href="http://iissnan.com/progit/" target="_blank" rel="noopener">如果对git感兴趣请参考</a></p>
<h1 id="搭建环境"><a href="#搭建环境" class="headerlink" title="搭建环境"></a>搭建环境</h1><h2 id="安装-node"><a href="#安装-node" class="headerlink" title="安装 node"></a>安装 node</h2><ul>
<li>因为 hexo 是基于 node 框架的,先下载安装 node ,查看<code>node -v</code>版本,没有的话就根据提示操作</li>
</ul>
<h2 id="安装-npm"><a href="#安装-npm" class="headerlink" title="安装 npm"></a>安装 npm</h2><ul>
<li>安装 nodejs 肯定要安装 npm ,Ubuntu下载可能会很慢,建议换成国内源,参考<a href="https://www.cnblogs.com/vipstone/p/9038023.html" target="_blank" rel="noopener">Ubuntu apt-get和pip源更换</a></li>
</ul>
<h2 id="初始化-blog"><a href="#初始化-blog" class="headerlink" title="初始化 blog"></a>初始化 blog</h2><ol>
<li>安装 hexo ,在 终端 中输入:<code>npm install hexo-cli -g</code>(<a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">参考Hexo文档</a>)</li>
<li>初始化 blog 目录:<code>hexo init happybear1234.github.io</code>(这里的 happybear1234 换成你自己的英文名,我这里就是github的用户名)</li>
<li>初始化之后,进入到 blog 目录下:<code>cd happybear1234.github.io</code>(以后对博客的所以操作都是在这)</li>
<li>安装<code>npm install</code></li>
<li>clean一下:<code>hexo clean</code></li>
<li>生成静态页面:<code>hexo g</code></li>
<li>运行起来:<code>hexo s</code></li>
</ol>
<ul>
<li>打开浏览器,输入 终端 里网址 localhost:4000 就能看到了(如果提示服务端口被占用,可以换个端口,<code>hexo server -p 5000</code>)</li>
</ul>
<h1 id="选一个Hexo主题"><a href="#选一个Hexo主题" class="headerlink" title="选一个Hexo主题"></a>选一个Hexo主题</h1><ul>
<li>这里提供<a href="https://www.zhihu.com/question/24422335" target="_blank" rel="noopener">知乎答主们推荐的hexo主题大全</a>,刚开始为了熟悉各种配置建议使用 NexT 主题,因为文档比较详细,界面也很简洁,如果安装 NexT 主题和配置可以参考<a href="https://theme-next.org/docs/getting-started/" target="_blank" rel="noopener">文档</a></li>
</ul>
<h1 id="部署到网上"><a href="#部署到网上" class="headerlink" title="部署到网上"></a>部署到网上</h1><ul>
<li>现在的 blog 只能自己本地访问,可以使用 Github Pages 免费部署</li>
</ul>
<h2 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h2><ul>
<li>创建一个 xxx.github.io 的 public 仓库,这里 xxx 写你的名字,我这里写的 happybear1234.github.io,那么之后我就可以用 happybear1234.github.io 来访问了</li>
</ul>
<h2 id="安装-hexo-deployer-git"><a href="#安装-hexo-deployer-git" class="headerlink" title="安装 hexo-deployer-git"></a>安装 hexo-deployer-git</h2><ul>
<li>在 blog 目录下输入下面命令,这样本地的文章才能 push 到 Github 上面去<br>  <code>npm install hexo-deployer-git --save</code></li>
</ul>
<h2 id="配置Git"><a href="#配置Git" class="headerlink" title="配置Git"></a>配置Git</h2><ul>
<li><p>打开 blog 目录下配置文件:<code>vi _config.yml</code>,输入你的 git 地址:</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  	    type: git</span><br><span class="line">  	    repo: https:&#x2F;&#x2F;github.com&#x2F;xxx&#x2F;xxxx.github.io.git</span><br></pre></td></tr></table></figure>
<h2 id="推送网站到-Github-上"><a href="#推送网站到-Github-上" class="headerlink" title="推送网站到 Github 上"></a>推送网站到 Github 上</h2></li>
<li><p>直接在 blog 目录下输入:<code>hexo d</code></p>
</li>
<li><p>push 上去以后你就可以输入 xxx.github.io 进行访问啦</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">happybear</p>
  <div class="site-description" itemprop="description">happybear的个人博客,主要涉及到编程(C++,Python,Linux),个人提升学习</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/happybear1234" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;happybear1234" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">happybear</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">104k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:35</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>




  <script src="https://www.gstatic.com/firebasejs/6.3.3/firebase-app.js"></script>
  <script src="https://www.gstatic.com/firebasejs/6.3.3/firebase-firestore.js"></script>
  <script>
    firebase.initializeApp({
      apiKey   : '',
      projectId: ''
    });

    function getCount(doc, increaseCount) {
      // IncreaseCount will be false when not in article page
      return doc.get().then(d => {
        var count = 0;
        if (!d.exists) { // Has no data, initialize count
          if (increaseCount) {
            doc.set({
              count: 1
            });
            count = 1;
          }
        } else { // Has data
          count = d.data().count;
          if (increaseCount) {
            // If first view this article
            doc.set({ // Increase count
              count: count + 1
            });
            count++;
          }
        }

        return count;
      });
    }

    function appendCountTo(el) {
      return count => {
        el.innerText = count;
      }
    }
  </script>
  <script>
    (function() {
      var db = firebase.firestore();
      var articles = db.collection('articles');

      if (CONFIG.page.isPost) { // Is article page
        var title = document.querySelector('.post-title').innerText.trim();
        var doc = articles.doc(title);
        var increaseCount = CONFIG.hostname === location.hostname;
        if (localStorage.getItem(title)) {
          increaseCount = false;
        } else {
          // Mark as visited
          localStorage.setItem(title, true);
        }
        getCount(doc, increaseCount).then(appendCountTo(document.querySelector('.firestore-visitors-count')));
      } else if (CONFIG.page.isHome) { // Is index page
        var promises = [...document.querySelectorAll('.post-title')].map(element => {
          var title = element.innerText.trim();
          var doc = articles.doc(title);
          return getCount(doc);
        });
        Promise.all(promises).then(counts => {
          var metas = document.querySelectorAll('.firestore-visitors-count');
          counts.forEach((val, idx) => {
            appendCountTo(metas[idx])(val);
          });
        });
      }
    })();
  </script>




      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
